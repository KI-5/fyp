#gdrive
from google.colab import drive
drive.mount('/content/drive')

## Imports

!pip install torch torchvision transformers

!pip install rouge-score


#imports
import json
import os
from sklearn.model_selection import train_test_split
import torch
from torch.utils.data import DataLoader, Dataset

import torch.nn as nn
import torch.optim as optim
import tensorflow as tf
from nltk.translate.bleu_score import sentence_bleu

from transformers import BertTokenizer
import time
from datetime import timedelta

## GPU

!nvidia-smi

torch.cuda.is_available()

tf.test.gpu_device_name()

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

print(f"Using device: {device}")

## Load data

#loading the json dataset
def load_json_data(file_path):
  with open(file_path, 'r') as f:
    data = [json.loads(line.strip()) for line in f]
  return data

#loading the description dataset
def load_text_data(file_path):
  with open(file_path, 'r') as f:
    data = [line.strip() for line in f]
  return data

# from numba import cuda

# cuda.select_device(0) # choosing second GPU

# cuda.close()

#paths to the datasets
json_file_path='/content/drive/My Drive/Implementation/Final Thesis/FYP/Data/json_data.jsonl'
text_file_path= '/content/drive/My Drive/Implementation/Final Thesis/FYP/Data/descriptions_data.txt'

json_data= load_json_data(json_file_path)
descriptions=load_text_data(text_file_path)

print(f"Length of JSON data: {len(json_data)}")
print(f"Length of description data: {len(descriptions)}")


data = [{'json':json.dumps(json_data[i]), 'description':descriptions[i]} for i in range(len(json_data))]

print(f"First JSON entry:{data[0]['json']}")
print(f"First description entry: {data[0]['description']}")
print(f"Second JSON entry:{data[1]['json']}")
print(f"Second description entry: {data[1]['description']}")

# def load_data(file_path):
#   """
#   method to load the dataset

#   Parameters
#   file_path= the path of file stored
#   Returns
#   data= a list of json objects
#   """
#   data = []
#   with open(file_path, 'r') as f:
#     for line in f:
#       data.append(json.loads(line.strip()))
#   return data

# #path of data
# data_path= '/content/drive/My Drive/fyp/FYP/Data/pairs_data.jsonl'

# data =load_data(data_path)

# #testing output
# print(json.dumps(data[:2], indent=2))

## Tokenize and preprocess

#flattening JSON and create tokenized pairs
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

def preprocess_dataset(data):
  """
  method to preprocess the dataset

  Parameters
  data= a list of json objects
  Returns
  input_ids= a tensor of input ids
  target_ids= a tensor of target ids
  """
  #input-json, target-description
  inputs, targets= [], []
  for item in data:
    #json
    json_string= item['json']
    #description
    description= item['description']
    inputs.append(json_string)
    targets.append(description)

  input_ids =tokenizer(inputs,padding=True,truncation=True, return_tensors="pt")['input_ids']
  target_ids= tokenizer(targets,padding=True,truncation=True,return_tensors="pt")['input_ids']
  return input_ids,target_ids

input_ids, target_ids= preprocess_dataset(data)

save_path= '/content/drive/My Drive/Implementation/Final Thesis/FYP/Prompt generation/processed_data_prompt/'

#check if there
if not os.path.exists(save_path):
  os.makedirs(save_path)

#input and target ids to save
torch.save(input_ids, os.path.join(save_path,'input_ids.pt'))
torch.save(target_ids, os.path.join(save_path,'target_ids.pt'))

#load from drive
input_ids = torch.load(os.path.join(save_path, 'input_ids.pt'))
target_ids = torch.load(os.path.join(save_path, 'target_ids.pt'))

print("input shape testing:", input_ids.shape)
print("target shape", target_ids.shape)

print("Sample input IDs:", input_ids[:5])
print("Sample target IDs:", target_ids[:5])

## Dataset and split

#split into train test validation sets
input_ids_train, input_ids_temp, target_ids_train, target_ids_temp = train_test_split(input_ids, target_ids, test_size=0.2, random_state=42)
input_ids_val, input_ids_test, target_ids_val, target_ids_test = train_test_split(input_ids_temp, target_ids_temp, test_size=0.5, random_state=42)


# input_ids_train, input_ids_val, target_ids_train, target_ids_val = train_test_split(input_ids, target_ids, test_size=0.2, random_state=42)

class LSTMDataset(Dataset):
  """
  Dataset class for LSTM model
  """
  def __init__(self, input_ids, target_ids):
    self.input_ids = input_ids
    self.target_ids = target_ids

  def __len__(self):
    return len(self.input_ids)
#get item
  def __getitem__(self, idx):
    return self.input_ids[idx], self.target_ids[idx]

#creating the dataset
train_dataset = LSTMDataset(input_ids_train, target_ids_train)
validation_dataset = LSTMDataset(input_ids_val, target_ids_val)
test_dataset = LSTMDataset(input_ids_test, target_ids_test)

#datalaoder for train test validation
train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)
validation_dataloader = DataLoader(validation_dataset, batch_size=32, shuffle=False)
test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)

## Model

class LSTMSeq2Seq(nn.Module):
  """
  LSTM Seq2Seq model
  """
  #inherit from torch.nn.Module!!!!!!!!!!!!!!!!
  def __init__(self, input_dim, emb_dim, hidden_dim, output_dim, num_layers):
    """ Initialize the model input_dim-input vocab size, emb_dim-embedding vector size,
    hidden_dim-hidden state no of features, output_dim-output vocab size, num_layers-recurrent layers in lstm"""
    #call nn.Module constructureeeeeee of paruent
    super(LSTMSeq2Seq, self).__init__()
    #create embedding layer tp convert input tokens to dense vectors to emb dim
    self.embedding= nn.Embedding(input_dim, emb_dim)
    #lstm created for encoder!! emb_dim-input dimention to lstm, input and output tensors are of shape (batch_size, seq_length, feature_dim)
    self.encoder =nn.LSTM(emb_dim, hidden_dim, num_layers, batch_first=True)
    #lstm created!!for decoder
    self.decoder= nn.LSTM(emb_dim, hidden_dim, num_layers, batch_first=True)
    #create fclayer, output of decoder projected it to output output_dim
    self.fc =nn.Linear(hidden_dim, output_dim)

  def forward(self, src, tgt):
    """ forward pass of the model src-input seq(json), tgt-tgt output seq(desc) """
    #convert input tokens into dense vectors
    embedded_src= self.embedding(src)
    #passes the embedded source sequence through the encoder lstm
    #output-output of lstm, hidden-hidden state of lstm
    encoder_output, (hidden, cell)= self.encoder(embedded_src)

    #convert tgt tokens to dense vectors
    embedded_tgt= self.embedding(tgt)
    #through decoder lstm. #output-output of lstm, hidden-hidden state of lstm
    decoder_output, _ =self.decoder(embedded_tgt, (hidden, cell))
    #output through fc. generate final token predictions
    output= self.fc(decoder_output)

    return output

input_dimentions = tokenizer.vocab_size
embedding_dimentions = 256
hidden_dimentions= 256
output_dimentions= tokenizer.vocab_size
layer_numbers = 2

model= LSTMSeq2Seq(input_dimentions, embedding_dimentions, hidden_dimentions, output_dimentions, layer_numbers).cuda()

print(model)

#optimizer
optimizer= torch.optim.Adam(model.parameters(), lr=0.001)


#criterion
criterion=nn.CrossEntropyLoss()


#scheduler
scheduler= optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.95)

# Gradient Clipping
max_grad_norm= 1.0

def validate(model, dataloader, criterion):
  """
  method to validate the model
  Parameters
  model= the model to be validated
  dataloader= the validation dataloader
  criterion= the loss function

  Returns
  avg_loss= the average loss of the validation set
  """
  model.eval()
  total_loss= 0
  with torch.no_grad():#no grad
  #batchs of validation data
    for src,tgt in dataloader:
      src, tgt= src.cuda().long(), tgt.cuda().long()
      #exclude last token of tgt seq
      output =model(src, tgt[:, :-1])
      #calculate loss,reshape output tensor,reshape tgt tensor
      loss= criterion(output.view(-1, output_dimentions), tgt[:, 1:].reshape(-1))
      total_loss+=loss.item()
  avg_loss =total_loss/len(dataloader)
  return avg_loss

def train(model, train_dataloader, val_dataloader, optimizer, criterion, epochs, checkpoint_path):
  """
  method to train the model
  Parameters
  model= the model to be trained
  train_dataloader= the training dataloader
  val_dataloader= the validation dataloader
  optimizer= the optimizer
  criterion= the loss function
  epochs= the number of epochs
  checkpoint_path= the path to save the checkpoints

  """
  best_val_loss= float('inf')
  start_time =time.time()

  for epoch in range(epochs):
    epoch_start_time= time.time()
    model.train()
    total_loss =0
    for i, (src,tgt) in enumerate(train_dataloader):
      src, tgt= src.to(device).long(), tgt.to(device).long()
       #####c;eartrrrrr
      optimizer.zero_grad()
      output= model(src, tgt[:, :-1])
      #class weight
      loss =criterion(output.view(-1, output_dimentions), tgt[:, 1:].contiguous().view(-1))
      #backpropagate loss
      loss.backward()
      torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)
      optimizer.step()
      total_loss+= loss.item()
      if i % 100== 0:
        #every 100 batches
        elapsed_time= time.time() - epoch_start_time
        print(f'Epoch {epoch+1}/{epochs}, Step {i+1}/{len(train_dataloader)}, Loss: {loss.item()}, Elapsed time: {timedelta(seconds=int(elapsed_time))}')

    avg_train_loss= total_loss / len(train_dataloader)
    avg_val_loss =validate(model, val_dataloader, criterion)
    scheduler.step()

    epoch_time= time.time() - epoch_start_time
    elapsed_time =time.time() - start_time
    estimated_total_time= elapsed_time / (epoch + 1) * epochs
    estimated_remaining_time= estimated_total_time - elapsed_time


    print(f'Epoch {epoch+1} completed. Average training loss: {avg_train_loss}, Average validation loss: {avg_val_loss}')
    print(f'Epoch Time:{timedelta(seconds=int(epoch_time))}, Elapsed Time: {timedelta(seconds=int(elapsed_time))}, Estimated Total Time: {timedelta(seconds=int(estimated_total_time))}, Estimated Remaining Time: {timedelta(seconds=int(estimated_remaining_time))}')

    if avg_val_loss< best_val_loss:
      best_val_loss= avg_val_loss
      checkpoint_file= os.path.join(checkpoint_path, 'best_model.pth')
      torch.save(model.state_dict(), checkpoint_file)
      print(f'Best model saved at epoch {epoch+1} +  validation loss: {avg_val_loss}')

    checkpoint_file= os.path.join(checkpoint_path, f'seq2seq_epoch{epoch+1}.pth')
    torch.save(model.state_dict(), checkpoint_file)
    print(f'Checkpoint saved for epoch {epoch+1} at {checkpoint_file}')

checkpoint_path= '/content/drive/My Drive/Implementation/Final Thesis/FYP/Prompt generation/LSTM Model checkpoints'

if not os.path.exists(checkpoint_path):
  os.makedirs(checkpoint_path)

# print(f"Checkpoint directory: {checkpoint_path}")
# gjbh


epochs=20

train(model, train_dataloader, validation_dataloader, optimizer, criterion, epochs, checkpoint_path=checkpoint_path)

def save_model(model, epoch, checkpoint_path):
  """
  method to save the model
  Parameters
  model= the model to be saved
  epoch= the epoch number
  checkpoint_path= the path to save the checkpoints
  """
  checkpoint_file= os.path.join(checkpoint_path, f'seq2seq_epoch{epoch}.pth')
  torch.save(model.state_dict(), checkpoint_file)
  print(f'Checkpoint saved for epoch {epoch} at {checkpoint_file}')


save_model(model, epoch=3, checkpoint_path='/content/drive/My Drive/Implementation/Final Thesis/FYP/Prompt generation/LSTM Model checkpoints')

# model_path = '/content/drive/My Drive/Implementation/Final Thesis/FYP/Model Checkpoints/seq2seq_epoch5.pth'
# model.load_state_dict(torch.load(model_path))
# model.cuda()

def load_model(model, checkpoint_path, epoch):
  checkpoint_file = os.path.join(checkpoint_path, f'seq2seq_epoch{epoch}.pth')
  model.load_state_dict(torch.load(checkpoint_file))
  model.cuda()
  print(f'Model loaded from {checkpoint_file}')

load_model(model, checkpoint_path='/content/drive/My Drive/Implementation/Final Thesis/FYP/Prompt generation/LSTM Model checkpoints', epoch=20)

# for i, desc in enumerate(generated_descriptions[:5]):
#     print(f"JSON {i}: {data[i]['json']}")
#     print(f"Generated Description {i}: {desc}")
#     print()

import torch.nn.functional as F

## Inference

def generate_description_for_json(model, json_input, tokenizer, max_len=50):
  """
  method to generate a description for a given JSON string
  Parameters
  model= the model to be used
  json_input= the JSON string to be used
  tokenizer= the tokenizer to be used
  max_len= the maximum length of the generated description
  Returns
  generated_description= the generated description
  """
  model.eval()
  with torch.no_grad():
    inputs = tokenizer(json_input, return_tensors="pt", padding=True, truncation=True)
    input_ids = inputs['input_ids'].cuda()

    #converts the input IDs to embeddings
    encoder_outputs, (hidden, cell) = model.encoder(model.embedding(input_ids))

  # start-of-sequence token
    sos_token_id = tokenizer.cls_token_id
    #end of sequence
    eos_token_id = tokenizer.sep_token_id
    #get next token takes , highest propert-argmax from logit to convert to py integer
    tgt_input = torch.tensor([[sos_token_id]], device=input_ids.device)

    generated_tokens = []
    for _ in range(max_len):
      decoder_output, (hidden, cell) = model.decoder(model.embedding(tgt_input), (hidden, cell))
      output_token = model.fc(decoder_output[:, -1, :])
      next_token_id = torch.argmax(output_token, dim=1).item()

      if next_token_id == eos_token_id:
        break

      generated_tokens.append(next_token_id)
      tgt_input = torch.tensor([[next_token_id]], device=input_ids.device)
    #converts list of generated token IDs back to a string
    generated_description = tokenizer.decode(generated_tokens, skip_special_tokens=True)
    return generated_description

example_json1= "{\"style\": \"Professional\", \"component_name\": \"Button\", \"subtype\": \"Default\", \"variant_details\": {\"State\": [\"Default\"], \"Size\": [\"Small\"]}, \"name\": \"State=Default, Size=Small\", \"type\": \"COMPONENT\", \"backgroundColor\": {\"r\": 0.0, \"g\": 0.0, \"b\": 0.0, \"a\": 0.0}, \"absoluteBoundingBox\": {\"x\": -4633.0, \"y\": -2143.0, \"width\": 105.0, \"height\": 36.0}, \"constraints\": {\"vertical\": \"TOP\", \"horizontal\": \"LEFT\"}, \"children\": [{\"name\": \"_Button base\", \"type\": \"FRAME\", \"backgroundColor\": {\"r\": 0.49640899896621704, \"g\": 0.33841174840927124, \"b\": 0.849823534488678, \"a\": 1.0}, \"absoluteBoundingBox\": {\"x\": -4633.0, \"y\": -2143.0, \"width\": 105.0, \"height\": 36.0}, \"constraints\": {\"vertical\": \"TOP\", \"horizontal\": \"LEFT\"}, \"children\": [{\"name\": \"Text\", \"type\": \"TEXT\", \"backgroundColor\": {}, \"absoluteBoundingBox\": {\"x\": -4619.0, \"y\": -2135.0, \"width\": 77.0, \"height\": 20.0}, \"constraints\": {\"vertical\": \"TOP\", \"horizontal\": \"LEFT\"}, \"children\": [], \"fills\": [{\"blendMode\": \"NORMAL\", \"type\": \"SOLID\", \"color\": {\"r\": 1.0, \"g\": 1.0, \"b\": 1.0, \"a\": 1.0}}], \"strokes\": [], \"strokeWeight\": 1.0, \"effects\": [], \"characters\": \"Button CTA\", \"styles\": {\"fill\": \"1:4\", \"text\": \"1:8\"}, \"itemSpacing\": 0, \"visible\": true}], \"fills\": [{\"blendMode\": \"NORMAL\", \"type\": \"SOLID\", \"color\": {\"r\": 0.49640899896621704, \"g\": 0.33841174840927124, \"b\": 0.849823534488678, \"a\": 1.0}}], \"strokes\": [{\"blendMode\": \"NORMAL\", \"type\": \"SOLID\", \"color\": {\"r\": 0.49640899896621704, \"g\": 0.33841174840927124, \"b\": 0.849823534488678, \"a\": 1.0}}], \"strokeWeight\": 1.0, \"effects\": [{\"type\": \"DROP_SHADOW\", \"visible\": true, \"color\": {\"r\": 0.062745101749897, \"g\": 0.0941176488995552, \"b\": 0.1568627506494522, \"a\": 0.05000000074505806}, \"blendMode\": \"NORMAL\", \"offset\": {\"x\": 0.0, \"y\": 1.0}, \"radius\": 2.0, \"showShadowBehindNode\": true}], \"characters\": \"\", \"styles\": {\"strokes\": \"1:2\", \"stroke\": \"1:2\", \"effect\": \"1:3\"}, \"itemSpacing\": 8.0, \"visible\": true}], \"fills\": [{\"blendMode\": \"NORMAL\", \"visible\": false, \"type\": \"SOLID\", \"color\": {\"r\": 1.0, \"g\": 1.0, \"b\": 1.0, \"a\": 1.0}}], \"strokes\": [], \"strokeWeight\": 1.0, \"effects\": [], \"characters\": \"\", \"styles\": {}, \"itemSpacing\": 0, \"visible\": true}"
example_json2 = "{\"style\": \"Basic\", \"component_name\": \"input field\", \"subtype\": \"Dark\", \"variant_details\": {\"State\": [\"Focused\"], \"Size\": [\"Medium\"]}, \"name\": \"State=Focused, Size=Medium\", \"type\": \"COMPONENT\", \"backgroundColor\": {\"r\": 1.0, \"g\": 1.0, \"b\": 1.0, \"a\": 1.0}, \"absoluteBoundingBox\": {\"x\": -200.0, \"y\": -400.0, \"width\": 200.0, \"height\": 50.0}, \"constraints\": {\"vertical\": \"TOP\", \"horizontal\": \"LEFT\"}, \"children\": [{\"name\": \"_Input base\", \"type\": \"FRAME\", \"backgroundColor\": {\"r\": 0.9, \"g\": 0.9, \"b\": 0.9, \"a\": 1.0}, \"absoluteBoundingBox\": {\"x\": -200.0, \"y\": -400.0, \"width\": 200.0, \"height\": 50.0}, \"constraints\": {\"vertical\": \"TOP\", \"horizontal\": \"LEFT\"}, \"children\": [{\"name\": \"Placeholder\", \"type\": \"TEXT\", \"backgroundColor\": {}, \"absoluteBoundingBox\": {\"x\": -190.0, \"y\": -390.0, \"width\": 180.0, \"height\": 30.0}, \"constraints\": {\"vertical\": \"TOP\", \"horizontal\": \"LEFT\"}, \"children\": [], \"fills\": [{\"blendMode\": \"NORMAL\", \"type\": \"SOLID\", \"color\": {\"r\": 0.6, \"g\": 0.6, \"b\": 0.6, \"a\": 1.0}}], \"strokes\": [], \"strokeWeight\": 1.0, \"effects\": [], \"characters\": \"Enter text here\", \"styles\": {\"fill\": \"1:4\", \"text\": \"1:8\"}, \"itemSpacing\": 0, \"visible\": true}], \"fills\": [{\"blendMode\": \"NORMAL\", \"type\": \"SOLID\", \"color\": {\"r\": 0.9, \"g\": 0.9, \"b\": 0.9, \"a\": 1.0}}], \"strokes\": [{\"blendMode\": \"NORMAL\", \"type\": \"SOLID\", \"color\": {\"r\": 0.8, \"g\": 0.8, \"b\": 0.8, \"a\": 1.0}}], \"strokeWeight\": 1.0, \"effects\": [], \"characters\": \"\", \"styles\": {\"strokes\": \"1:2\", \"stroke\": \"1:2\", \"effect\": \"1:3\"}, \"itemSpacing\": 8.0, \"visible\": true}], \"fills\": [{\"blendMode\": \"NORMAL\", \"visible\": false, \"type\": \"SOLID\", \"color\": {\"r\": 1.0, \"g\": 1.0, \"b\": 1.0, \"a\": 1.0}}], \"strokes\": [], \"strokeWeight\": 1.0, \"effects\": [], \"characters\": \"\", \"styles\": {}, \"itemSpacing\": 0, \"visible\": true}"


generated_description1= generate_description_for_json(model, example_json1, tokenizer)
generated_description2= generate_description_for_json(model, example_json2, tokenizer)


print(f"Generated description for example 1: {generated_description1}")
print(f"Generated description for example 2: {generated_description2}")

print(f"Generated description for example 1: {generated_description1}")
print(f"Generated description for example 2: {generated_description2}")

from tqdm import tqdm

## BLEU and ROUGE

def generate_descriptions_for_dataset_with_batched_inference(model, data, tokenizer, batch_size=32, max_len=50, output_file_path=None):
    model.eval()
    generated_descriptions = []
    with torch.no_grad():
        for i in tqdm(range(0, len(data), batch_size), desc="Generating descriptions"):
            batch_data = data[i:i + batch_size]
            batch_json_inputs = [item['json'] for item in batch_data]

            # Tokenize the batch of JSON inputs
            inputs = tokenizer(batch_json_inputs, return_tensors="pt", padding=True, truncation=True).to(device)
            input_ids = inputs['input_ids']

            # Encode the batch of inputs
            encoder_outputs, (hidden, cell) = model.encoder(model.embedding(input_ids))

            # Initialize the target input with the start-of-sequence token for each item in the batch
            sos_token_id = tokenizer.cls_token_id
            eos_token_id = tokenizer.sep_token_id
            tgt_input = torch.tensor([[sos_token_id]] * input_ids.size(0), device=device)

            batch_generated_tokens = [[] for _ in range(input_ids.size(0))]
            for _ in range(max_len):
                decoder_output, (hidden, cell) = model.decoder(model.embedding(tgt_input), (hidden, cell))
                output_token = model.fc(decoder_output[:, -1, :])
                next_token_ids = torch.argmax(output_token, dim=1)

                for j in range(input_ids.size(0)):
                    next_token_id = next_token_ids[j].item()
                    if next_token_id == eos_token_id:
                        continue
                    batch_generated_tokens[j].append(next_token_id)
                tgt_input = next_token_ids.unsqueeze(1)

            for tokens in batch_generated_tokens:
                generated_description = tokenizer.decode(tokens, skip_special_tokens=True)
                generated_descriptions.append(generated_description)

                if output_file_path:
                    with open(output_file_path, 'a') as f:
                        f.write(generated_description + '\n')

    return generated_descriptions

# Generate descriptions for the entire dataset with batched inference
generated_descriptions = generate_descriptions_for_dataset_with_batched_inference(model, data, tokenizer)

output_file_path = '/content/drive/My Drive/fyp/FYP/lstm_generated_data.txt'

# Prepare references and hypotheses for evaluation
reference_dict = {}
for item in data:
    json_str = item['json']
    description = item['description']
    if json_str not in reference_dict:
        reference_dict[json_str] = []
    reference_dict[json_str].append(description)

json_inputs = [item['json'] for item in data]  # List of JSON inputs in the same order as generated_descriptions


# Prepare references and hypotheses
references = []
hypotheses = []

for json_input, generated_description in zip(json_inputs, generated_descriptions):
    references.append([ref.split() for ref in reference_dict[json_input]])  # Multiple references for each JSON input
    hypotheses.append(generated_description.split())  # Generated description


bleu_score = corpus_bleu(references, hypotheses, smoothing_function=SmoothingFunction().method1)


import numpy as np

print(f'BLEU Score: {bleu_score}')


# Initialize ROUGE scorer
scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)


rouge1_scores = []
rouge2_scores = []
rougeL_scores = []


for refs, hyp in tqdm(zip(references, hypotheses), total=len(references), desc="Calculating ROUGE scores"):
    refs_text = [' '.join(ref) for ref in refs]
    hyp_text = ' '.join(hyp)
    scores = [scorer.score(ref, hyp_text) for ref in refs_text]

    # Take the maximum score among the multiple references
    rouge1_scores.append(max(score['rouge1'].fmeasure for score in scores))
    rouge2_scores.append(max(score['rouge2'].fmeasure for score in scores))
    rougeL_scores.append(max(score['rougeL'].fmeasure for score in scores))


# Average the scores
rouge1 = sum(rouge1_scores) / len(rouge1_scores)
rouge2 = sum(rouge2_scores) / len(rouge2_scores)
rougeL = sum(rougeL_scores) / len(rougeL_scores)



print(f'ROUGE-1: {rouge1}')
print(f'ROUGE-2: {rouge2}')
print(f'ROUGE-L: {rougeL}')

#check this!
# def average_rouge_scores(rouge_scores):
#     rouge1 = sum(score['rouge1'].fmeasure for score in rouge_scores) / len(rouge_scores)
#     rouge2 = sum(score['rouge2'].fmeasure for score in rouge_scores) / len(rouge_scores)
#     rougeL = sum(score['rougeL'].fmeasure for score in rouge_scores) / len(rouge_scores)
#     return

# avg_rouge1, avg_rouge2, avg_rougeL = average_rouge_scores(rouge_scores)


def token_level_metrics(references, hypotheses):
    ref_tokens = [token for ref_list in references for ref in ref_list for token in ref]
    hyp_tokens = [token for hyp in hypotheses for token in hyp]

    ref_counter = Counter(ref_tokens)
    hyp_counter = Counter(hyp_tokens)

    common_tokens = set(ref_tokens) & set(hyp_tokens)

    precision = sum(hyp_counter[token] for token in common_tokens) / sum(hyp_counter.values())
    recall = sum(ref_counter[token] for token in common_tokens) / sum(ref_counter.values())
    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0

    return precision, recall, f1

precision, recall, f1 = token_level_metrics(references, hypotheses)


print(f'Precision: {precision}')
print(f'Recall: {recall}')
print(f'F1 Score: {f1}')

def token_level_accuracy(references, hypotheses):
    total_tokens = 0
    matching_tokens = 0

    for refs, hyp in zip(references, hypotheses):
        ref_tokens = set(token for ref in refs for token in ref)
        hyp_tokens = set(hyp)
        total_tokens += len(hyp_tokens)
        matching_tokens += len(hyp_tokens & ref_tokens)

    return matching_tokens / total_tokens

accuracy = token_level_accuracy(references, hypotheses)


print(f'Token-level Accuracy: {accuracy}')



# def generate_description_for_json(model, json_input, tokenizer, max_len=50, temperature=1.0, top_p=0.9):
#     model.eval()
#     with torch.no_grad():
#         inputs = tokenizer(json_input, return_tensors="pt", padding=True, truncation=True)
#         input_ids = inputs['input_ids'].cuda()

#         encoder_outputs, (hidden, cell) = model.encoder(model.embedding(input_ids))

#         sos_token_id = tokenizer.cls_token_id
#         eos_token_id = tokenizer.sep_token_id
#         tgt_input = torch.tensor([[sos_token_id]], device=input_ids.device)

#         generated_tokens = []
#         for _ in range(max_len):
#             decoder_output, (hidden, cell) = model.decoder(model.embedding(tgt_input), (hidden, cell))
#             output_token = model.fc(decoder_output[:, -1, :])
#             output_token = output_token / temperature  # Apply temperature to logits

#             sorted_logits, sorted_indices = torch.sort(output_token, descending=True)
#             cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)

#             sorted_indices_to_remove = cumulative_probs > top_p
#             sorted_indices_to_remove[:, 1:] = sorted_indices_to_remove[:, :-1].clone()
#             sorted_indices_to_remove[:, 0] = 0

#             indices_to_remove = sorted_indices[sorted_indices_to_remove]
#             output_token[:, indices_to_remove] = -float('Inf')

#             probabilities = F.softmax(output_token, dim=-1)
#             next_token_id = torch.multinomial(probabilities, 1).item()

#             if next_token_id == eos_token_id:
#                 break

#             generated_tokens.append(next_token_id)
#             tgt_input = torch.tensor([[next_token_id]], device=input_ids.device)

#         generated_description = tokenizer.decode(generated_tokens, skip_special_tokens=True)
#         return generated_description

json_input_label = "{\"color\": \"rgba(255, 255, 255, 1.0)\", \"strokes\": [\"rgba(126, 86, 216, 1.0)\"], \"strokeWeight\": 1.0, \"text\": \"Label CTA\", \"textColor\": \"rgba(255, 255, 255, 1.0)\", \"borderRadius\": 10.0, \"fontFamily\": \"Inter\", \"fontWeight\": 500, \"fontSize\": 14.0, \"effects\": [{\"type\": \"DROP_SHADOW\", \"color\": \"rgba(16, 24, 40, 0.05000000074505806)\"}], \"padding\": 0, \"width\": 77.0, \"height\": 20.0, \"x\": -4619.0, \"y\": -2135.0, \"hasIcon\": false, \"style\": \"Playful\", \"component_name\": \"Label\", \"subtype\": \"Default\", \"variant_details\": {\"State\": [\"Default\"], \"Size\": [\"Small\"]}}"

generated_description= generate_description_for_json(model, json_input_label, tokenizer)



print(f"Generated description: {generated_description}")

def generate_description_for_json(model, json_input, tokenizer, max_len=50, temperature=1.0, top_p=0.9):
  """
  method to generate a description for a given JSON string
  Parameters
  model= the model to be used
  json_input= the JSON string to be used
  tokenizer= the tokenizer to be used
  max_len= the maximum length of the generated description
  Returns
  generated_description= the generated description
  """
  model.eval()
  with torch.no_grad():
    inputs= tokenizer(json_input, return_tensors="pt", padding=True, truncation=True)
    input_ids =inputs['input_ids'].cuda()

    encoder_outputs, (hidden, cell)= model.encoder(model.embedding(input_ids))

    sos_token_id= tokenizer.cls_token_id
    eos_token_id =tokenizer.sep_token_id
    tgt_input =torch.tensor([[sos_token_id]], device=input_ids.device)

    generated_tokens= []
    for _ in range(max_len):
      decoder_output, (hidden, cell)= model.decoder(model.embedding(tgt_input), (hidden, cell))
      output_token=model.fc(decoder_output[:, -1, :])
      output_token= output_token / temperature

      sorted_logits, sorted_indices = torch.sort(output_token, descending=True)
      cumulative_probs= torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)

      sorted_indices_to_remove= cumulative_probs > top_p
      sorted_indices_to_remove[:, 1:]= sorted_indices_to_remove[:, :-1].clone()
      sorted_indices_to_remove[:, 0]= 0

      indices_to_remove= sorted_indices[sorted_indices_to_remove]
      output_token[:, indices_to_remove]= -float('Inf')

      probabilities= F.softmax(output_token, dim=-1)
      next_token_id= torch.multinomial(probabilities, 1).item()

      if next_token_id== eos_token_id:
        break

      generated_tokens.append(next_token_id)
      tgt_input= torch.tensor([[next_token_id]], device=input_ids.device)

      generated_description= tokenizer.decode(generated_tokens, skip_special_tokens=True)
      return generated_description

def generate_multiple_descriptions_for_json(model, json_input, tokenizer, max_len=50, num_descriptions=5):
  """
  method to generate multiple descriptions for a given JSON string

  Parameters
  model= the model to be used
  json_input= the JSON string to be used
  tokenizer= the tokenizer to be used
  max_len= the maximum length of the generated description
  num_descriptions= the number of descriptions to be generated

  Returns
  descriptions= the generated descriptions
  """
  descriptions= []
  for _ in range(num_descriptions):
    description= generate_description_for_json(model, json_input, tokenizer, max_len)
    descriptions.append(description)
  return descriptions

def generate_and_save_descriptions(model, json_dataset_path, tokenizer, output_json_path, output_desc_path, num_descriptions=5):
  """
  method to generate and save descriptions for a given JSON dataset

  Parameters
  model= the model to be used
  json_dataset_path= the path to the JSON dataset
  tokenizer= the tokenizer to be used

  output_json_path= the path to save the generated JSONs
  output_desc_path= the path to save the generated descriptions
  num_descriptions= the number of descriptions to be generated for each JSON
  """

  #load JSON dataset
  with open(json_dataset_path, 'r') as f:
      json_dataset = [json.loads(line.strip()) for line in f]

  all_jsons= []
  all_descriptions =[]

  for json_item in json_dataset:
    json_str= json.dumps(json_item)
    descriptions= generate_multiple_descriptions_for_json(model, json_str, tokenizer, num_descriptions=num_descriptions)

    all_jsons.extend([json_str] * num_descriptions)
    all_descriptions.extend(descriptions)

    print(f"Generated {num_descriptions} descriptions for JSON: {json_str}")

  #save JSONs to a file
  with open(output_json_path, 'w') as f:
    for json_str in all_jsons:
      f.write(json_str + '\n')

  #save descriptions to a file
  with open(output_desc_path, 'w') as f:
    for description in all_descriptions:
      f.write(description + '\n')

json_dataset_path= '/content/drive/My Drive/Implementation/Final Thesis/FYP/simplecomponents.jsonl'


output_json_path ='/content/drive/My Drive/Implementation/Final Thesis/FYP/generated_jsons.txt'

output_desc_path= '/content/drive/My Drive/Implementation/Final Thesis/FYP/generated_descriptions.txt'


# json_dataset = '/content/drive/My Drive/Implementation/Final Thesis/FYP/simplecomponents.jsonl'


generate_and_save_descriptions(model, json_dataset_path, tokenizer, output_json_path, output_desc_path, num_descriptions=5)



## Stats

import random
import matplotlib.pyplot as plt
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, roc_curve, roc_auc_score
import seaborn as sns

from nltk.translate.bleu_score import corpus_bleu

# Function to extract component name from description
def get_component_name_from_description(description):
    components = ['icon-button', 'button', 'label', 'input-field', 'menu', 'list-item']
    description_lower = description.lower()

    # Normalize description by removing spaces and hyphens
    normalized_description = description_lower.replace(" ", "").replace("-", "")

    for component in components:
        normalized_component = component.replace("-", "")
        if normalized_component in normalized_description:
            return component

    return 'Unknown'

# Function to calculate component name metrics
def calculate_component_metrics(true_components, generated_components):
    accuracy = accuracy_score(true_components, generated_components) * 100
    precision = precision_score(true_components, generated_components, average='weighted') * 100
    recall = recall_score(true_components, generated_components, average='weighted') * 100
    f1 = f1_score(true_components, generated_components, average='weighted') * 100
    return accuracy, precision, recall, f1



def evaluate_model(model, dataset, tokenizer, sample_size=10, max_len=50):
    sample_indices = random.sample(range(len(dataset)), sample_size)

    true_component_names = []
    generated_component_names = []

    for idx in sample_indices:
        json_input = dataset[idx]['json']
        true_descriptions_sample = dataset[idx]['description']  # assuming this returns a list of descriptions
        true_component_name = json.loads(json_input).get('variant_properties', {}).get('component_name', 'Unknown').lower()

        generated_description = generate_description_for_json(model, json_input, tokenizer, max_len)
        generated_component_name = get_component_name_from_description(generated_description)

        true_component_names.append(true_component_name)
        generated_component_names.append(generated_component_name)

        print(f"JSON Input: {json_input}")
        print(f"True Descriptions: {true_descriptions_sample}")
        print(f"Generated Description: {generated_description}")
        print(f"True Component Name: {true_component_name}")
        print(f"Generated Component Name: {generated_component_name}")
        print("----")

    component_accuracy, component_precision, component_recall, component_f1 = calculate_component_metrics(true_component_names, generated_component_names)

    metrics = {
        'Component Accuracy (%)': component_accuracy,
        'Component Precision (%)': component_precision,
        'Component Recall (%)': component_recall,
        'Component F1 (%)': component_f1,
    }

    return metrics, true_component_names, generated_component_names


def progressive_evaluation(model, dataset, tokenizer, max_len=50):
    sample_sizes = [100, 200, 300, 400, 500]  # Different sample sizes for progressive evaluation
    accuracy_list = []
    precision_list = []
    recall_list = []
    f1_list = []

    all_true_components = []
    all_generated_components = []

    for sample_size in sample_sizes:
        metrics, true_components, generated_components = evaluate_model(model, dataset, tokenizer, sample_size=sample_size, max_len=max_len)
        accuracy_list.append(metrics['Component Accuracy (%)'])
        precision_list.append(metrics['Component Precision (%)'])
        recall_list.append(metrics['Component Recall (%)'])
        f1_list.append(metrics['Component F1 (%)'])
        all_true_components.extend(true_components)
        all_generated_components.extend(generated_components)
        print(f"Sample Size: {sample_size}, Metrics: {metrics}")

    # Plotting
    plt.figure(figsize=(10, 6))
    plt.plot(sample_sizes, accuracy_list, label='Accuracy (%)', marker='o')
    plt.plot(sample_sizes, precision_list, label='Precision (%)', marker='o')
    plt.plot(sample_sizes, recall_list, label='Recall (%)', marker='o')
    plt.plot(sample_sizes, f1_list, label='F1 Score (%)', marker='o')
    plt.xlabel('Sample Size')
    plt.ylabel('Percentage')
    plt.title('Progressive Evaluation Metrics')
    plt.legend()
    plt.grid(True)
    plt.show()

    # Confusion Matrix
    labels = list(set(all_true_components))
    cm = confusion_matrix(all_true_components, all_generated_components, labels=labels)

    plt.figure(figsize=(10, 7))
    sns.heatmap(cm, annot=True, fmt='d', xticklabels=labels, yticklabels=labels, cmap='Blues')
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title('Confusion Matrix')
    plt.show()

    # ROC-AUC for each class
    plt.figure(figsize=(10, 7))
    for label in labels:
        true_binary = [1 if x == label else 0 for x in all_true_components]
        pred_binary = [1 if x == label else 0 for x in all_generated_components]
        fpr, tpr, _ = roc_curve(true_binary, pred_binary)
        auc = roc_auc_score(true_binary, pred_binary)
        plt.plot(fpr, tpr, label=f'{label} (AUC = {auc:.2f})')

    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC-AUC for Each Class')
    plt.legend()
    plt.grid(True)
    plt.show()

progressive_evaluation(model, data, tokenizer, max_len=50)

