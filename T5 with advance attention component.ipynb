#gdrive
from google.colab import drive
drive.mount('/content/drive')

## Libraries

!pip install transformers[torch] accelerate -U

!pip install torch torchvision transformers

!pip install rouge_score


#imports
import os
import json
import torch
from transformers import T5Tokenizer, T5ForConditionalGeneration, BertTokenizer, BertModel, T5Config
from torch.utils.data import Dataset, DataLoader, random_split
from transformers import TrainingArguments, Trainer, DataCollatorForSeq2Seq
from sklearn.model_selection import train_test_split
import tensorflow as tf
import torch
import re

import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
from transformers import (BertTokenizer, BertModel, T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments, DataCollatorForSeq2Seq, get_linear_schedule_with_warmup, get_cosine_with_hard_restarts_schedule_with_warmup)
import torch.nn.functional as F
import torch.nn as nn
from transformers import AdamW
from transformers import get_linear_schedule_with_warmup
from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments

# from sklearn.metrics import accuracy_score, f1_score
# import numpy as np
# from datasets import load_metric


torch.cuda.empty_cache()

torch.cuda.is_available()

tf.test.gpu_device_name()

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

print(f"Using device: {device}")

## Simple

#lists
json_data=[]
prompts=[]

# load json-only
json_only= '/content/drive/My Drive/Implementation/Final Thesis/json_only.jsonl'

with open(json_only, 'r', encoding='utf-8') as f:
  for line in f:
    json_data.append(json.loads(line))

# load prompt only
prompt_only= '/content/drive/My Drive/Implementation/Final Thesis/prompt_only.txt'

with open(prompt_only, 'r', encoding='utf-8') as f:
  prompts =[line.strip() for line in f]

print(f"Number of prompts: {len(prompts)}")
print(f"Number of JSON entries: {len(json_data)}")

# Example of a JSON entry
print("Example JSON entry:")
print(json.dumps(json_data[0], indent=2))

class UIDataset(Dataset):
   """ class for loading paired dataset """

  def __init__(self, prompts, json_data):
    """method to initialize the class
    Parameters:
    prompts (list): list of prompts
    json_data (list): list of JSON data
    """
    #initialise prompts
    self.prompts= prompts
    #initialise json data
    self.json_data= json_data
    #loading bert tokenizer
    self.bert_tokenizer= BertTokenizer.from_pretrained('bert-base-uncased')
    self.t5_tokenizer= T5Tokenizer.from_pretrained('t5-small')
    #bert model
    self.bert_model= BertModel.from_pretrained('bert-base-uncased').to(device)

  def __len__(self):
    """ method to return the length of the dataset """
    return len(self.prompts)

  def __getitem__(self, idx):
    """ method to return the item at the given index
    Parameters:
    idx (int): index of the item
    Returns:
    dict: dictionary containing the input_ids, attention_mask, labels, and bert_embeddings
    """
    #indices of promtp and json
    prompt= self.prompts[idx]
    json_entry =json.dumps(self.json_data[idx])

    #bert used to convert prompts to tokens
    inputs= self.bert_tokenizer(prompt, return_tensors='pt', padding='max_length', truncation=True, max_length=512).to(device)
    #bert embeddings generated without gradient
    with torch.no_grad():
        outputs =self.bert_model(**inputs)
    #average last hidden states from bert to get a single embedding vector for the prompt
    bert_embeddings= outputs.last_hidden_state.mean(dim=1).squeeze()

    # check for NaN in embeddings
    if torch.isnan(bert_embeddings).any():
      print(f"NaN detected in BERT embeddings at index {idx}")

    #t5 tokenizer to conver prompt to tokens
    t5_inputs= self.t5_tokenizer(prompt, return_tensors='pt', padding='max_length', truncation=True, max_length=512)
    #t5 tokenizer to convert json to tokens
    targets =self.t5_tokenizer(json_entry, return_tensors='pt', padding='max_length', truncation=True, max_length=512)

    return {
      'input_ids':t5_inputs['input_ids'].squeeze(),
      'attention_mask':t5_inputs['attention_mask'].squeeze(),
      'labels': targets['input_ids'].squeeze(),
      'bert_embeddings':bert_embeddings
    }

dataset= UIDataset(prompts, json_data)
#split the dataset
total_size= len(dataset)
train_size= int(0.8 * total_size)
val_size= int(0.1 * total_size)
test_size= total_size - train_size - val_size

train_dataset, val_dataset, test_dataset= random_split(dataset, [train_size, val_size, test_size])

#dataloaders
train_loader= DataLoader(train_dataset, batch_size=4, shuffle=True)
val_loader= DataLoader(val_dataset, batch_size=4)
test_loader= DataLoader(test_dataset, batch_size=4)


#tokenizers and models bert
bert_tokenizer =BertTokenizer.from_pretrained('bert-base-uncased')
bert_model= BertModel.from_pretrained('bert-base-uncased')


#tokenizers and models t5
t5_tokenizer= T5Tokenizer.from_pretrained('t5-small')
t5_model =T5ForConditionalGeneration.from_pretrained('t5-small')

#move to device
device= 'cuda' if torch.cuda.is_available() else 'cpu'
bert_model.to(device)
t5_model.to(device)

# Training arguments
training_args= TrainingArguments(
  output_dir='/content/drive/My Drive/Implementation/Final Thesis/FYP/JSON generation/T5 CROSS SIMPLE',
  num_train_epochs=3,
  #training batch size
  per_device_train_batch_size=4,
  #evaluation batch size
  per_device_eval_batch_size=4,
  save_steps=500,
  save_total_limit=2,
  logging_steps=10,
  eval_steps=500,
  evaluation_strategy="steps",
  prediction_loss_only=True,
  load_best_model_at_end=True,
  fp16=torch.cuda.is_available(),
  #not push to HF
  push_to_hub=False
)


#DATALOADER for seq2seq
data_collator= DataCollatorForSeq2Seq(tokenizer=t5_tokenizer, model=t5_model)

class T5WithCrossAttention(T5ForConditionalGeneration):
  """ class for T5 model with cross attention """
  def __init__(self, config):
    """ method to initialize the class
    Parameters:
    config (dict): configuration of the model
    """
    super().__init__(config)
    #initialise multi head attention layer for cross attention
    self.cross_attention =torch.nn.MultiheadAttention(embed_dim=config.d_model, num_heads=8)
    #initialise linear layer to put out bert embeddings to t5 hidden state dimension
    self.bert_proj= torch.nn.Linear(bert_model.config.hidden_size, config.d_model)

  def forward(self, input_ids, attention_mask=None, decoder_input_ids=None, labels=None, bert_embeddings=None):
    """ method to forward the input
    Parameters:
    input_ids (tensor): input tokens
    attention_mask (tensor): attention mask
    decoder_input_ids (tensor): decoder input tokens
    labels (tensor): labels
    bert_embeddings (tensor): bert embeddings
    Returns:
    tuple: loss and logits
    """
    #pass inputs through t5 encoder
    outputs= self.encoder(input_ids=input_ids,attention_mask=attention_mask,)

    hidden_states= outputs[0]

    #bert embeddings to match the T5 hidden state dimension
    bert_embeddings= self.bert_proj(bert_embeddings).unsqueeze(1).expand(-1, hidden_states.size(1), -1)

    #putting cross-attention between T5 embeddings and BERT embeddings
    hidden_states, _= self.cross_attention(hidden_states, bert_embeddings, bert_embeddings)

    #pass inputs through t5 decoder
    decoder_outputs= self.decoder(input_ids=decoder_input_ids,encoder_hidden_states=hidden_states,encoder_attention_mask=attention_mask,)

    sequence_output = decoder_outputs[0]
    lm_logits = self.lm_head(sequence_output)

    loss = None
    if labels is not None:
      loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100)
      #flatten logits and labels for loss computation
      lm_logits = lm_logits.view(-1, lm_logits.size(-1))
      #labels
      labels = labels.view(-1)
      if lm_logits.size(0)== labels.size(0):
        loss = loss_fct(lm_logits, labels)
      else:
        print(f"Shape mismatch between logits and labels: {lm_logits.size()} vs {labels.size()}")

    return loss, lm_logits


# def custom_train(trainer, num_train_epochs):
#   for epoch in range(num_train_epochs):
#     trainer.train()
#     print(f"Epoch {epoch+1}/{num_train_epochs} completed")
#     eval_results = trainer.evaluate()
#     print(f"Validation results: {eval_results}")


# t5_model = T5WithCrossAttention.from_pretrained('t5-small').to(device)

#Trainer instance
trainer = Trainer(
  model=t5_model,
  args=training_args,
  train_dataset=train_dataset,
  eval_dataset=val_dataset,
  data_collator=data_collator
)

# custom_train(trainer, training_args.num_train_epochs)

#path
checkpoint_path= "/content/drive/My Drive/Implementation/Final Thesis/FYP/UpdatedResults/checkpoint-16500"

#LOAD
loaded_model= T5ForConditionalGeneration.from_pretrained(checkpoint_path).to(device)

original_model_path= "t5-small"

#LOAD TOKENIZER
loaded_tokenizer= T5Tokenizer.from_pretrained(original_model_path)

def generate_json_from_description(description, model, tokenizer, max_length=512):
  """ method to generate JSON from description
  Parameters:
  description (str): description
  model (T5ForConditionalGeneration): model
  tokenizer (T5Tokenizer): tokenizer
  max_length (int): maximum length of the generated JSON
  Returns:
  str: generated JSON
  """
  #tokenize input description
  input_ids= tokenizer(description, return_tensors="pt", padding="max_length", truncation=True, max_length=max_length).input_ids.to(device)
  #generate JSON output
  generated_ids= model.generate(input_ids, max_length=max_length, num_beams=5, early_stopping=True)
  #decode generated tokens to JSON string
  generated_json =tokenizer.decode(generated_ids[0], skip_special_tokens=True)
  return generated_json

description="Create a professional label with a drop shadow effect"

generated_json= generate_json_from_description(description, loaded_model, loaded_tokenizer)


print(generated_json)

# save_path= "/content/drive/My Drive/Implementation/Final Thesis/FYP/UpdatedResults/unfinished_model"

# if not os.path.exists(save_path):
#   os.makedirs(save_path)

# loaded_model.save_pretrained(save_path)
# loaded_tokenizer.save_pretrained(save_path)



## Training again1 with pairs

# Load the paired dataset
pairs_file_1= '/content/drive/My Drive/Implementation/Final Thesis/FYP/Data/t5_json_description_pairs.jsonl'



data_pairs_1= []


with open(pairs_file_1, 'r', encoding='utf-8') as f:
  for line in f:
    data_pairs_1.append(json.loads(line))


json_data_1 =[pair['input'] for pair in data_pairs_1]


prompts_1= [pair['output'] for pair in data_pairs_1]


print(f"Number of pairs: {len(data_pairs_1)}")


print(f"Number of jsno: {len(json_data_1)}")


print(f"Number of description: {len(prompts_1)}")


for i in range(2):
  print(f"JSON: {json_data_1[i]}")
  print(f"Description: {prompts_1[i]}")


class UIDataset(Dataset):
  """ class for loading paired dataset """

  def __init__(self, prompts, json_data):
    """method to initialize the class
    Parameters:
    prompts (list): list of prompts
    json_data (list): list of JSON data
    """
    #initialise prompts
    self.prompts= prompts
    #initialise json data
    self.json_data=json_data
    #loading bert tokenizer
    self.bert_tokenizer= BertTokenizer.from_pretrained('bert-base-uncased')
    self.t5_tokenizer =T5Tokenizer.from_pretrained('t5-small')
    #bert model
    self.bert_model= BertModel.from_pretrained('bert-base-uncased').to(device)

  def __len__(self):
    """ method to return the length of the dataset """
    return len(self.prompts)

  def __getitem__(self, idx):
    """ method to return the item at the given index
    Parameters:
    idx (int): index of the item
    Returns:
    dict: dictionary containing the input_ids, attention_mask, labels, and bert_embeddings
    """
    #indices of promtp and json
    prompt= self.prompts[idx]
    json_entry =json.dumps(self.json_data[idx])

    #bert used to convert prompts to tokens
    inputs= self.bert_tokenizer(prompt, return_tensors='pt', padding='max_length', truncation=True, max_length=512).to(device)
    #bert embeddings generated without gradient
    with torch.no_grad():
      outputs = self.bert_model(**inputs)
    #average last hidden states from bert to get a single embedding vector for the prompt
    bert_embeddings= outputs.last_hidden_state.mean(dim=1).squeeze()

    # check for NaN in embeddings
    if torch.isnan(bert_embeddings).any():
      print(f"NaN detected in BERT embeddings at index {idx}")

    #t5 tokenizer to conver prompt to tokens
    t5_inputs= self.t5_tokenizer(prompt, return_tensors='pt', padding='max_length', truncation=True, max_length=512)
    #t5 tokenizer to convert json to tokens
    targets = self.t5_tokenizer(json_entry, return_tensors='pt', padding='max_length', truncation=True, max_length=512)

    return {
      'input_ids':t5_inputs['input_ids'].squeeze(),
      'attention_mask':t5_inputs['attention_mask'].squeeze(),
      'labels': targets['input_ids'].squeeze(),
      'bert_embeddings': bert_embeddings
    }

# Split the dataset
dataset= UIDataset(prompts_1, json_data_1)
total_size =len(dataset)
train_size= int(0.8 * total_size)
val_size =int(0.1 * total_size)
test_size= total_size - train_size - val_size

train_dataset, val_dataset, test_dataset= random_split(dataset, [train_size, val_size, test_size])

#dataloaders
train_loader= DataLoader(train_dataset, batch_size=4, shuffle=True)
val_loader= DataLoader(val_dataset, batch_size=4)
test_loader= DataLoader(test_dataset, batch_size=4)


#tokenizers and models bert
bert_tokenizer =BertTokenizer.from_pretrained('bert-base-uncased')
bert_model =BertModel.from_pretrained('bert-base-uncased')


#tokenizers and models t5
t5_tokenizer= T5Tokenizer.from_pretrained('t5-small')
t5_model= T5ForConditionalGeneration.from_pretrained('t5-small')

#move to device
device= 'cuda' if torch.cuda.is_available() else 'cpu'
bert_model.to(device)
t5_model.to(device)

epochs=10

# Training arguments
training_args= TrainingArguments(
  output_dir='/content/drive/My Drive/Implementation/Final Thesis/FYP/JSON generation/T5 CROSS SIMPLE',
  num_train_epochs=epochs,
  #training batch size
  per_device_train_batch_size=16,
  #evaluation batch size
  per_device_eval_batch_size=16,
  save_steps=500,
  save_total_limit=2,
  logging_steps=10,
  eval_steps=500,
  evaluation_strategy="steps",
  prediction_loss_only=True,
  load_best_model_at_end=True,
  fp16=torch.cuda.is_available(),
  #not push to HF
  push_to_hub=False
)


data_collator= DataCollatorForSeq2Seq(tokenizer=t5_tokenizer, model=t5_model)

class T5WithCrossAttention(T5ForConditionalGeneration):
  """ class for T5 model with cross attention """
  def __init__(self, config):
    """ method to initialize the class
    Parameters:
    config (dict): configuration of the model
    """
    super().__init__(config)
    #initialise multi head attention layer for cross attention
    self.cross_attention= torch.nn.MultiheadAttention(embed_dim=config.d_model, num_heads=8)
    #initialise linear layer to put out bert embeddings to t5 hidden state dimension
    self.bert_proj = torch.nn.Linear(bert_model.config.hidden_size, config.d_model)

  def forward(self, input_ids, attention_mask=None, decoder_input_ids=None, labels=None, bert_embeddings=None):
    """ method to forward the input
    Parameters:
    input_ids (tensor): input tokens
    attention_mask (tensor): attention mask
    decoder_input_ids (tensor): decoder input tokens
    labels (tensor): labels
    bert_embeddings (tensor): bert embeddings
    Returns:
    tuple: loss and logits
    """
    #pass inputs through t5 encoder
    outputs= self.encoder(input_ids=input_ids,attention_mask=attention_mask,)
    hidden_states= outputs[0]

    #bert embeddings to match the T5 hidden state dimension
    bert_embeddings= self.bert_proj(bert_embeddings).unsqueeze(1).expand(-1, hidden_states.size(1), -1)

    #putting cross-attention between T5 embeddings and BERT embeddings
    hidden_states, _= self.cross_attention(hidden_states, bert_embeddings, bert_embeddings)

    #pass inputs through t5 decoder
    decoder_outputs= self.decoder(input_ids=decoder_input_ids,encoder_hidden_states=hidden_states,encoder_attention_mask=attention_mask,)

    sequence_output= decoder_outputs[0]
    lm_logits= self.lm_head(sequence_output)

    loss = None
    if labels is not None:
      loss_fct= torch.nn.CrossEntropyLoss(ignore_index=-100)
      #flatten logits and labels for loss computation
      lm_logits= lm_logits.view(-1, lm_logits.size(-1))
      #labels
      labels =labels.view(-1)
      if lm_logits.size(0)== labels.size(0):
        loss = loss_fct(lm_logits, labels)
      else:
        print(f"Shape mismatch between logits and labels: {lm_logits.size()} vs {labels.size()}")

    return loss, lm_logits


#Trainer instance
trainer = Trainer(
  model=t5_model,
  args=training_args,
  train_dataset=train_dataset,
  eval_dataset=val_dataset,
  data_collator=data_collator
)

trainer.train()

#path
checkpoint_path= "/content/drive/My Drive/Implementation/Final Thesis/FYP/JSON generation/T5 CROSS SIMPLE.t5_cross_model.pth"

#save for future
torch.save(t5_model.state_dict(), checkpoint_path)

#LOAD
t5_model= T5ForConditionalGeneration.from_pretrained('t5-small')
t5_tokenizer= T5Tokenizer.from_pretrained('t5-small')

t5_model.load_state_dict(torch.load(checkpoint_path))

t5_model.to(device)

Retrain

#train again
trainer = Trainer(
  model=t5_model,
  args=training_args,
  train_dataset=train_dataset,
  eval_dataset=val_dataset,
  data_collator=data_collator
)

#resume from checkpoint
trainer.train(resume_from_checkpoint=True)

def generate_json_from_description(description, model, tokenizer, max_length=512):
  """method to generate JSON from description
  Parameters:
  description (str): description
  model (T5ForConditionalGeneration): model
  tokenizer (T5Tokenizer): tokenizer
  max_length (int): maximum length of the generated JSON
  Returns:
  str: generated JSON"""
  #tokenize input description
  input_ids= tokenizer(description, return_tensors="pt", padding="max_length", truncation=True, max_length=max_length).input_ids.to(device)
  #generate JSON output
  generated_ids= model.generate(input_ids, max_length=max_length, num_beams=5, early_stopping=True)
  #decode generated tokens to JSON string
  generated_json= tokenizer.decode(generated_ids[0], skip_special_tokens=True)
  return generated_json

description="Create a professional button with a state of hover"

generated_json= generate_json_from_description(description, t5_model, t5_tokenizer)


print(generated_json)

## Nested

#lists
json_data=[]
prompts=[]

# load json-only
json_only_input= '/content/drive/My Drive/Implementation/Final Thesis/FYP/Data/nested_dataset.jsonl'

# json_only= '/content/drive/My Drive/Implementation/Final Thesis/FYP/Data/nested_dataset_5.jsonl'

jsons = []

# with open(json_only_input, 'r', encoding='utf-8') as f:
#   for line in f:
#     json_data = json.loads(line)

#     for _ in range(5):
#         jsons.append(json_data)

with open(json_only, 'w', encoding='utf-8') as f:
  for data in jsons:
    f.write(json.dumps(data) + '\n')

# load prompt only
prompt_only='/content/drive/My Drive/Implementation/Final Thesis//FYP/Data/t5_descriptions.txt'

with open(prompt_only, 'r', encoding='utf-8') as f:
  prompts=[line.strip() for line in f]

print(f"Number of prompts: {len(prompts)}")
print(f"Number of JSON entries: {len(jsons)}")

if len(prompts)!= len(jsons):
    raise ValueError("The number of prompts and JSON entries must be the same")

class UIDataset(Dataset):
  """ class for loading paired dataset """
  def __init__(self, prompts, json_data):
    """ method to initialize the class
    Parameters:
    prompts (list): list of prompts
    json_data (list): list of JSON data
    """
    #initialise prompts
    self.prompts = prompts
    #initialise json data
    self.json_data = json_data
    #loading bert tokenizer
    self.bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
    self.t5_tokenizer = T5Tokenizer.from_pretrained('t5-small')
    #bert model
    self.bert_model = BertModel.from_pretrained('bert-base-uncased').to(device)

  def __len__(self):
    """ method to return the length of the dataset """
    return len(self.prompts)

  def __getitem__(self, idx):
    """ method to return the item at the given index
    Parameters:
    idx (int): index of the item
    Returns:
    dict: dictionary containing the input_ids, attention_mask, labels, and bert_embeddings
    """
    #indices of prompt and json
    prompt = self.prompts[idx]
    json_entry = json.dumps(self.json_data[idx])

    #bert used to convert prompts to tokens
    inputs = self.bert_tokenizer(prompt, return_tensors='pt', padding='max_length', truncation=True, max_length=512).to(device)
     #bert embeddings generated without gradient
    with torch.no_grad():
        outputs = self.bert_model(**inputs)
    #average last hidden states from bert to get a single embedding vector for the prompt
    bert_embeddings = outputs.last_hidden_state.mean(dim=1).squeeze()

    # check for NaN in embeddings
    if torch.isnan(bert_embeddings).any():
      print(f"NaN detected in BERT embeddings at index {idx}")

     #t5 tokenizer to conver prompt to tokens
    t5_inputs = self.t5_tokenizer(prompt, return_tensors='pt', padding='max_length', truncation=True, max_length=512)
    #t5 tokenizer to convert json to tokens
    targets = self.t5_tokenizer(json_entry, return_tensors='pt', padding='max_length', truncation=True, max_length=512)

    return {
      'input_ids': t5_inputs['input_ids'].squeeze(),
      'attention_mask': t5_inputs['attention_mask'].squeeze(),
      'labels': targets['input_ids'].squeeze(),
      'bert_embeddings': bert_embeddings
    }

# Split the dataset
dataset= UIDataset(prompts, jsons)
total_size= len(dataset)
train_size =int(0.8 * total_size)
val_size= int(0.1 * total_size)
test_size = total_size - train_size - val_size

train_dataset, val_dataset, test_dataset= random_split(dataset, [train_size, val_size, test_size])

#dataloaders
train_loader= DataLoader(train_dataset, batch_size=4, shuffle=True)
val_loader= DataLoader(val_dataset, batch_size=4)
test_loader= DataLoader(test_dataset, batch_size=4)


#tokenizers and models bert
bert_tokenizer= BertTokenizer.from_pretrained('bert-base-uncased')
bert_model= BertModel.from_pretrained('bert-base-uncased')


#tokenizers and models t5
t5_tokenizer= T5Tokenizer.from_pretrained('t5-small')
t5_model= T5ForConditionalGeneration.from_pretrained('t5-small')

#move to device
device= 'cuda' if torch.cuda.is_available() else 'cpu'
bert_model.to(device)
t5_model.to(device)

# Training arguments
training_args= TrainingArguments(
  output_dir='/content/drive/My Drive/Implementation/Final Thesis/FYP/JSON generation/T5 CROSS NESTED',
  num_train_epochs=5,
  #training batch size
  per_device_train_batch_size=16,
  #evaluation batch size
  per_device_eval_batch_size=16,
  save_steps=500,
  save_total_limit=2,
  logging_steps=10,
  eval_steps=500,
  evaluation_strategy="steps",
  prediction_loss_only=True,
  load_best_model_at_end=True,
  fp16=torch.cuda.is_available(),
  #not push to HF
  push_to_hub=False
)


#datacollator for seq2seq
data_collator = DataCollatorForSeq2Seq(tokenizer=t5_tokenizer, model=t5_model)

class T5WithCrossAttention(T5ForConditionalGeneration):
  """ class for T5 model with cross attention """
  def __init__(self, config):
    """ method to initialize the class
    Parameters:
    config (dict): configuration of the model
    """
    super().__init__(config)
    #initialise multi head attention layer for cross attention
    self.cross_attention = torch.nn.MultiheadAttention(embed_dim=config.d_model, num_heads=8)
    #initialise linear layer to put out bert embeddings to t5 hidden state dimension
    self.bert_proj = torch.nn.Linear(bert_model.config.hidden_size, config.d_model)

  def forward(self, input_ids, attention_mask=None, decoder_input_ids=None, labels=None, bert_embeddings=None):
    """ method to forward the input
    Parameters:
    input_ids (tensor): input tokens
    attention_mask (tensor): attention mask
    decoder_input_ids (tensor): decoder input tokens
    labels (tensor): labels
    bert_embeddings (tensor): bert embeddings
    Returns:
    tuple: loss and logits
    """
    #pass inputs through t5 encoder
    outputs= self.encoder(input_ids=input_ids,attention_mask=attention_mask,)
    hidden_states= outputs[0]

    #bert embeddings to match the T5 hidden state dimension
    bert_embeddings = self.bert_proj(bert_embeddings).unsqueeze(1).expand(-1, hidden_states.size(1), -1)

    #putting cross-attention between T5 embeddings and BERT embeddings
    hidden_states, _ = self.cross_attention(hidden_states, bert_embeddings, bert_embeddings)

    #pass inputs through t5 decoder
    decoder_outputs= self.decoder(input_ids=decoder_input_ids,encoder_hidden_states=hidden_states,encoder_attention_mask=attention_mask,)

    sequence_output = decoder_outputs[0]
    lm_logits = self.lm_head(sequence_output)

    loss= None
    if labels is not None:
      loss_fct= torch.nn.CrossEntropyLoss(ignore_index=-100)
      #flatten logits and labels for loss computation
      lm_logits = lm_logits.view(-1, lm_logits.size(-1))
      #labels
      labels= labels.view(-1)
      if lm_logits.size(0)== labels.size(0):
        loss = loss_fct(lm_logits, labels)
      else:
        print(f"Shape mismatch between logits and labels: {lm_logits.size()} vs {labels.size()}")

    return loss, lm_logits


#Trainer instance
trainer = Trainer(
  model=t5_model,
  args=training_args,
  train_dataset=train_dataset,
  eval_dataset=val_dataset,
  data_collator=data_collator
)

trainer.train()

#path
model_save_path= '/content/drive/My Drive/Implementation/Final Thesis/FYP/JSON generation/T5 CROSS NESTED'

#save
t5_model.save_pretrained(model_save_path)

t5_tokenizer.save_pretrained(model_save_path)

def load_model_and_tokenizer(model_path):
  """ method to load the model and tokenizer
  Parameters:
  model_path (str): path to the model
  Returns:
  tuple: model and tokenizer
  """
  model = T5ForConditionalGeneration.from_pretrained(model_path)
  tokenizer = T5Tokenizer.from_pretrained(model_path)
  return model, tokenizer

model, tokenizer = load_model_and_tokenizer(model_save_path)

def generate_json(prompt, model, tokenizer, max_length=1024):
  """method to generate json from prompt
  Parameters:
  prompt (str): prompt to generate json
  model (T5ForConditionalGeneration): model to generate json
  tokenizer (T5Tokenizer): tokenizer to generate json
  max_length (int): maximum length of the generated json
  Returns:
  str: generated json
  """
  device = 'cuda' if torch.cuda.is_available() else 'cpu'
  model.to(device)
  #tokenize input description
  inputs= tokenizer(prompt, return_tensors='pt', padding='max_length', truncation=True, max_length=max_length).to(device)
   #generate JSON output
  outputs= model.generate(inputs['input_ids'], max_length=max_length,num_beams=5,early_stopping=True,temperature=0)
  #decode generated tokens to JSON string
  generated_json_str= tokenizer.decode(outputs[0], skip_special_tokens=True)
  return generated_json_str

description="Create a trendy icon-button with a state of hover"

generated_json= generate_json(description, model, tokenizer)


print(generated_json)

