#gdrive connect
from google.colab import drive
drive.mount('/content/drive')

pip install transformers[torch]

pip install accelerate -U

!pip install torch torchvision transformers

!pip install rouge-score nltk

!pip install Datasets

#imports
import json
from sklearn.model_selection import train_test_split
import torch
from torch.utils.data import DataLoader, Dataset
from datasets import Dataset
import torch.nn as nn
import torch.optim as optim
import tensorflow as tf
import torch.nn.functional as F
import pandas as pd
from transformers import BertTokenizer
# from transformers import GPT2Tokenizer, GPT2LMHeadModel
import os
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
from collections import Counter
from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments


!nvidia-smi

torch.cuda.is_available()

tf.test.gpu_device_name()

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

print(f"Using device: {device}")

# #loading the dataset
# def load_synthetic_data(file_path):
#     data = []
#     with open(file_path, 'r') as f:
#         for line in f:
#             data.append(json.loads(line.strip()))
#     return data

# file_path = '/content/drive/My Drive/fyp/FYP/synthetic_pairs.jsonl'

# data = load_synthetic_data(file_path)

# print(json.dumps(data[:2], indent=2))

#loading the json dataset
def load_json_data(file_path):
  with open(file_path, 'r') as f:
    data = [json.loads(line.strip()) for line in f]
  return data

#loading the description dataset
def load_text_data(file_path):
  with open(file_path, 'r') as f:
    data = [line.strip() for line in f]
  return data

# from numba import cuda

# cuda.select_device(0) # choosing second GPU

# cuda.close()

#paths to the datasets
json_file_path = '/content/drive/My Drive/Implementation/Final Thesis/FYP/Data/json_data.jsonl'
text_file_path = '/content/drive/My Drive/Implementation/Final Thesis/FYP/Data/descriptions_data.txt'

json_data = load_json_data(json_file_path)
descriptions = load_text_data(text_file_path)

print(f"Length of JSON data: {len(json_data)}")
print(f"Length of description data: {len(descriptions)}")


assert len(json_data) == len(descriptions), "Mismatched lengths of JSON and description data"

data = {'input': [json.dumps(json_data[i]) for i in range(len(json_data))], 'output': descriptions}
# data = [{'json':json.dumps(json_data[i]), 'description':descriptions[i]} for i in range(len(json_data))]

df = pd.DataFrame(data)

print(f"Length of combined data: {len(df)}")
print(f"First entry: {df.iloc[0]}")

#split90 10 shuffle with train test split
train_df, eval_df = train_test_split(df, test_size=0.1)

#to higgingface dataset. hugging face trainer api
train_dataset = Dataset.from_pandas(train_df)
eval_dataset = Dataset.from_pandas(eval_df)


#tokenizer-convert text to token ids
tokenizer = T5Tokenizer.from_pretrained('t5-small')

def tokenize_function(examples):
  """ method to tokenize the data
  Parameters
  examples- input to the tokenize
  """
  #tokenize the input with padding
  model_inputs = tokenizer(examples["input"], padding="max_length", truncation=True, max_length=256)
  #tokenize the output with padding
  with tokenizer.as_target_tokenizer():
    #tokenize output similar to input
    labels = tokenizer(examples["output"], padding="max_length", truncation=True, max_length=256)
    #adds the tokenized output text to the dictionary of model inputs
  model_inputs["labels"] = labels["input_ids"]
  return model_inputs


#tokenizing the datasets
tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)
tokenized_eval_dataset = eval_dataset.map(tokenize_function, batched=True)


#save
tokenized_train_dataset.save_to_disk("/content/drive/My Drive/Implementation/Final Thesis/FYP/Prompt generation/tokenized_train_dataset_t5")
tokenized_eval_dataset.save_to_disk("/content/drive/My Drive/Implementation/Final Thesis/FYP/Prompt generation/tokenized_eval_dataset_t5")

from datasets import load_from_disk

tokenized_train_dataset_path = "/content/drive/My Drive/Implementation/Final Thesis/FYP/Prompt generation/tokenized_train_dataset_t5"
tokenized_eval_dataset_path = "/content/drive/My Drive/Implementation/Final Thesis/FYP/Prompt generation/tokenized_eval_dataset_t5"


#load
tokenized_train_dataset = load_from_disk(tokenized_train_dataset_path)
tokenized_eval_dataset = load_from_disk(tokenized_eval_dataset_path)

#model
model = T5ForConditionalGeneration.from_pretrained('t5-small')


# epochs=8
epochs=12


#arguments
training_args = TrainingArguments(
  output_dir="/content/drive/My Drive/Implementation/Final Thesis/FYP/Prompt generation/T5 Model checkpoints",
  num_train_epochs=epochs,
  # per_device_train_batch_size=16
  per_device_train_batch_size=32,
  #accumulate gradients before performing a backward/update pass.
  gradient_accumulation_steps=4,
  warmup_steps=100,
  weight_decay=0.01,
  logging_dir='./logs',
  logging_steps=10,
  evaluation_strategy="epoch",
  save_strategy="epoch",
  load_best_model_at_end=False,
  fp16=True,
  dataloader_num_workers=4,
  disable_tqdm=False,
  save_total_limit=1,
)


trainer = Trainer(
  model=model,
  args=training_args,
  train_dataset=tokenized_train_dataset,
  eval_dataset=tokenized_eval_dataset,
  tokenizer=tokenizer,
)


trainer.train()

#save model
model.save_pretrained('./model')

tokenizer.save_pretrained('./tokenizer')



#save model and tokenizer
model.save_pretrained(t5model_path)
tokenizer.save_pretrained(t5tokenizert_path)

torch.save(model.state_dict(), t5model_path + "/pytorch_model.bin")

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model.to(device)

#load
model = T5ForConditionalGeneration.from_pretrained(t5model_path)
tokenizer = T5Tokenizer.from_pretrained(t5tokenizert_path)

trainer.train(resume_from_checkpoint=True)

t5model_path = "/content/drive/My Drive/Implementation/Final Thesis/FYP/Prompt generation/T5 Model"
t5tokenizert_path = "/content/drive/My Drive/Implementation/Final Thesis/FYP/Prompt generation/T5 Tokenizer"


trainer.save_model(t5model_path)
tokenizer.save_pretrained(t5tokenizert_path)
trainer.save_state()

#load again
model = T5ForConditionalGeneration.from_pretrained(t5model_path)
tokenizer = T5Tokenizer.from_pretrained(t5tokenizert_path)



# For resuming training
trainer_state = torch.load(os.path.join(t5model_path, "trainer_state.pt"))
trainer.load_state_dict(trainer_state)

def generate_prompts(model, tokenizer, json_input, device, num_return_sequences=5):
  """inference method
  Parameters
  model- modelinput
  tokenizer-tokenizer
  json_input- inpur from dataset
  device-cuda or not
  num_return_sequences- number of return sequences

  Returns
  prompts- generated prompts
  """
  model.to(device)
  model.eval()
  input_text = "generate prompt: " + json.dumps(json_input)
  input_ids = tokenizer(input_text, return_tensors="pt", padding=True, truncation=True, max_length=512).input_ids.to(device)

  generated_ids = model.generate(
    input_ids,
    max_length=100,
    num_return_sequences=num_return_sequences,
    #randomness
    temperature=1.5,
    #top k candidates
    top_k=50,
    #nucleus sampling
    top_p=0.95,
    #sampling
    do_sample=True,
    no_repeat_ngram_size=2,
    early_stopping=False,
  )

  prompts = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]
  return prompts

sample_json = {
    "variant_properties": {
      "component_name": "Label",
      "style": "Trendy",
      "subtype": "Dark",
      "variant_details": {
        "State": ["Enabled", "Focused"],
        "Size": ["Medium"]
      },
      "borderRadius": "8.0",
      "strokeWeight": "2.0",
      "effects": [
        {"type": "SHADOW", "color": "rgba(0,0,0,0.2)"}
      ]
  }
}

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

generated_prompts = generate_prompts(model, tokenizer, sample_json, device=device)

for i, prompt in enumerate(generated_prompts):
  print(f"Generated Prompt {i+1}: {prompt}")

# model = T5ForConditionalGeneration.from_pretrained(t5model_path)
# tokenizer = T5Tokenizer.from_pretrained(t5tokenizert_path)

json_data_path = "/content/drive/My Drive/Implementation/Final Thesis/FYP/simplecomponents.jsonl"

# Load JSON data
with open(json_data_path, 'r') as f:
  json_data = [json.loads(line.strip()) for line in f]


output_descriptions_path = "/content/drive/My Drive/Implementation/Final Thesis/FYP/Data/t5_descriptions.txt"

output_jsonl_path = "/content/drive/My Drive/Implementation/Final Thesis/FYP/Data/t5_generated_data.jsonl"


output_pairs_path = "/content/drive/My Drive/Implementation/Final Thesis/FYP/Data/t5_json_description_pairs.jsonl"

total_jsons = len(json_data)

all_prompts = []
pairs = []

with open(output_descriptions_path, 'w') as desc_file, open(output_jsonl_path, 'w') as jsonl_file, open(output_pairs_path, 'w') as pairs_file:
  for idx, json_input in enumerate(json_data):
    prompts = generate_prompts(model, tokenizer, json_input, device, num_return_sequences=5)
    for prompt in prompts:
      desc_file.write(prompt + '\n')

    json_record = {
      "json_input": json_input,
      "descriptions": prompts
    }
    all_prompts.append(json_record)

    for prompt in prompts:
      pair_record = {
        "input": json.dumps(json_input),
        "output": prompt
      }
      pairs.append(pair_record)

    jsonl_file.write(json.dumps(json_record) + '\n')
    for pair_record in pairs:
      pairs_file.write(json.dumps(pair_record) + '\n')

    # Print progress
    print(f"Processed {idx + 1}/{total_jsons} JSONs")

with open(output_jsonl_path, 'w') as f:
  for item in all_prompts:
    f.write(json.dumps(item) + '\n')

with open(output_pairs_path, 'w') as f:
  for pair in pairs:
    f.write(json.dumps(pair) + '\n')

## Stats

#load pairs dataset
pairs_file_path = "/content/drive/My Drive/Implementation/Final Thesis/FYP/Data/pairs_data.jsonl"

from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
from rouge_score import rouge_scorer
import numpy as np
import matplotlib.pyplot as plt

def load_json_description_pairs(file_path):
    data = []
    with open(file_path, 'r') as f:
        for line in f:
            try:
                data.append(json.loads(line.strip()))
            except json.JSONDecodeError as e:
                print(f"Error decoding JSON on line: {line.strip()}")
                print(f"Error: {e}")
    return data

generated_pairs = load_json_description_pairs(output_pairs_path)

def load_json_description_pairs(file_path):
    with open(file_path, 'r') as f:
        data = json.load(f)  # Load the entire file as a JSON array
    return data

dataset_pairs = load_json_description_pairs(pairs_file_path)


# Load JSON and description pairs from the file
def load_json_description_pairs(file_path):
    data = []
    with open(file_path, 'r') as f:
        for line in f:
            data.append(json.loads(line.strip()))
    return data

# Load the pairs
pairs_data = load_json_description_pairs(output_pairs_path)


print("Generated Pairs Sample: ", generated_pairs[:2])
print("Dataset Pairs Sample: ", dataset_pairs[:2])

sample_size = 50
sample_indices = np.random.choice(len(pairs_data), sample_size, replace=False)
sample_data = [pairs_data[i] for i in sample_indices]


def find_descriptions_for_json(json_input, dataset_pairs):
    """Find all descriptions for a given JSON input in the dataset pairs."""
    descriptions = []
    for pair in dataset_pairs:
        if pair['json'] == json.dumps(json_input):
            descriptions.append(pair['description'])
    return descriptions

def evaluate_model(model, tokenizer, generated_pairs, dataset_pairs, device):
    """Evaluates the model using BLEU and ROUGE scores."""
    smooth = SmoothingFunction().method4
    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)

    bleu_scores = []
    rouge_scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}

    for pair in generated_pairs:
        json_input = json.loads(pair['input'])
        reference_texts = find_descriptions_for_json(json_input, dataset_pairs)

        # Generate multiple prompts
        generated_prompts = generate_prompts(model, tokenizer, json_input, device=device, num_return_sequences=5)

        for generated_text in generated_prompts:
            for ref in reference_texts:
                # Calculate BLEU score
                bleu_score = sentence_bleu([ref.split()], generated_text.split(), smoothing_function=smooth)
                bleu_scores.append(bleu_score)

                # Calculate ROUGE score
                scores = scorer.score(generated_text, ref)
                for key in rouge_scores:
                    rouge_scores[key].append(scores[key].fmeasure)

    # Calculate average scores
    avg_bleu = np.mean(bleu_scores)
    avg_rouge = {key: np.mean(values) for key, values in rouge_scores.items()}

    return avg_bleu, avg_rouge, bleu_scores, rouge_scores

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


avg_bleu, avg_rouge, bleu_scores, rouge_scores = evaluate_model(model, tokenizer, generated_pairs, dataset_pairs, device)


print(f"Average BLEU score: {avg_bleu}")
print(f"Average ROUGE scores: {avg_rouge}")

### Accuracy

import random
import matplotlib.pyplot as plt
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, roc_curve, roc_auc_score
import seaborn as sns

from nltk.translate.bleu_score import corpus_bleu

def get_component_name_from_description(description):
    components = ['icon-button', 'button', 'label', 'input-field', 'menu', 'list-item']
    description_lower = description.lower().split()

    for idx, word in enumerate(description_lower):
        # Check for multi-word components
        for component in components:
            if word.startswith(component):
                return component

    return 'Unknown'


def calculate_component_metrics(true_components, generated_components):
    accuracy = accuracy_score(true_components, generated_components) * 100
    precision = precision_score(true_components, generated_components, average='weighted') * 100
    recall = recall_score(true_components, generated_components, average='weighted') * 100
    f1 = f1_score(true_components, generated_components, average='weighted') * 100
    return accuracy, precision, recall, f1


def evaluate_t5_model(model, dataset, tokenizer, sample_size=10, max_len=50, device="cpu"):
    sample_indices = random.sample(range(len(dataset)), sample_size)

    true_component_names = []
    generated_component_names = []

    for idx in sample_indices:
        json_input = dataset[idx]['input']
        true_description = dataset[idx]['output']  # assuming this returns a list of descriptions
        true_component_name = json.loads(json_input).get('variant_properties', {}).get('component_name', 'Unknown').lower()

        generated_description = generate_prompts(model, tokenizer, json.loads(json_input), device, num_return_sequences=1)[0]
        generated_component_name = get_component_name_from_description(generated_description)

        true_component_names.append(true_component_name)
        generated_component_names.append(generated_component_name)

        print(f"JSON Input: {json_input}")
        print(f"True Description: {true_description}")
        print(f"Generated Description: {generated_description}")
        print(f"True Component Name: {true_component_name}")
        print(f"Generated Component Name: {generated_component_name}")
        print("----")

    component_accuracy, component_precision, component_recall, component_f1 = calculate_component_metrics(true_component_names, generated_component_names)

    metrics = {
        'Component Accuracy (%)': component_accuracy,
        'Component Precision (%)': component_precision,
        'Component Recall (%)': component_recall,
        'Component F1 (%)': component_f1,
    }

    return metrics, true_component_names, generated_component_names


def progressive_t5_evaluation(model, dataset, tokenizer, max_len=50, device="cpu"):
    sample_sizes = [100, 200, 300, 400, 500]  # Different sample sizes for progressive evaluation
    accuracy_list = []
    precision_list = []
    recall_list = []
    f1_list = []

    all_true_components = []
    all_generated_components = []

    for sample_size in sample_sizes:
        metrics, true_components, generated_components = evaluate_t5_model(model, dataset, tokenizer, sample_size=sample_size, max_len=max_len, device=device)
        accuracy_list.append(metrics['Component Accuracy (%)'])
        precision_list.append(metrics['Component Precision (%)'])
        recall_list.append(metrics['Component Recall (%)'])
        f1_list.append(metrics['Component F1 (%)'])
        all_true_components.extend(true_components)
        all_generated_components.extend(generated_components)
        print(f"Sample Size: {sample_size}, Metrics: {metrics}")

    # Plotting
    plt.figure(figsize=(10, 6))
    plt.plot(sample_sizes, accuracy_list, label='Accuracy (%)', marker='o')
    plt.plot(sample_sizes, precision_list, label='Precision (%)', marker='o')
    plt.plot(sample_sizes, recall_list, label='Recall (%)', marker='o')
    plt.plot(sample_sizes, f1_list, label='F1 Score (%)', marker='o')
    plt.xlabel('Sample Size')
    plt.ylabel('Percentage')
    plt.title('Progressive Evaluation Metrics')
    plt.legend()
    plt.grid(True)
    plt.show()

    # Confusion Matrix
    labels = list(set(all_true_components))
    cm = confusion_matrix(all_true_components, all_generated_components, labels=labels)

    plt.figure(figsize=(10, 7))
    sns.heatmap(cm, annot=True, fmt='d', xticklabels=labels, yticklabels=labels, cmap='Blues')
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title('Confusion Matrix')
    plt.show()

    # ROC-AUC for each class
    plt.figure(figsize=(10, 7))
    for label in labels:
        true_binary = [1 if x == label else 0 for x in all_true_components]
        pred_binary = [1 if x == label else 0 for x in all_generated_components]
        fpr, tpr, _ = roc_curve(true_binary, pred_binary)
        auc = roc_auc_score(true_binary, pred_binary)
        plt.plot(fpr, tpr, label=f'{label} (AUC = {auc:.2f})')

    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC-AUC for Each Class')
    plt.legend()
    plt.grid(True)
    plt.show()



import random

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


# Assuming your T5 model is correctly loaded and the generate_prompts function is adapted for the T5 model.
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
progressive_t5_evaluation(model, df.to_dict('records'), tokenizer, max_len=50, device=device)
