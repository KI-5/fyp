

#gdrive
from google.colab import drive
drive.mount('/content/drive')

!pip install transformers[torch] accelerate -U

!pip install torch torchvision transformers

!pip install rouge_score

#imports
import os
import json
import torch
from transformers import T5Tokenizer, T5ForConditionalGeneration, BertTokenizer, BertModel, T5Config
from torch.utils.data import Dataset, DataLoader, random_split
from transformers import TrainingArguments, Trainer, DataCollatorForSeq2Seq
from sklearn.model_selection import train_test_split
import tensorflow as tf
import torch
import re

import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
from transformers import (BertTokenizer, BertModel, T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments, DataCollatorForSeq2Seq, get_linear_schedule_with_warmup, get_cosine_with_hard_restarts_schedule_with_warmup)
import torch.nn.functional as F
import torch.nn as nn
from transformers import AdamW
from transformers import get_linear_schedule_with_warmup
from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments

device = 'cuda' if torch.cuda.is_available() else 'cpu'

## LOAD

### Load for simple JSON

# Load the paired dataset
pairs_file_1= '/content/drive/My Drive/Implementation/Final Thesis/FYP/Data/t5_json_description_pairs.jsonl'



data_pairs_1 = []


with open(pairs_file_1, 'r', encoding='utf-8') as f:
    for line in f:
        data_pairs_1.append(json.loads(line))


json_data_1 = [pair['input'] for pair in data_pairs_1]


prompts_1 = [pair['output'] for pair in data_pairs_1]


print(f"Number of pairs: {len(data_pairs_1)}")


print(f"Number of jsno: {len(json_data_1)}")


print(f"Number of description: {len(prompts_1)}")


for i in range(2):
    print(f"JSON: {json_data_1[i]}")
    print(f"Description: {prompts_1[i]}")


### Load for nested

# load json-only
json_only_input= '/content/drive/My Drive/Implementation/Final Thesis/FYP/Data/nested_dataset.jsonl'

json_only= '/content/drive/My Drive/Implementation/Final Thesis/FYP/Data/nested_dataset_5.jsonl'

jsons = []

with open(json_only_input, 'r', encoding='utf-8') as f:
  for line in f:
    json_data = json.loads(line)
   #get 5 times
    for _ in range(5):
      jsons.append(json_data)

with open(json_only, 'w', encoding='utf-8') as f:
  for data in jsons:
    f.write(json.dumps(data) + '\n')

#load prompt only
prompt_only ='/content/drive/My Drive/Implementation/Final Thesis//FYP/Data/t5_descriptions.txt'

with open(prompt_only, 'r', encoding='utf-8') as f:
  prompts=[line.strip() for line in f]

print(f"Number of prompts: {len(prompts)}")
print(f"Number of JSON entries: {len(jsons)}")

## Simple T5 Simple JSON

### Model

model_save_path= '/content/drive/My Drive/Implementation/Final Thesis/FYP/JSON generation/T5 NORMAL SIMPLE/'

def load_model_and_tokenizer(model_path):
    model = T5ForConditionalGeneration.from_pretrained(model_path)
    tokenizer = T5Tokenizer.from_pretrained(model_path)
    return model, tokenizer

model, tokenizer = load_model_and_tokenizer(model_save_path)

def generate_json(prompt, model, tokenizer, max_length=1024):

    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    model.to(device)

    inputs = tokenizer(prompt, return_tensors='pt', padding='max_length', truncation=True, max_length=max_length).to(device)
    outputs = model.generate(
        inputs['input_ids'],
        max_length=max_length,
        num_beams=5,
        early_stopping=True,
        temperature=0
    )
    generated_json_str = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return generated_json_str

device = 'cuda' if torch.cuda.is_available() else 'cpu'
model.to(device)

### button

prompt_button_1= "Generate a Professional button with a state of hover."

generated_json= generate_json(prompt_button_1, model, tokenizer)

print("Generated JSON:", generated_json)

prompt_button_2= "Generate a Professional button with a size of small, state of default."

generated_json= generate_json(prompt_button_2, model, tokenizer)

print("Generated JSON:", generated_json)

prompt_button_3= "Generate a Basic button with DROP_SHADOW effects."

generated_json= generate_json(prompt_button_3, model, tokenizer)

print("Generated JSON:", generated_json)

prompt_button_4="Generate a Professional Button with a border radius of 10.0"

generated_json= generate_json(prompt_button_4, model, tokenizer)

print("Generated JSON:", generated_json)

prompt_button_5="Generate a Trendy Button"

generated_json= generate_json(prompt_button_5, model, tokenizer)

print("Generated JSON:", generated_json)

### Label

prompt_label_1= "Generate a Professional label with a state of hover."

generated_json= generate_json(prompt_label_1, model, tokenizer)

print("Generated JSON:", generated_json)

prompt_label_2= "Generate a casual label with a size of small."

generated_json= generate_json(prompt_label_2, model, tokenizer)

print("Generated JSON:", generated_json)

prompt_label_3= "Generate a label with DROP_SHADOW effects."

generated_json= generate_json(prompt_label_3, model, tokenizer)

print("Generated JSON:", generated_json)

prompt_label_4="Generate a default label with a border radius of 10.0"

generated_json= generate_json(prompt_label_4, model, tokenizer)

print("Generated JSON:", generated_json)

prompt_label_5="Generate a Professional label"

generated_json= generate_json(prompt_label_5, model, tokenizer)

print("Generated JSON:", generated_json)

### input field

prompt_input_1= "Generate a Basic input-field with a state of hover."

generated_json= generate_json(prompt_input_1, model, tokenizer)

print("Generated JSON:", generated_json)

prompt_input_2= "Generate a input-field."

generated_json= generate_json(prompt_input_2, model, tokenizer)

print("Generated JSON:", generated_json)

prompt_input_3= "Generate a Professional input-field with a state of small."

generated_json= generate_json(prompt_input_3, model, tokenizer)

print("Generated JSON:", generated_json)

prompt_input_4="Generate a default input field with a border radius of 10.0"

generated_json= generate_json(prompt_input_4, model, tokenizer)

print("Generated JSON:", generated_json)

prompt_input_5="Generate a playful input field with drop_shadow effects"

generated_json= generate_json(prompt_input_5, model, tokenizer)

print("Generated JSON:", generated_json)

### Menu

def standardize_prompt(prompt):
    # Define a dictionary for term mappings
    term_mappings = {
        "menu item": "menu",
        "menuitem": "menu",
        "menu list": "menu",
        "list item": "list-item",
        "listitem": "list-item",
        "input field": "input-field",
        "inputfield": "input-field",
        "iconbutton": "icon-button",
        "iconbutton": "icon-button",
        "button": "button",
        "label": "label"

    }
    for term, standard_term in term_mappings.items():
        prompt = prompt.replace(term, standard_term)

    return prompt

def generate_json(prompt, model, tokenizer, max_length=1024):
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    model.to(device)
    # Standardize the prompt
    prompt = standardize_prompt(prompt)

    # Tokenize the input and move to the correct device
    inputs = tokenizer(prompt, return_tensors='pt', padding='max_length', truncation=True, max_length=max_length).to(device)

    # Generate the output using beam search
    outputs = model.generate(
        inputs['input_ids'],
        max_length=max_length,
        num_beams=5,
        early_stopping=True,
        temperature=0
    )

    # Decode the generated tokens to a string
    generated_json_str = tokenizer.decode(outputs[0], skip_special_tokens=True)

    return generated_json_str

prompt_menu_1= "Generate a Basic menu with a state of hover."

generated_json= generate_json(prompt_menu_1, model, tokenizer)

print("Generated JSON:", generated_json)

prompt_menu_2= "Generate a menu."

generated_json= generate_json(prompt_menu_2, model, tokenizer)

print("Generated JSON:", generated_json)

prompt_menu_3= "Generate a Professional menu item with a small state."

generated_json= generate_json(prompt_menu_3, model, tokenizer)

print("Generated JSON:", generated_json)

prompt_menu_4="Generate a default menu item with a border radius of 10.0"

generated_json= generate_json(prompt_menu_4, model, tokenizer)

print("Generated JSON:", generated_json)

prompt_menu_5="Generate a playful menu list with drop_shadow effects"

generated_json= generate_json(prompt_menu_5, model, tokenizer)

print("Generated JSON:", generated_json)

### list-item



prompt_list_1= "Generate a Basic list item with a state of hover."

generated_json= generate_json(prompt_list_1, model, tokenizer)

print("Generated JSON:", generated_json)

prompt_list_2= "Generate a list item."

generated_json= generate_json(prompt_list_2, model, tokenizer)

print("Generated JSON:", generated_json)

prompt_list_3= "Generate a playful listitem with a state of hover."

generated_json= generate_json(prompt_list_3, model, tokenizer)

print("Generated JSON:", generated_json)

prompt_list_4="Generate a default listitem with a border radius of 10.0"

generated_json= generate_json(prompt_list_4, model, tokenizer)

print("Generated JSON:", generated_json)

prompt_list_5="Generate a trendy menu list with drop_shadow effects"

generated_json= generate_json(prompt_list_5, model, tokenizer)

print("Generated JSON:", generated_json)

### icon button

prompt_icon_1= "Generate a Basic icon button with a state of hover."

generated_json= generate_json(prompt_icon_1, model, tokenizer)

print("Generated JSON:", generated_json)

prompt_icon_2= "Generate a iconbutton."

generated_json= generate_json(prompt_icon_2, model, tokenizer)

print("Generated JSON:", generated_json)

prompt_icon_3= "Generate a playful iconbutton with a state of hover."

generated_json= generate_json(prompt_icon_3, model, tokenizer)

print("Generated JSON:", generated_json)

prompt_icon_4="Generate a default icon button with a border radius of 10.0"

generated_json= generate_json(prompt_icon_4, model, tokenizer)

print("Generated JSON:", generated_json)

prompt_icon_5="Generate a trendy icon button with a stroke weight of 2.0"

generated_json= generate_json(prompt_icon_5, model, tokenizer)

print("Generated JSON:", generated_json)

### Metrics

import json
from nltk.translate.bleu_score import sentence_bleu
from rouge_score import rouge_scorer
import nltk
import re
import random
import matplotlib.pyplot as plt

nltk.download('punkt')



def calculate_bleu(reference, hypothesis):
  reference_tokens = nltk.word_tokenize(reference)
  hypothesis_tokens = nltk.word_tokenize(hypothesis)
  return nltk.translate.bleu_score.sentence_bleu([reference_tokens], hypothesis_tokens)


def calculate_rouge(reference, hypothesis):
  scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)
  scores = scorer.score(reference, hypothesis)
  return scores['rouge1'].fmeasure, scores['rougeL'].fmeasure


from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score


def calculate_key_metrics(reference_json, generated_json_str):
  try:
    reference_data = json.loads(reference_json)
  except json.JSONDecodeError:
    print("Error decoding reference JSON")
    return 0, 0, 0, 0, 0  # default values if decoding fails

  # Check key phrases directly in the generated JSON string
  component_name_correct = reference_data.get('component_name', '') in generated_json_str
  style_correct = reference_data.get('style', '') in generated_json_str
  border_radius_correct = str(reference_data.get('border_radius', '')) in generated_json_str
  stroke_weight_correct = str(reference_data.get('stroke_weight', '')) in generated_json_str
  drop_shadow_correct = 'drop_shadow' in generated_json_str if 'drop_shadow' in reference_data else True

  return component_name_correct, style_correct, border_radius_correct, stroke_weight_correct, drop_shadow_correct

subset_size = 100

# Select a random subset of the dataset
subset_indices = random.sample(range(len(json_data_1)), subset_size)
subset_prompts = [prompts_1[i] for i in subset_indices]
subset_reference_jsons = [json_data_1[i] for i in subset_indices]

generated_jsons = [generate_json(prompt, model, tokenizer) for prompt in subset_prompts]

total_bleu, total_rouge1, total_rougeL = 0, 0, 0
true_component_names = []
predicted_component_names = []

for i, reference_json in enumerate(subset_reference_jsons):
  generated_json = generated_jsons[i]

  # Calculate BLEU score
  bleu_score = calculate_bleu(reference_json, generated_json)
  total_bleu += bleu_score

  # Calculate ROUGE score
  rouge1_score, rougeL_score = calculate_rouge(reference_json, generated_json)
  total_rouge1 += rouge1_score
  total_rougeL += rougeL_score


overall_component_correct, overall_style_correct = 0, 0
overall_border_radius_correct, overall_stroke_weight_correct, overall_drop_shadow_correct = 0, 0, 0


for i, reference_json in enumerate(subset_reference_jsons):
  generated_json = generated_jsons[i]

  # Calculate key metrics
  component_correct, style_correct, border_radius_correct, stroke_weight_correct, drop_shadow_correct = calculate_key_metrics(reference_json, generated_json)

  # Sum up the individual metrics for overall calculation
  overall_component_correct += component_correct
  overall_style_correct += style_correct
  overall_border_radius_correct += border_radius_correct
  overall_stroke_weight_correct += stroke_weight_correct
  overall_drop_shadow_correct += drop_shadow_correct

total_prompts = len(subset_prompts)
overall_component_correct /= total_prompts
overall_style_correct /= total_prompts
overall_border_radius_correct /= total_prompts
overall_stroke_weight_correct /= total_prompts
overall_drop_shadow_correct /= total_prompts


# Calculate overall BLEU and ROUGE scores
average_bleu = total_bleu / total_prompts
average_rouge1 = total_rouge1 / total_prompts
average_rougeL = total_rougeL / total_prompts


print(f"Overall Component Name Correct: {overall_component_correct}")
print(f"Overall Style Correct: {overall_style_correct}")
print(f"Overall Border Radius Correct: {overall_border_radius_correct}")
print(f"Overall Stroke Weight Correct: {overall_stroke_weight_correct}")
print(f"Overall Drop Shadow Correct: {overall_drop_shadow_correct}")
print(f"Average BLEU Score: {average_bleu}")
print(f"Average ROUGE-1 Score: {average_rouge1}")
print(f"Average ROUGE-L Score: {average_rougeL}")

metrics = ['BLEU', 'ROUGE-1', 'ROUGE-L']

values = [average_bleu, average_rouge1, average_rougeL]


plt.figure(figsize=(10, 5))
plt.bar(metrics, values, color='skyblue')
plt.xlabel('Metrics')
plt.ylabel('Scores')
plt.title('Average BLEU, ROUGE-1, and ROUGE-L scores')
plt.ylim(0, 1)
plt.show()

key_metrics = ['Component Name', 'Style', 'Border Radius', 'Stroke Weight', 'Drop Shadow']


correctness_values = [overall_component_correct, overall_style_correct, overall_border_radius_correct, overall_stroke_weight_correct, overall_drop_shadow_correct]


plt.figure(figsize=(10, 5))
plt.bar(key_metrics, correctness_values, color='lightgreen')
plt.xlabel('Key metrics')
plt.ylabel('Correctness ratio')
plt.title('Correctness of metrics for generated JSONs')
plt.ylim(0, 1)
plt.show()

bleu_scores = []
rouge1_scores = []
rougeL_scores = []


for i, reference_json in enumerate(subset_reference_jsons):
    generated_json = generated_jsons[i]
    reference_json_str = json.dumps(reference_json) if isinstance(reference_json, dict) else reference_json

    bleu_scores.append(calculate_bleu(reference_json_str, generated_json))
    rouge1_score, rougeL_score = calculate_rouge(reference_json_str, generated_json)
    rouge1_scores.append(rouge1_score)
    rougeL_scores.append(rougeL_score)

plt.figure(figsize=(15, 5))

plt.subplot(1, 3, 1)
plt.hist(bleu_scores, bins=20, color='skyblue', edgecolor='black')
plt.title('Distribution of BLEU scores')
plt.xlabel('BLEU scores')
plt.ylabel('Frequency')

plt.subplot(1, 3, 2)
plt.hist(rouge1_scores, bins=20, color='lightgreen', edgecolor='black')
plt.title('Distribution of ROUGE-1 scores')
plt.xlabel('ROUGE-1 scores')
plt.ylabel('Frequency')

plt.subplot(1, 3, 3)
plt.hist(rougeL_scores, bins=20, color='salmon', edgecolor='black')
plt.title('Distribution of ROUGE-L scores')
plt.xlabel('ROUGE-L scores')
plt.ylabel('Frequency')

plt.tight_layout()
plt.show()

def identify_component_name_in_json(json_str):
    # Extract the component_name value from the JSON string
    match = re.search(r'"component_name"\s*:\s*"([^"]+)"', json_str)
    if match:
        return match.group(1).replace("-", " ").lower()
    return None

def identify_component_name_in_description(description):
    components = ['icon-button', 'button', 'label', 'input-field', 'menu', 'list-item']
    description_lower = description.lower()

    # Normalize description by removing spaces and hyphens
    normalized_description = description_lower.replace(" ", "").replace("-", "")

    for component in components:
        normalized_component = component.replace("-", "")
        if normalized_component in normalized_description:
            return component

    return None

def calculate_component_accuracy(reference_str, generated_str):
    ref_component_name = identify_component_name_in_json(reference_str)
    gen_component_name = identify_component_name_in_json(generated_str)
    # Handle "icon" being equal to "icon button"
    if ref_component_name == "icon button" and gen_component_name == "icon":
        gen_component_name = "icon button"
    if ref_component_name is None or gen_component_name is None:
        return False, ref_component_name, gen_component_name
    return ref_component_name == gen_component_name, ref_component_name, gen_component_name


true_labels = []
predicted_labels = []
correct_component_predictions = 0


for i, reference_str in enumerate(subset_reference_jsons):
    generated_str = generated_jsons[i]

    # Print the JSON strings for debugging
    print(f"Dataset JSON: {reference_str}")
    print(f"Generated JSON: {generated_str}")

    # Calculate component accuracy
    is_correct, ref_component_name, gen_component_name = calculate_component_accuracy(reference_str, generated_str)

    # Only append non-None values
    if ref_component_name is not None and gen_component_name is not None:
        true_labels.append(ref_component_name)
        predicted_labels.append(gen_component_name)
        if is_correct:
            correct_component_predictions += 1

    # Print progress
    print(f"Description: {subset_prompts[i]}")
    print(f"Name in Dataset: {ref_component_name}")
    print(f"Name in Generated: {gen_component_name}")
    print("--------------------------------------------------")

# Replace None with 'none' to avoid type errors
true_labels = ['none' if label is None else label for label in true_labels]
predicted_labels = ['none' if label is None else label for label in predicted_labels]


accuracy = accuracy_score(true_labels, predicted_labels)
precision = precision_score(true_labels, predicted_labels, average='weighted', zero_division=0)
recall = recall_score(true_labels, predicted_labels, average='weighted', zero_division=0)
f1 = f1_score(true_labels, predicted_labels, average='weighted', zero_division=0)

print(f"Accuracy: {accuracy}")
print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"F1 Score: {f1}")

from sklearn.preprocessing import label_binarize
from sklearn.metrics import confusion_matrix, roc_curve, auc

# Generate confusion matrix
labels = ["button", "label", "input-field", "list-item", "menu", "icon button", "none"]
conf_matrix = confusion_matrix(true_labels, predicted_labels, labels=labels)


plt.figure(figsize=(10, 7))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

fpr = {}
tpr = {}
roc_auc = {}

# Binarize the labels for ROC calculation
true_binarized = label_binarize(true_labels, classes=labels)
pred_binarized = label_binarize(predicted_labels, classes=labels)


for i in range(true_binarized.shape[1]):
    fpr[i], tpr[i], _ = roc_curve(true_binarized[:, i], pred_binarized[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Plot ROC curve for each class
plt.figure(figsize=(10, 7))
colors = ['blue', 'green', 'red', 'cyan', 'magenta', 'yellow', 'orange']
for i, color in enumerate(colors):
    plt.plot(fpr[i], tpr[i], color=color, lw=2, label=f'ROC curve (area = {roc_auc[i]:.2f}) for class {labels[i]}')
plt.plot([0, 1], [0, 1], 'k--', lw=2)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curves')
plt.legend(loc="lower right")
plt.show()

## Simple T5 Complex JSON

### Model

#save to drive
model_save_path= '/content/drive/My Drive/Implementation/Final Thesis/FYP/JSON generation/T5 NESTED'

def load_model_and_tokenizer(model_path):
  """ method to load the model and tokenizer
  Parameters
  model_path: path to the model and tokenizer
  Return
  model: loaded model
  tokenizer: loaded tokenizer
  """
  model= T5ForConditionalGeneration.from_pretrained(model_path)
  tokenizer= T5Tokenizer.from_pretrained(model_path)
  return model,tokenizer

model, tokenizer = load_model_and_tokenizer(model_save_path)

def generate_json(prompt, model, tokenizer, max_length=1024):
  """ method to generate the json from the prompt
  Parameters
  prompt: prompt to generate the json
  model: model to generate the json
  tokenizer: tokenizer to generate the json
  max_length: maximum length of the json
  Return
  generated_json_str: generated json string
  """

  device= 'cuda' if torch.cuda.is_available() else 'cpu'
  model.to(device)
  #tokenizing prompt
  inputs= tokenizer(prompt, return_tensors='pt', padding='max_length', truncation=True, max_length=max_length).to(device)
  outputs =model.generate(inputs['input_ids'],max_length=max_length,num_beams=5,early_stopping=True, temperature=0)
  generated_json_str= tokenizer.decode(outputs[0], skip_special_tokens=True)
  return generated_json_str

### button

prompt_button_1= "Generate a Professional button with a state of hover."

generated_json= generate_json(prompt_button_1, model, tokenizer)

print("Generated JSON:", generated_json)

prompt_button_2= "Generate a Professional button with a size of small, state of default."

generated_json= generate_json(prompt_button_2, model, tokenizer)

print("Generated JSON:", generated_json)

prompt_button_3= "Generate a Basic button with DROP_SHADOW effects."

generated_json= generate_json(prompt_button_3, model, tokenizer)

print("Generated JSON:", generated_json)

prompt_button_4="Generate a Professional Button with a border radius of 10.0"

generated_json= generate_json(prompt_button_4, model, tokenizer)

print("Generated JSON:", generated_json)

prompt_button_5="Generate a Trendy Button"

generated_json= generate_json(prompt_button_5, model, tokenizer)

print("Generated JSON:", generated_json)

### Label

prompt_label_1= "Generate a Professional label with a state of hover."

generated_json= generate_json(prompt_label_1, model, tokenizer)

print("Generated JSON:", generated_json)

prompt_label_2= "Generate a casual label with a size of small."

generated_json= generate_json(prompt_label_2, model, tokenizer)

print("Generated JSON:", generated_json)

prompt_label_3= "Generate a label with DROP_SHADOW effects."

generated_json= generate_json(prompt_label_3, model, tokenizer)

print("Generated JSON:", generated_json)

prompt_label_4="Generate a default label with a border radius of 10.0"

generated_json= generate_json(prompt_label_4, model, tokenizer)

print("Generated JSON:", generated_json)

prompt_label_5="Generate a Professional label"

generated_json= generate_json(prompt_label_5, model, tokenizer)

print("Generated JSON:", generated_json)

### input field

prompt_input_1= "Generate a Basic input-field with a state of hover."

generated_json= generate_json(prompt_input_1, model, tokenizer)

print("Generated JSON:", generated_json)

prompt_input_2= "Generate a input-field."

generated_json= generate_json(prompt_input_2, model, tokenizer)

print("Generated JSON:", generated_json)

prompt_input_3= "Generate a Professional input-field with a state of small."

generated_json= generate_json(prompt_input_3, model, tokenizer)

print("Generated JSON:", generated_json)

prompt_input_4="Generate a default input field with a border radius of 10.0"

generated_json= generate_json(prompt_input_4, model, tokenizer)

print("Generated JSON:", generated_json)

prompt_input_5="Generate a playful input field with drop_shadow effects"

generated_json= generate_json(prompt_input_5, model, tokenizer)

print("Generated JSON:", generated_json)

### Menu

def standardize_prompt(prompt):
    # Define a dictionary for term mappings
    term_mappings = {
        "menu item": "menu",
        "menuitem": "menu",
        "menu list": "menu",
        "list item": "list-item",
        "listitem": "list-item",
        "input field": "input-field",
        "inputfield": "input-field",
        "iconbutton": "icon-button",
        "iconbutton": "icon-button",
        "button": "button",
        "label": "label"

    }
    for term, standard_term in term_mappings.items():
        prompt = prompt.replace(term, standard_term)

    return prompt

def generate_json(prompt, model, tokenizer, max_length=1024):

    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    model.to(device)

# Standardize the prompt
    prompt = standardize_prompt(prompt)
    inputs = tokenizer(prompt, return_tensors='pt', padding='max_length', truncation=True, max_length=max_length).to(device)
    outputs = model.generate(
        inputs['input_ids'],
        max_length=max_length,
        num_beams=5,
        early_stopping=True,
        temperature=0
    )
    generated_json_str = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return generated_json_str

prompt_menu_1= "Generate a Basic menu with a state of hover."

generated_json= generate_json(prompt_menu_1, model, tokenizer)

print("Generated JSON:", generated_json)

prompt_menu_2= "Generate a menu."

generated_json= generate_json(prompt_menu_2, model, tokenizer)

print("Generated JSON:", generated_json)

prompt_menu_3= "Generate a Professional menu item with a small state."

generated_json= generate_json(prompt_menu_3, model, tokenizer)

print("Generated JSON:", generated_json)

prompt_menu_4="Generate a default menu item with a border radius of 10.0"

generated_json= generate_json(prompt_menu_4, model, tokenizer)

print("Generated JSON:", generated_json)

prompt_menu_5="Generate a playful menu list with drop_shadow effects"

generated_json= generate_json(prompt_menu_5, model, tokenizer)

print("Generated JSON:", generated_json)

### list-item



prompt_list_1= "Generate a Basic list item with a state of hover."

generated_json= generate_json(prompt_list_1, model, tokenizer)

print("Generated JSON:", generated_json)

prompt_list_2= "Generate a list item."

generated_json= generate_json(prompt_list_2, model, tokenizer)

print("Generated JSON:", generated_json)

prompt_list_3= "Generate a playful listitem with a state of hover."

generated_json= generate_json(prompt_list_3, model, tokenizer)

print("Generated JSON:", generated_json)

prompt_list_4="Generate a default listitem with a border radius of 10.0"

generated_json= generate_json(prompt_list_4, model, tokenizer)

print("Generated JSON:", generated_json)

prompt_list_5="Generate a trendy menu list with drop_shadow effects"

generated_json= generate_json(prompt_list_5, model, tokenizer)

print("Generated JSON:", generated_json)

### icon button

prompt_icon_1= "Generate a Basic icon button with a state of hover."

generated_json= generate_json(prompt_icon_1, model, tokenizer)

print("Generated JSON:", generated_json)

prompt_icon_2= "Generate a iconbutton."

generated_json= generate_json(prompt_icon_2, model, tokenizer)

print("Generated JSON:", generated_json)

prompt_icon_3= "Generate a playful iconbutton with a state of hover."

generated_json= generate_json(prompt_icon_3, model, tokenizer)

print("Generated JSON:", generated_json)

prompt_icon_4="Generate a default icon button with a border radius of 10.0"

generated_json= generate_json(prompt_icon_4, model, tokenizer)

print("Generated JSON:", generated_json)

prompt_icon_5="Generate a trendy icon button with a stroke weight of 2.0"

generated_json= generate_json(prompt_icon_5, model, tokenizer)

print("Generated JSON:", generated_json)



### Metrics


def calculate_bleu(reference, hypothesis):
    reference_tokens = nltk.word_tokenize(reference)
    hypothesis_tokens = nltk.word_tokenize(hypothesis)
    return nltk.translate.bleu_score.sentence_bleu([reference_tokens], hypothesis_tokens)


def calculate_rouge(reference, hypothesis):
    scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)
    scores = scorer.score(reference, hypothesis)
    return scores['rouge1'].fmeasure, scores['rougeL'].fmeasure


from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score


def calculate_key_metrics(reference_json, generated_json_str):
    try:
        reference_data = json.loads(reference_json)
    except json.JSONDecodeError:
        print("Error decoding reference JSON")
        return 0, 0, 0, 0, 0  # default values if decoding fails

    # Check key phrases directly in the generated JSON string
    component_name_correct = reference_data.get('component_name', '') in generated_json_str
    style_correct = reference_data.get('style', '') in generated_json_str
    border_radius_correct = str(reference_data.get('border_radius', '')) in generated_json_str
    stroke_weight_correct = str(reference_data.get('stroke_weight', '')) in generated_json_str
    drop_shadow_correct = 'drop_shadow' in generated_json_str if 'drop_shadow' in reference_data else True

    return component_name_correct, style_correct, border_radius_correct, stroke_weight_correct, drop_shadow_correct

subset_size = 100

# Select a random subset of the dataset
subset_indices = random.sample(range(len(json_data_1)), subset_size)
subset_prompts = [prompts_1[i] for i in subset_indices]
subset_reference_jsons = [json_data_1[i] for i in subset_indices]

generated_jsons = [generate_json(prompt, model, tokenizer) for prompt in subset_prompts]

total_bleu, total_rouge1, total_rougeL = 0, 0, 0
true_component_names = []
predicted_component_names = []

for i, reference_json in enumerate(subset_reference_jsons):
    generated_json = generated_jsons[i]

    # Calculate BLEU score
    bleu_score = calculate_bleu(reference_json, generated_json)
    total_bleu += bleu_score

    # Calculate ROUGE score
    rouge1_score, rougeL_score = calculate_rouge(reference_json, generated_json)
    total_rouge1 += rouge1_score
    total_rougeL += rougeL_score


overall_component_correct, overall_style_correct = 0, 0
overall_border_radius_correct, overall_stroke_weight_correct, overall_drop_shadow_correct = 0, 0, 0


for i, reference_json in enumerate(subset_reference_jsons):
    generated_json = generated_jsons[i]

    # Calculate key metrics
    component_correct, style_correct, border_radius_correct, stroke_weight_correct, drop_shadow_correct = calculate_key_metrics(reference_json, generated_json)

    # Sum up the individual metrics for overall calculation
    overall_component_correct += component_correct
    overall_style_correct += style_correct
    overall_border_radius_correct += border_radius_correct
    overall_stroke_weight_correct += stroke_weight_correct
    overall_drop_shadow_correct += drop_shadow_correct

total_prompts = len(subset_prompts)
overall_component_correct /= total_prompts
overall_style_correct /= total_prompts
overall_border_radius_correct /= total_prompts
overall_stroke_weight_correct /= total_prompts
overall_drop_shadow_correct /= total_prompts


# Calculate overall BLEU and ROUGE scores
average_bleu = total_bleu / total_prompts
average_rouge1 = total_rouge1 / total_prompts
average_rougeL = total_rougeL / total_prompts


print(f"Overall Component Name Correct: {overall_component_correct}")
print(f"Overall Style Correct: {overall_style_correct}")
print(f"Overall Border Radius Correct: {overall_border_radius_correct}")
print(f"Overall Stroke Weight Correct: {overall_stroke_weight_correct}")
print(f"Overall Drop Shadow Correct: {overall_drop_shadow_correct}")
print(f"Average BLEU Score: {average_bleu}")
print(f"Average ROUGE-1 Score: {average_rouge1}")
print(f"Average ROUGE-L Score: {average_rougeL}")

metrics = ['BLEU', 'ROUGE-1', 'ROUGE-L']

values = [average_bleu, average_rouge1, average_rougeL]


plt.figure(figsize=(10, 5))
plt.bar(metrics, values, color='skyblue')
plt.xlabel('Metrics')
plt.ylabel('Scores')
plt.title('Average BLEU, ROUGE-1, and ROUGE-L scores')
plt.ylim(0, 1)
plt.show()

key_metrics = ['Component Name', 'Style', 'Border Radius', 'Stroke Weight', 'Drop Shadow']


correctness_values = [overall_component_correct, overall_style_correct, overall_border_radius_correct, overall_stroke_weight_correct, overall_drop_shadow_correct]


plt.figure(figsize=(10, 5))
plt.bar(key_metrics, correctness_values, color='lightgreen')
plt.xlabel('Key metrics')
plt.ylabel('Correctness ratio')
plt.title('Correctness of metrics for generated JSONs')
plt.ylim(0, 1)
plt.show()

bleu_scores = []
rouge1_scores = []
rougeL_scores = []


for i, reference_json in enumerate(subset_reference_jsons):
    generated_json = generated_jsons[i]
    reference_json_str = json.dumps(reference_json) if isinstance(reference_json, dict) else reference_json

    bleu_scores.append(calculate_bleu(reference_json_str, generated_json))
    rouge1_score, rougeL_score = calculate_rouge(reference_json_str, generated_json)
    rouge1_scores.append(rouge1_score)
    rougeL_scores.append(rougeL_score)

plt.figure(figsize=(15, 5))

plt.subplot(1, 3, 1)
plt.hist(bleu_scores, bins=20, color='skyblue', edgecolor='black')
plt.title('Distribution of BLEU scores')
plt.xlabel('BLEU scores')
plt.ylabel('Frequency')

plt.subplot(1, 3, 2)
plt.hist(rouge1_scores, bins=20, color='lightgreen', edgecolor='black')
plt.title('Distribution of ROUGE-1 scores')
plt.xlabel('ROUGE-1 scores')
plt.ylabel('Frequency')

plt.subplot(1, 3, 3)
plt.hist(rougeL_scores, bins=20, color='salmon', edgecolor='black')
plt.title('Distribution of ROUGE-L scores')
plt.xlabel('ROUGE-L scores')
plt.ylabel('Frequency')

plt.tight_layout()
plt.show()

def identify_component_name_in_json(json_str):
    # Extract the component_name value from the JSON string
    match = re.search(r'"component_name"\s*:\s*"([^"]+)"', json_str)
    if match:
        return match.group(1).replace("-", " ").lower()
    return None

def identify_component_name_in_description(description):
    components = ['icon-button', 'button', 'label', 'input-field', 'menu', 'list-item']
    description_lower = description.lower()

    # Normalize description by removing spaces and hyphens
    normalized_description = description_lower.replace(" ", "").replace("-", "")

    for component in components:
        normalized_component = component.replace("-", "")
        if normalized_component in normalized_description:
            return component

    return None

def calculate_component_accuracy(reference_str, generated_str):
    ref_component_name = identify_component_name_in_json(reference_str)
    gen_component_name = identify_component_name_in_json(generated_str)
    # Handle "icon" being equal to "icon button"
    if ref_component_name == "icon button" and gen_component_name == "icon":
        gen_component_name = "icon button"
    if ref_component_name is None or gen_component_name is None:
        return False, ref_component_name, gen_component_name
    return ref_component_name == gen_component_name, ref_component_name, gen_component_name


true_labels = []
predicted_labels = []
correct_component_predictions = 0


for i, reference_str in enumerate(subset_reference_jsons):
    generated_str = generated_jsons[i]

    # Print the JSON strings for debugging
    print(f"Dataset JSON: {reference_str}")
    print(f"Generated JSON: {generated_str}")

    # Calculate component accuracy
    is_correct, ref_component_name, gen_component_name = calculate_component_accuracy(reference_str, generated_str)

    # Only append non-None values
    if ref_component_name is not None and gen_component_name is not None:
        true_labels.append(ref_component_name)
        predicted_labels.append(gen_component_name)
        if is_correct:
            correct_component_predictions += 1

    # Print progress
    print(f"Description: {subset_prompts[i]}")
    print(f"Name in Dataset: {ref_component_name}")
    print(f"Name in Generated: {gen_component_name}")
    print("--------------------------------------------------")

# Replace None with 'none' to avoid type errors
true_labels = ['none' if label is None else label for label in true_labels]
predicted_labels = ['none' if label is None else label for label in predicted_labels]


accuracy = accuracy_score(true_labels, predicted_labels)
precision = precision_score(true_labels, predicted_labels, average='weighted', zero_division=0)
recall = recall_score(true_labels, predicted_labels, average='weighted', zero_division=0)
f1 = f1_score(true_labels, predicted_labels, average='weighted', zero_division=0)

print(f"Accuracy: {accuracy}")
print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"F1 Score: {f1}")

from sklearn.preprocessing import label_binarize
from sklearn.metrics import confusion_matrix, roc_curve, auc

# Generate confusion matrix
labels = ["button", "label", "input-field", "list-item", "menu", "icon button", "none"]
conf_matrix = confusion_matrix(true_labels, predicted_labels, labels=labels)


plt.figure(figsize=(10, 7))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

fpr = {}
tpr = {}
roc_auc = {}

# Binarize the labels for ROC calculation
true_binarized = label_binarize(true_labels, classes=labels)
pred_binarized = label_binarize(predicted_labels, classes=labels)


for i in range(true_binarized.shape[1]):
    fpr[i], tpr[i], _ = roc_curve(true_binarized[:, i], pred_binarized[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Plot ROC curve for each class
plt.figure(figsize=(10, 7))
colors = ['blue', 'green', 'red', 'cyan', 'magenta', 'yellow', 'orange']
for i, color in enumerate(colors):
    plt.plot(fpr[i], tpr[i], color=color, lw=2, label=f'ROC curve (area = {roc_auc[i]:.2f}) for class {labels[i]}')
plt.plot([0, 1], [0, 1], 'k--', lw=2)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curves')
plt.legend(loc="lower right")
plt.show()

## Complex T5 Simple JSON

### Model

checkpoint_path = "/content/drive/My Drive/Implementation/Final Thesis/FYP/UpdatedResults/checkpoint-16500"

loaded_model = T5ForConditionalGeneration.from_pretrained(checkpoint_path).to(device)

original_model_path = "t5-small"

loaded_tokenizer = T5Tokenizer.from_pretrained(original_model_path)

def generate_json(prompt, model, tokenizer, max_length=1024):

    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    model.to(device)

    # Standardize the prompt
    prompt = standardize_prompt(prompt)


    inputs = tokenizer(prompt, return_tensors='pt', padding='max_length', truncation=True, max_length=max_length).to(device)
    outputs = model.generate(
        inputs['input_ids'],
        max_length=max_length,
        num_beams=5,
        early_stopping=True,
        temperature=0
    )
    generated_json_str = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return generated_json_str

def standardize_prompt(prompt):
    # Define a dictionary for term mappings
    term_mappings = {
        "menu item": "menu",
        "menuitem": "menu",
        "menu list": "menu",
        "list item": "list-item",
        "listitem": "list-item",
        "input field": "input-field",
        "inputfield": "input-field",
        "iconbutton": "icon-button",
        "iconbutton": "icon-button",
        "button": "button",
        "label": "label"

    }
    for term, standard_term in term_mappings.items():
        prompt = prompt.replace(term, standard_term)

    return prompt

### Button

prompt_button_1= "Generate a Professional button with a state of hover."

generated_json= generate_json(prompt_button_1, loaded_model, loaded_tokenizer)


print(generated_json)

print("Generated JSON:", generated_json)

prompt_button_2= "Generate a Professional button with a size of small, state of default."

generated_json= generate_json(prompt_button_2, loaded_model, loaded_tokenizer)

print("Generated JSON:", generated_json)

prompt_button_3= "Generate a Basic button with DROP_SHADOW effects."

generated_json= generate_json(prompt_button_3, loaded_model, loaded_tokenizer)

print("Generated JSON:", generated_json)

prompt_button_4="Generate a Professional Button with a border radius of 10.0"

generated_json= generate_json(prompt_button_4, loaded_model, loaded_tokenizer)

print("Generated JSON:", generated_json)

prompt_button_5="Generate a Trendy Button"

generated_json= generate_json(prompt_button_5, loaded_model, loaded_tokenizer)

print("Generated JSON:", generated_json)

### Label

prompt_label_1= "Generate a Professional label with a state of hover."

generated_json= generate_json(prompt_label_1, loaded_model, loaded_tokenizer)

print("Generated JSON:", generated_json)

prompt_label_2= "Generate a casual label with a size of small."

generated_json= generate_json(prompt_label_2, loaded_model, loaded_tokenizer)

print("Generated JSON:", generated_json)

prompt_label_3= "Generate a label with DROP_SHADOW effects."

generated_json= generate_json(prompt_label_3, loaded_model, loaded_tokenizer)

print("Generated JSON:", generated_json)

prompt_label_4="Generate a default label with a border radius of 10.0"

generated_json= generate_json(prompt_label_4, loaded_model, loaded_tokenizer)

print("Generated JSON:", generated_json)

prompt_label_5="Generate a Professional label"

generated_json= generate_json(prompt_label_5, loaded_model, loaded_tokenizer)

print("Generated JSON:", generated_json)

### input field

prompt_input_1= "Generate a Basic input-field with a state of hover."

generated_json= generate_json(prompt_input_1, loaded_model, loaded_tokenizer)

print("Generated JSON:", generated_json)

prompt_input_2= "Generate a input-field."

generated_json= generate_json(prompt_input_2, loaded_model, loaded_tokenizer)

print("Generated JSON:", generated_json)

prompt_input_3= "Generate a Professional input-field with a state of small."

generated_json= generate_json(prompt_input_3, loaded_model, loaded_tokenizer)

print("Generated JSON:", generated_json)

prompt_input_4="Generate a default input field with a border radius of 10.0"

generated_json= generate_json(prompt_input_4, loaded_model, loaded_tokenizer)

print("Generated JSON:", generated_json)

prompt_input_5="Generate a playful input field with drop_shadow effects"

generated_json= generate_json(prompt_input_5, loaded_model, loaded_tokenizer)

print("Generated JSON:", generated_json)

### Menu

prompt_menu_1= "Generate a Basic menu item with a state of hover."

generated_json= generate_json(prompt_menu_1, loaded_model, loaded_tokenizer)

print("Generated JSON:", generated_json)

prompt_menu_2= "Generate a menu."

generated_json= generate_json(prompt_menu_2, loaded_model, loaded_tokenizer)

print("Generated JSON:", generated_json)

prompt_menu_3= "Generate a Professional menu item with a small state."

generated_json= generate_json(prompt_menu_3, loaded_model, loaded_tokenizer)

print("Generated JSON:", generated_json)

prompt_menu_4="Generate a default menu item with a border radius of 10.0"

generated_json= generate_json(prompt_menu_4, loaded_model, loaded_tokenizer)

print("Generated JSON:", generated_json)

prompt_menu_5="Generate a playful menu list with drop_shadow effects"

generated_json= generate_json(prompt_menu_5, loaded_model, loaded_tokenizer)

print("Generated JSON:", generated_json)

### list-item

prompt_list_1= "Generate a Basic list item with a state of hover."

generated_json= generate_json(prompt_list_1, loaded_model, loaded_tokenizer)

print("Generated JSON:", generated_json)

prompt_list_2= "Generate a list item."

generated_json= generate_json(prompt_list_2, loaded_model, loaded_tokenizer)

print("Generated JSON:", generated_json)

prompt_list_3= "Generate a playful listitem with a state of hover."

generated_json= generate_json(prompt_list_3, loaded_model, loaded_tokenizer)

print("Generated JSON:", generated_json)

prompt_list_4="Generate a default listitem with a border radius of 10.0"

generated_json= generate_json(prompt_list_4, loaded_model, loaded_tokenizer)

print("Generated JSON:", generated_json)

prompt_list_5="Generate a trendy menu list with drop_shadow effects"

generated_json= generate_json(prompt_list_5, loaded_model, loaded_tokenizer)

print("Generated JSON:", generated_json)



### icon button

prompt_icon_1= "Generate a Basic icon button with a state of hover."

generated_json= generate_json(prompt_icon_1, loaded_model, loaded_tokenizer)

print("Generated JSON:", generated_json)

prompt_icon_2= "Generate a iconbutton."

generated_json= generate_json(prompt_icon_2, loaded_model, loaded_tokenizer)

print("Generated JSON:", generated_json)

prompt_icon_3= "Generate a playful iconbutton with a state of hover."

generated_json= generate_json(prompt_icon_3, loaded_model, loaded_tokenizer)

print("Generated JSON:", generated_json)

prompt_icon_4="Generate a default icon button with a border radius of 10.0"

generated_json= generate_json(prompt_icon_4, loaded_model, loaded_tokenizer)

print("Generated JSON:", generated_json)

prompt_icon_5="Generate a trendy icon button with a stroke weight of 2.0"

generated_json= generate_json(prompt_icon_5, loaded_model, loaded_tokenizer)

print("Generated JSON:", generated_json)

### Metrics

subset_size = 100
subset_indices = random.sample(range(len(json_data_1)), subset_size)
subset_prompts = [prompts_1[i] for i in subset_indices]
subset_reference_jsons = [json_data_1[i] for i in subset_indices]

generated_jsons = [generate_json(prompt, loaded_model, loaded_tokenizer) for prompt in subset_prompts]


def calculate_bleu(reference, hypothesis):
    reference_tokens = nltk.word_tokenize(reference)
    hypothesis_tokens = nltk.word_tokenize(hypothesis)
    return nltk.translate.bleu_score.sentence_bleu([reference_tokens], hypothesis_tokens)



def calculate_rouge(reference, hypothesis):
    scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)
    scores = scorer.score(reference, hypothesis)
    return scores['rouge1'].fmeasure, scores['rougeL'].fmeasure


def calculate_key_metrics(reference_json_str, generated_json_str):
    try:
        reference_data = json.loads(reference_json_str) if isinstance(reference_json_str, str) else reference_json_str
    except json.JSONDecodeError:
        print("Error decoding reference JSON")
        return 0, 0, 0, 0, 0  # default values if decoding fails

    # Check key phrases directly in the generated JSON string
    component_name_correct = reference_data.get('component_name', '') in generated_json_str
    style_correct = reference_data.get('style', '') in generated_json_str
    border_radius_correct = str(reference_data.get('border_radius', '')) in generated_json_str
    stroke_weight_correct = str(reference_data.get('stroke_weight', '')) in generated_json_str
    drop_shadow_correct = 'DROP_SHADOW' in generated_json_str if 'DROP_SHADOW' in reference_data.get('effects', []) else True

    return component_name_correct, style_correct, border_radius_correct, stroke_weight_correct, drop_shadow_correct


# Evaluate the generated JSONs
total_bleu, total_rouge1, total_rougeL = 0, 0, 0
overall_component_correct, overall_style_correct = 0, 0
overall_border_radius_correct, overall_stroke_weight_correct, overall_drop_shadow_correct = 0, 0, 0


overall_component_correct, overall_style_correct = 0, 0
overall_border_radius_correct, overall_stroke_weight_correct, overall_drop_shadow_correct = 0, 0, 0
total_bleu, total_rouge1, total_rougeL = 0, 0, 0
total_prompts = len(subset_prompts)

for i, prompt in enumerate(subset_prompts):
    reference_json = subset_reference_jsons[i]
    generated_json = generated_jsons[i]

    # Convert reference_json to string for comparison if necessary
    reference_json_str = json.dumps(reference_json) if isinstance(reference_json, dict) else reference_json

    # Calculate key metrics
    component_correct, style_correct, border_radius_correct, stroke_weight_correct, drop_shadow_correct = calculate_key_metrics(reference_json_str, generated_json)

    # Sum up the individual metrics for overall calculation
    overall_component_correct += component_correct
    overall_style_correct += style_correct
    overall_border_radius_correct += border_radius_correct
    overall_stroke_weight_correct += stroke_weight_correct
    overall_drop_shadow_correct += drop_shadow_correct

    # Calculate BLEU score
    bleu_score = calculate_bleu(reference_json_str, generated_json)
    total_bleu += bleu_score

    # Calculate ROUGE score
    rouge1_score, rougeL_score = calculate_rouge(reference_json_str, generated_json)
    total_rouge1 += rouge1_score
    total_rougeL += rougeL_score

# Calculate overall metrics
total_prompts = len(subset_prompts)
overall_component_correct /= total_prompts
overall_style_correct /= total_prompts
overall_border_radius_correct /= total_prompts
overall_stroke_weight_correct /= total_prompts
overall_drop_shadow_correct /= total_prompts
average_bleu = total_bleu / total_prompts
average_rouge1 = total_rouge1 / total_prompts
average_rougeL = total_rougeL / total_prompts


print(f"Overall Component Name Correct: {overall_component_correct}")
print(f"Overall Style Correct: {overall_style_correct}")
print(f"Overall Border Radius Correct: {overall_border_radius_correct}")
print(f"Overall Stroke Weight Correct: {overall_stroke_weight_correct}")
print(f"Overall Drop Shadow Correct: {overall_drop_shadow_correct}")
print(f"Average BLEU Score: {average_bleu}")
print(f"Average ROUGE-1 Score: {average_rouge1}")
print(f"Average ROUGE-L Score: {average_rougeL}")

metrics = ['BLEU', 'ROUGE-1', 'ROUGE-L']

values = [average_bleu, average_rouge1, average_rougeL]


plt.figure(figsize=(10, 5))
plt.bar(metrics, values, color='skyblue')
plt.xlabel('Metrics')
plt.ylabel('Scores')
plt.title('Average BLEU, ROUGE-1, and ROUGE-L scores')
plt.ylim(0, 1)
plt.show()

metrics = ['BLEU', 'ROUGE-1', 'ROUGE-L']
values = [average_bleu, average_rouge1, average_rougeL]


plt.figure(figsize=(10, 5))
plt.bar(metrics, values, color='skyblue')
plt.xlabel('Metrics')
plt.ylabel('Scores')
plt.title('Average BLEU, ROUGE-1, and ROUGE-L Scores')
plt.ylim(0, 1)
plt.show()

key_metrics = ['Component Name', 'Style', 'Border Radius', 'Stroke Weight', 'Drop Shadow']


correctness_values = [overall_component_correct, overall_style_correct, overall_border_radius_correct, overall_stroke_weight_correct, overall_drop_shadow_correct]


plt.figure(figsize=(10, 5))
plt.bar(key_metrics, correctness_values, color='lightgreen')
plt.xlabel('Key metrics')
plt.ylabel('Correctness ratio')
plt.title('Correctness of metrics for generated JSONs')
plt.ylim(0, 1)
plt.show()

bleu_scores = []
rouge1_scores = []
rougeL_scores = []

for i, reference_json in enumerate(subset_reference_jsons):
    generated_json = generated_jsons[i]
    reference_json_str = json.dumps(reference_json) if isinstance(reference_json, dict) else reference_json

    bleu_scores.append(calculate_bleu(reference_json_str, generated_json))
    rouge1_score, rougeL_score = calculate_rouge(reference_json_str, generated_json)
    rouge1_scores.append(rouge1_score)
    rougeL_scores.append(rougeL_score)

plt.figure(figsize=(15, 5))

plt.subplot(1, 3, 1)
plt.hist(bleu_scores, bins=20, color='skyblue', edgecolor='black')
plt.title('Distribution of BLEU scores')
plt.xlabel('BLEU scores')
plt.ylabel('Frequency')

plt.subplot(1, 3, 2)
plt.hist(rouge1_scores, bins=20, color='lightgreen', edgecolor='black')
plt.title('Distribution of ROUGE-1 scores')
plt.xlabel('ROUGE-1 scores')
plt.ylabel('Frequency')

plt.subplot(1, 3, 3)
plt.hist(rougeL_scores, bins=20, color='salmon', edgecolor='black')
plt.title('Distribution of ROUGE-L scores')
plt.xlabel('ROUGE-L scores')
plt.ylabel('Frequency')

plt.tight_layout()
plt.show()



def identify_component_name_in_json(json_str):
    # Extract the component_name value from the JSON string
    match = re.search(r'"component_name"\s*:\s*"([^"]+)"', json_str)
    if match:
        return match.group(1).replace("-", " ").lower()
    return None

def identify_component_name_in_description(description):
    components = ['icon-button', 'button', 'label', 'input-field', 'menu', 'list-item']
    description_lower = description.lower()

    # Normalize description by removing spaces and hyphens
    normalized_description = description_lower.replace(" ", "").replace("-", "")

    for component in components:
        normalized_component = component.replace("-", "")
        if normalized_component in normalized_description:
            return component

    return None

def calculate_component_accuracy(reference_str, generated_str):
    ref_component_name = identify_component_name_in_json(reference_str)
    gen_component_name = identify_component_name_in_json(generated_str)
    # Handle "icon" being equal to "icon button"
    if ref_component_name == "icon button" and gen_component_name == "icon":
        gen_component_name = "icon button"
    if ref_component_name is None or gen_component_name is None:
        return False, ref_component_name, gen_component_name
    return ref_component_name == gen_component_name, ref_component_name, gen_component_name


true_labels = []
predicted_labels = []
correct_component_predictions = 0


for i, reference_str in enumerate(subset_reference_jsons):
    generated_str = generated_jsons[i]

    # Print the JSON strings for debugging
    print(f"Dataset JSON: {reference_str}")
    print(f"Generated JSON: {generated_str}")

    # Calculate component accuracy
    is_correct, ref_component_name, gen_component_name = calculate_component_accuracy(reference_str, generated_str)

    # Only append non-None values
    if ref_component_name is not None and gen_component_name is not None:
        true_labels.append(ref_component_name)
        predicted_labels.append(gen_component_name)
        if is_correct:
            correct_component_predictions += 1

    # Print progress
    print(f"Description: {subset_prompts[i]}")
    print(f"Name in Dataset: {ref_component_name}")
    print(f"Name in Generated: {gen_component_name}")
    print("--------------------------------------------------")

# Replace None with 'none' to avoid type errors
true_labels = ['none' if label is None else label for label in true_labels]
predicted_labels = ['none' if label is None else label for label in predicted_labels]


accuracy = accuracy_score(true_labels, predicted_labels)
precision = precision_score(true_labels, predicted_labels, average='weighted', zero_division=0)
recall = recall_score(true_labels, predicted_labels, average='weighted', zero_division=0)
f1 = f1_score(true_labels, predicted_labels, average='weighted', zero_division=0)

print(f"Accuracy: {accuracy}")
print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"F1 Score: {f1}")

from sklearn.preprocessing import label_binarize
from sklearn.metrics import confusion_matrix, roc_curve, auc

# Generate confusion matrix
labels = ["button", "label", "input-field", "list-item", "menu", "icon button", "none"]
conf_matrix = confusion_matrix(true_labels, predicted_labels, labels=labels)


plt.figure(figsize=(10, 7))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

fpr = {}
tpr = {}
roc_auc = {}

# Binarize the labels for ROC calculation
true_binarized = label_binarize(true_labels, classes=labels)
pred_binarized = label_binarize(predicted_labels, classes=labels)


for i in range(true_binarized.shape[1]):
    fpr[i], tpr[i], _ = roc_curve(true_binarized[:, i], pred_binarized[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Plot ROC curve for each class
plt.figure(figsize=(10, 7))
colors = ['blue', 'green', 'red', 'cyan', 'magenta', 'yellow', 'orange']
for i, color in enumerate(colors):
    plt.plot(fpr[i], tpr[i], color=color, lw=2, label=f'ROC curve (area = {roc_auc[i]:.2f}) for class {labels[i]}')
plt.plot([0, 1], [0, 1], 'k--', lw=2)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curves')
plt.legend(loc="lower right")
plt.show()

## Complex T5 Complex JSON

### Model

#path
model_save_path= '/content/drive/My Drive/Implementation/Final Thesis/FYP/JSON generation/T5 CROSS NESTED'

def load_model_and_tokenizer(model_path):
  """ method to load the model and tokenizer
  Parameters:
  model_path (str): path to the model
  Returns:
  tuple: model and tokenizer
  """
  model = T5ForConditionalGeneration.from_pretrained(model_path)
  tokenizer = T5Tokenizer.from_pretrained(model_path)
  return model, tokenizer

model, tokenizer = load_model_and_tokenizer(model_save_path)

def standardize_prompt(prompt):
    # Define a dictionary for term mappings
    term_mappings = {
        "menu item": "menu",
        "menuitem": "menu",
        "menu list": "menu",
        "list item": "list-item",
        "listitem": "list-item",
        "input field": "input-field",
        "inputfield": "input-field",
        "iconbutton": "icon-button",
        "iconbutton": "icon-button",
        "button": "button",
        "label": "label"

    }
    for term, standard_term in term_mappings.items():
        prompt = prompt.replace(term, standard_term)

    return prompt

def generate_json(prompt, model, tokenizer, max_length=1024):
  """method to generate json from prompt
  Parameters:
  prompt (str): prompt to generate json
  model (T5ForConditionalGeneration): model to generate json
  tokenizer (T5Tokenizer): tokenizer to generate json
  max_length (int): maximum length of the generated json
  Returns:
  str: generated json
  """
  device = 'cuda' if torch.cuda.is_available() else 'cpu'
  model.to(device)
  prompt = standardize_prompt(prompt)
  #tokenize input description
  inputs= tokenizer(prompt, return_tensors='pt', padding='max_length', truncation=True, max_length=max_length).to(device)
   #generate JSON output
  outputs= model.generate(inputs['input_ids'], max_length=max_length,num_beams=5,early_stopping=True,temperature=0)
  #decode generated tokens to JSON string
  generated_json_str= tokenizer.decode(outputs[0], skip_special_tokens=True)
  return generated_json_str

### button

prompt_button_1= "Generate a Professional button with a state of hover."

generated_json= generate_json(prompt_button_1, model, tokenizer)

print("Generated JSON:", generated_json)

prompt_button_2= "Generate a Professional button with a size of small, state of default."

generated_json= generate_json(prompt_button_2, model, tokenizer)

print("Generated JSON:", generated_json)

prompt_button_3= "Generate a Basic button with DROP_SHADOW effects."

generated_json= generate_json(prompt_button_3, model, tokenizer)

print("Generated JSON:", generated_json)

prompt_button_4="Generate a Professional Button with a border radius of 10.0"

generated_json= generate_json(prompt_button_4, model, tokenizer)

print("Generated JSON:", generated_json)

prompt_button_5="Generate a Trendy Button"

generated_json= generate_json(prompt_button_5, model, tokenizer)

print("Generated JSON:", generated_json)

### Label

prompt_label_1= "Generate a Professional label with a state of hover."

generated_json= generate_json(prompt_label_1, model, tokenizer)

print("Generated JSON:", generated_json)

prompt_label_2= "Generate a casual label with a size of small."

generated_json= generate_json(prompt_label_2, model, tokenizer)

print("Generated JSON:", generated_json)

prompt_label_3= "Generate a label with DROP_SHADOW effects."

generated_json= generate_json(prompt_label_3, model, tokenizer)

print("Generated JSON:", generated_json)

prompt_label_4="Generate a default label with a border radius of 10.0"

generated_json= generate_json(prompt_label_4, model, tokenizer)

print("Generated JSON:", generated_json)

prompt_label_5="Generate a Professional label"

generated_json= generate_json(prompt_label_5, model, tokenizer)

print("Generated JSON:", generated_json)

### input field

prompt_input_1= "Generate a Basic input-field with a state of hover."

generated_json= generate_json(prompt_input_1, model, tokenizer)

print("Generated JSON:", generated_json)

prompt_input_2= "Generate a input-field."

generated_json= generate_json(prompt_input_2, model, tokenizer)

print("Generated JSON:", generated_json)

prompt_input_3= "Generate a Professional input-field with a state of small."

generated_json= generate_json(prompt_input_3, model, tokenizer)

print("Generated JSON:", generated_json)

prompt_input_4="Generate a default input field with a border radius of 10.0"

generated_json= generate_json(prompt_input_4, model, tokenizer)

print("Generated JSON:", generated_json)

prompt_input_5="Generate a playful input field with drop_shadow effects"

generated_json= generate_json(prompt_input_5, model, tokenizer)

print("Generated JSON:", generated_json)

### Menu

prompt_menu_1= "Generate a Basic menu with a state of hover."

generated_json= generate_json(prompt_menu_1, model, tokenizer)

print("Generated JSON:", generated_json)

prompt_menu_2= "Generate a menu."

generated_json= generate_json(prompt_menu_2, model, tokenizer)

print("Generated JSON:", generated_json)

prompt_menu_3= "Generate a Professional menu item with a small state."

generated_json= generate_json(prompt_menu_3, model, tokenizer)

print("Generated JSON:", generated_json)

prompt_menu_4="Generate a default menu item with a border radius of 10.0"

generated_json= generate_json(prompt_menu_4, model, tokenizer)

print("Generated JSON:", generated_json)

prompt_menu_5="Generate a playful menu list with drop_shadow effects"

generated_json= generate_json(prompt_menu_5, model, tokenizer)

print("Generated JSON:", generated_json)

### list-item

prompt_list_1= "Generate a Basic list item with a state of hover."

generated_json= generate_json(prompt_list_1, model, tokenizer)

print("Generated JSON:", generated_json)

prompt_list_2= "Generate a list item."

generated_json= generate_json(prompt_list_2, model, tokenizer)

print("Generated JSON:", generated_json)

prompt_list_3= "Generate a playful listitem with a state of hover."

generated_json= generate_json(prompt_list_3, model, tokenizer)

print("Generated JSON:", generated_json)

prompt_list_4="Generate a default listitem with a border radius of 10.0"

generated_json= generate_json(prompt_list_4, model, tokenizer)

print("Generated JSON:", generated_json)

prompt_list_5="Generate a trendy list item with drop_shadow effects"

generated_json= generate_json(prompt_list_5, model, tokenizer)

print("Generated JSON:", generated_json)

### icon button

prompt_icon_1= "Generate a Basic icon button with a state of hover."

generated_json= generate_json(prompt_icon_1, model, tokenizer)

print("Generated JSON:", generated_json)

prompt_icon_2= "Generate a iconbutton."

generated_json= generate_json(prompt_icon_2, model, tokenizer)

print("Generated JSON:", generated_json)

prompt_icon_3= "Generate a playful iconbutton with a state of hover."

generated_json= generate_json(prompt_icon_3, model, tokenizer)

print("Generated JSON:", generated_json)

prompt_icon_4="Generate a default icon button with a border radius of 10.0"

generated_json= generate_json(prompt_icon_4, model, tokenizer)

print("Generated JSON:", generated_json)

prompt_icon_5="Generate a trendy icon button with a stroke weight of 2.0"

generated_json= generate_json(prompt_icon_5, model, tokenizer)

print("Generated JSON:", generated_json)

### Metrics

import json
from nltk.translate.bleu_score import sentence_bleu
from rouge_score import rouge_scorer
import nltk
import re
import random
import matplotlib.pyplot as plt


def calculate_bleu(reference, hypothesis):
    reference_tokens = nltk.word_tokenize(reference)
    hypothesis_tokens = nltk.word_tokenize(hypothesis)
    return nltk.translate.bleu_score.sentence_bleu([reference_tokens], hypothesis_tokens)


def calculate_rouge(reference, hypothesis):
    scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)
    scores = scorer.score(reference, hypothesis)
    return scores['rouge1'].fmeasure, scores['rougeL'].fmeasure


from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score


def calculate_key_metrics(reference_json, generated_json_str):
    try:
        reference_data = json.loads(reference_json)
    except json.JSONDecodeError:
        print("Error decoding reference JSON")
        return 0, 0, 0, 0, 0  # default values if decoding fails

    # Check key phrases directly in the generated JSON string
    component_name_correct = reference_data.get('component_name', '') in generated_json_str
    style_correct = reference_data.get('style', '') in generated_json_str
    border_radius_correct = str(reference_data.get('border_radius', '')) in generated_json_str
    stroke_weight_correct = str(reference_data.get('stroke_weight', '')) in generated_json_str
    drop_shadow_correct = 'drop_shadow' in generated_json_str if 'drop_shadow' in reference_data else True

    return component_name_correct, style_correct, border_radius_correct, stroke_weight_correct, drop_shadow_correct

subset_size = 100

# Select a random subset of the dataset
subset_indices = random.sample(range(len(json_data_1)), subset_size)
subset_prompts = [prompts_1[i] for i in subset_indices]
subset_reference_jsons = [json_data_1[i] for i in subset_indices]

generated_jsons = [generate_json(prompt, model, tokenizer) for prompt in subset_prompts]

total_bleu, total_rouge1, total_rougeL = 0, 0, 0
true_component_names = []
predicted_component_names = []

for i, reference_json in enumerate(subset_reference_jsons):
    generated_json = generated_jsons[i]

    # Calculate BLEU score
    bleu_score = calculate_bleu(reference_json, generated_json)
    total_bleu += bleu_score

    # Calculate ROUGE score
    rouge1_score, rougeL_score = calculate_rouge(reference_json, generated_json)
    total_rouge1 += rouge1_score
    total_rougeL += rougeL_score


overall_component_correct, overall_style_correct = 0, 0
overall_border_radius_correct, overall_stroke_weight_correct, overall_drop_shadow_correct = 0, 0, 0


for i, reference_json in enumerate(subset_reference_jsons):
    generated_json = generated_jsons[i]

    # Calculate key metrics
    component_correct, style_correct, border_radius_correct, stroke_weight_correct, drop_shadow_correct = calculate_key_metrics(reference_json, generated_json)

    # Sum up the individual metrics for overall calculation
    overall_component_correct += component_correct
    overall_style_correct += style_correct
    overall_border_radius_correct += border_radius_correct
    overall_stroke_weight_correct += stroke_weight_correct
    overall_drop_shadow_correct += drop_shadow_correct

total_prompts = len(subset_prompts)
overall_component_correct /= total_prompts
overall_style_correct /= total_prompts
overall_border_radius_correct /= total_prompts
overall_stroke_weight_correct /= total_prompts
overall_drop_shadow_correct /= total_prompts


# Calculate overall BLEU and ROUGE scores
average_bleu = total_bleu / total_prompts
average_rouge1 = total_rouge1 / total_prompts
average_rougeL = total_rougeL / total_prompts


print(f"Overall Component Name Correct: {overall_component_correct}")
print(f"Overall Style Correct: {overall_style_correct}")
print(f"Overall Border Radius Correct: {overall_border_radius_correct}")
print(f"Overall Stroke Weight Correct: {overall_stroke_weight_correct}")
print(f"Overall Drop Shadow Correct: {overall_drop_shadow_correct}")
print(f"Average BLEU Score: {average_bleu}")
print(f"Average ROUGE-1 Score: {average_rouge1}")
print(f"Average ROUGE-L Score: {average_rougeL}")

metrics = ['BLEU', 'ROUGE-1', 'ROUGE-L']

values = [average_bleu, average_rouge1, average_rougeL]


plt.figure(figsize=(10, 5))
plt.bar(metrics, values, color='skyblue')
plt.xlabel('Metrics')
plt.ylabel('Scores')
plt.title('Average BLEU, ROUGE-1, and ROUGE-L scores')
plt.ylim(0, 1)
plt.show()

key_metrics = ['Component Name', 'Style', 'Border Radius', 'Stroke Weight', 'Drop Shadow']


correctness_values = [overall_component_correct, overall_style_correct, overall_border_radius_correct, overall_stroke_weight_correct, overall_drop_shadow_correct]


plt.figure(figsize=(10, 5))
plt.bar(key_metrics, correctness_values, color='lightgreen')
plt.xlabel('Key metrics')
plt.ylabel('Correctness ratio')
plt.title('Correctness of metrics for generated JSONs')
plt.ylim(0, 1)
plt.show()

bleu_scores = []
rouge1_scores = []
rougeL_scores = []


for i, reference_json in enumerate(subset_reference_jsons):
    generated_json = generated_jsons[i]
    reference_json_str = json.dumps(reference_json) if isinstance(reference_json, dict) else reference_json

    bleu_scores.append(calculate_bleu(reference_json_str, generated_json))
    rouge1_score, rougeL_score = calculate_rouge(reference_json_str, generated_json)
    rouge1_scores.append(rouge1_score)
    rougeL_scores.append(rougeL_score)

plt.figure(figsize=(15, 5))

plt.subplot(1, 3, 1)
plt.hist(bleu_scores, bins=20, color='skyblue', edgecolor='black')
plt.title('Distribution of BLEU scores')
plt.xlabel('BLEU scores')
plt.ylabel('Frequency')

plt.subplot(1, 3, 2)
plt.hist(rouge1_scores, bins=20, color='lightgreen', edgecolor='black')
plt.title('Distribution of ROUGE-1 scores')
plt.xlabel('ROUGE-1 scores')
plt.ylabel('Frequency')

plt.subplot(1, 3, 3)
plt.hist(rougeL_scores, bins=20, color='salmon', edgecolor='black')
plt.title('Distribution of ROUGE-L scores')
plt.xlabel('ROUGE-L scores')
plt.ylabel('Frequency')

plt.tight_layout()
plt.show()

def identify_component_name_in_json(json_str):
    # Extract the component_name value from the JSON string
    match = re.search(r'"component_name"\s*:\s*"([^"]+)"', json_str)
    if match:
        return match.group(1).replace("-", " ").lower()
    return None

def identify_component_name_in_description(description):
    components = ['icon-button', 'button', 'label', 'input-field', 'menu', 'list-item']
    description_lower = description.lower()

    # Normalize description by removing spaces and hyphens
    normalized_description = description_lower.replace(" ", "").replace("-", "")

    for component in components:
        normalized_component = component.replace("-", "")
        if normalized_component in normalized_description:
            return component

    return None

def calculate_component_accuracy(reference_str, generated_str):
    ref_component_name = identify_component_name_in_json(reference_str)
    gen_component_name = identify_component_name_in_json(generated_str)
    # Handle "icon" being equal to "icon button"
    if ref_component_name == "icon button" and gen_component_name == "icon":
        gen_component_name = "icon button"
    if ref_component_name is None or gen_component_name is None:
        return False, ref_component_name, gen_component_name
    return ref_component_name == gen_component_name, ref_component_name, gen_component_name


true_labels = []
predicted_labels = []
correct_component_predictions = 0


for i, reference_str in enumerate(subset_reference_jsons):
    generated_str = generated_jsons[i]

    # Print the JSON strings for debugging
    print(f"Dataset JSON: {reference_str}")
    print(f"Generated JSON: {generated_str}")

    # Calculate component accuracy
    is_correct, ref_component_name, gen_component_name = calculate_component_accuracy(reference_str, generated_str)

    # Only append non-None values
    if ref_component_name is not None and gen_component_name is not None:
        true_labels.append(ref_component_name)
        predicted_labels.append(gen_component_name)
        if is_correct:
            correct_component_predictions += 1

    # Print progress
    print(f"Description: {subset_prompts[i]}")
    print(f"Name in Dataset: {ref_component_name}")
    print(f"Name in Generated: {gen_component_name}")
    print("--------------------------------------------------")

# Replace None with 'none' to avoid type errors
true_labels = ['none' if label is None else label for label in true_labels]
predicted_labels = ['none' if label is None else label for label in predicted_labels]


accuracy = accuracy_score(true_labels, predicted_labels)
precision = precision_score(true_labels, predicted_labels, average='weighted', zero_division=0)
recall = recall_score(true_labels, predicted_labels, average='weighted', zero_division=0)
f1 = f1_score(true_labels, predicted_labels, average='weighted', zero_division=0)

print(f"Accuracy: {accuracy}")
print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"F1 Score: {f1}")

from sklearn.preprocessing import label_binarize
from sklearn.metrics import confusion_matrix, roc_curve, auc

# Generate confusion matrix
labels = ["button", "label", "input-field", "list-item", "menu", "icon button", "none"]
conf_matrix = confusion_matrix(true_labels, predicted_labels, labels=labels)


plt.figure(figsize=(10, 7))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

fpr = {}
tpr = {}
roc_auc = {}

# Binarize the labels for ROC calculation
true_binarized = label_binarize(true_labels, classes=labels)
pred_binarized = label_binarize(predicted_labels, classes=labels)


for i in range(true_binarized.shape[1]):
    fpr[i], tpr[i], _ = roc_curve(true_binarized[:, i], pred_binarized[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Plot ROC curve for each class
plt.figure(figsize=(10, 7))
colors = ['blue', 'green', 'red', 'cyan', 'magenta', 'yellow', 'orange']
for i, color in enumerate(colors):
    plt.plot(fpr[i], tpr[i], color=color, lw=2, label=f'ROC curve (area = {roc_auc[i]:.2f}) for class {labels[i]}')
plt.plot([0, 1], [0, 1], 'k--', lw=2)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curves')
plt.legend(loc="lower right")
plt.show()

