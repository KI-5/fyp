#gdrive connect
from google.colab import drive
drive.mount('/content/drive')

!pip install torch torchvision transformers

#imports
import json
from sklearn.model_selection import train_test_split
import torch
from torch.utils.data import DataLoader, Dataset
import torch.nn as nn
import torch.optim as optim
import tensorflow as tf
from transformers import BertTokenizer
import os
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
from collections import Counter

## GPU

!nvidia-smi

torch.cuda.is_available()

tf.test.gpu_device_name()

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

print(f"Using device: {device}")

# #loading the dataset
# def load_synthetic_data(file_path):
#     data = []
#     with open(file_path, 'r') as f:
#         for line in f:
#             data.append(json.loads(line.strip()))
#     return data

# file_path = '/content/drive/My Drive/fyp/FYP/synthetic_pairs.jsonl'

# data = load_synthetic_data(file_path)

# print(json.dumps(data[:2], indent=2))

## Loading

#loading the json dataset
def load_json_data(file_path):
  with open(file_path, 'r') as f:
    data = [json.loads(line.strip()) for line in f]
  return data

#loading the description dataset
def load_text_data(file_path):
  with open(file_path, 'r') as f:
    data = [line.strip() for line in f]
  return data

# from numba import cuda

# cuda.select_device(0) # choosing second GPU

# cuda.close()

#paths to the datasets
json_file_path='/content/drive/My Drive/Implementation/Final Thesis/FYP/Data/json_data.jsonl'
text_file_path= '/content/drive/My Drive/Implementation/Final Thesis/FYP/Data/descriptions_data.txt'

json_data= load_json_data(json_file_path)
descriptions=load_text_data(text_file_path)

print(f"Length of JSON data: {len(json_data)}")
print(f"Length of description data: {len(descriptions)}")


data = [{'json':json.dumps(json_data[i]), 'description':descriptions[i]} for i in range(len(json_data))]

print(f"First JSON entry:{data[0]['json']}")
print(f"First description entry: {data[0]['description']}")
print(f"Second JSON entry:{data[1]['json']}")
print(f"Second description entry: {data[1]['description']}")

## Preprocess and tokenization

#flattening JSON and create tokenized pairs
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

#preprocess
def preprocess_dataset(data, tokenizer):
  """ method to preprocess the dataset

  Parameters
  data- data input
  tokenizere- tokenizer used for tokenization bert

  Returns
  input_ids, target_ids, component_types
  """
  #input-json, target-description
  inputs, targets= [], []
  component_types= []
  for item in data:
    json_str =item['json']
    description= item['description']
    inputs.append(json_str)
    targets.append(description)

    #extract component type-----CLASS WEIGHTS
    item_dict= json.loads(json_str)
    component_type= item_dict.get('variant_properties', {}).get('component_name', 'Unknown')
    component_types.append(component_type)

  input_ids =tokenizer(inputs, padding=True, truncation=True, return_tensors="pt")['input_ids']
  target_ids= tokenizer(targets, padding=True, truncation=True, return_tensors="pt")['input_ids']
  return input_ids, target_ids, component_types

input_ids, target_ids, component_types= preprocess_dataset(data, tokenizer)

##NOTE ALWAYS!!
def save_preprocessed_data(input_ids, target_ids, component_types, output_dir):
  if not os.path.exists(output_dir):
    os.makedirs(output_dir)

  torch.save(input_ids, os.path.join(output_dir, 'input_ids.pt'))
  torch.save(target_ids, os.path.join(output_dir, 'target_ids.pt'))

  with open(os.path.join(output_dir, 'component_types.json'), 'w') as f:
    json.dump(component_types, f)

  print(f"Data saved to {output_dir}")

preprocessed_data_bert = '/content/drive/My Drive/Implementation/Final Thesis/FYP/Prompt generation/BERT Preprocessed Data new'

save_preprocessed_data(input_ids, target_ids, component_types, preprocessed_data_bert)

def load_preprocessed_data(input_dir):
  """ method to load the preprocessed data
  Parameters
  input_dir- directory where the preprocessed data is stored
  Returns
  input_ids, target_ids, component_types
  """
  input_ids = torch.load(os.path.join(input_dir, 'input_ids.pt'))
  target_ids = torch.load(os.path.join(input_dir, 'target_ids.pt'))
  with open(os.path.join(input_dir, 'component_types.json'), 'r') as f:
    component_types = json.load(f)
  return input_ids, target_ids, component_types

input_ids, target_ids, component_types = load_preprocessed_data(preprocessed_data_bert)

print("input shape testing:", input_ids.shape)
print("target shape", target_ids.shape)

print("Sample input IDs:",input_ids[:5])
print("Sample target IDs:",target_ids[:5])

def calculate_class_weights(component_types, multiplier=1.05, max_weight=10):
  """ method to calculate the weights
  pArameters
  component_types-button
  multiplier-value to multiply
  max-weight- 10 here
  Returns
  class_weights-dictionary
  """

  #Count of components
  component_counts = Counter(component_types)
  total_count = sum(component_counts.values())
  class_weights = {comp: total_count / count for comp, count in component_counts.items()}

  #multiplier and cap weights
  for comp in class_weights:
    if comp != 'Button':
      class_weights[comp] = min(class_weights[comp] * multiplier, max_weight)

  return class_weights

class_weights = calculate_class_weights(component_types, multiplier=1.05, max_weight=10)

print(f"Class weights: {class_weights}")

#split into train test validation sets80 10 10
input_ids_train, input_ids_temp, target_ids_train, target_ids_temp, component_types_train, component_types_temp= train_test_split(input_ids, target_ids, component_types, test_size=0.2, random_state=42)
input_ids_val, input_ids_test, target_ids_val, target_ids_test, component_types_val, component_types_test=train_test_split(input_ids_temp, target_ids_temp, component_types_temp, test_size=0.5, random_state=42)


class GRUDataset(Dataset):
  """ Custom dataset class for description generation """
  #desc generation
  def __init__(self, input_ids, target_ids, component_types, class_weights):
    self.input_ids = input_ids
    self.target_ids = target_ids
    self.component_types = component_types
    self.class_weights = class_weights

  #get length
  def __len__(self):
    return len(self.input_ids)

  #get item
  def __getitem__(self, idx):
    input_id = self.input_ids[idx]
    target_id = self.target_ids[idx]
    component_type = self.component_types[idx]
    weight = self.class_weights.get(component_type, 1.0)
    return input_id, target_id, weight

#dataset
train_dataset= GRUDataset(input_ids_train, target_ids_train, component_types_train, class_weights)
val_dataset= GRUDataset(input_ids_val, target_ids_val, component_types_val, class_weights)
test_dataset= GRUDataset(input_ids_test, target_ids_test, component_types_test, class_weights)


#datalaoder for train test validation
train_dataloader= DataLoader(train_dataset, batch_size=32, shuffle=True)
val_dataloader= DataLoader(val_dataset, batch_size=32, shuffle=False)
test_dataloader= DataLoader(test_dataset, batch_size=32, shuffle=False)

class Seq2SeqGRU(nn.Module):
  """ Custom Seq2Seq model class for description generation """
  #inherit from torch.nn.Module!!!!!!!!!!!!!!!!
  def __init__(self, input_dim, emb_dim, hidden_dim, output_dim, num_layers):
    """ Initialize the model input_dim-input vocab size, emb_dim-embedding vector size,
    hidden_dim-hidden state no of features, output_dim-output vocab size, num_layers-recurrent layers in gru"""
    #call nn.Module constructureeeeeee of paruent
    super(Seq2SeqGRU, self).__init__()
    #create embedding layer tp convert input tokens to dense vectors to emb dim
    self.embedding = nn.Embedding(input_dim, emb_dim)
    #gru created for encoder!! emb_dim-input dimention to gru, input and output tensors are of shape (batch_size, seq_length, feature_dim)
    self.encoder = nn.GRU(emb_dim, hidden_dim, num_layers, batch_first=True)
    #gru created!!for decoder
    self.decoder = nn.GRU(emb_dim, hidden_dim, num_layers, batch_first=True)
    #create fclayer, output of decoder projected it to output output_dim
    self.fc = nn.Linear(hidden_dim, output_dim)

  def forward(self, src, tgt):
    """ forward pass of the model src-input seq(json), tgt-tgt output seq(desc) """
    #convert input tokens into dense vectors
    embedded_src = self.embedding(src)
    #passes the embedded source sequence through the encoder GRU
    #output-output of gru, hidden-hidden state of gru
    encoder_output, hidden = self.encoder(embedded_src)

    #convert tgt tokens to dense vectors
    embedded_tgt = self.embedding(tgt)
    #through decoder gru. #output-output of gru, hidden-hidden state of gru
    decoder_output, hidden = self.decoder(embedded_tgt, hidden)
  #output through fc. generate final token predictions
    output = self.fc(decoder_output)
    return output


input_dimentions= tokenizer.vocab_size
embedding_dimentions= 512
hidden_dimentions= 512
output_dimentions= tokenizer.vocab_size
layers_numbers= 3

model = Seq2SeqGRU(input_dimentions,embedding_dimentions,hidden_dimentions,output_dimentions,layers_numbers).to(device)

print(model)

optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
criterion = nn.CrossEntropyLoss()

def validate(model, dataloader, criterion):
  """ method to validate the model
 Parameters
 model- model used for validation
 dataloader- dataloader used for validation
 criterion- loss function used for validation
 Returns
 avg_loss
  """
  model.eval()
  total_loss = 0
  with torch.no_grad():#no grad
  #batchs of validation data
    for src, tgt, _ in dataloader:
      src, tgt = src.to(device), tgt.to(device)
      #exclude last token of tgt seq
      output = model(src, tgt[:, :-1])
      #calculate loss,reshape output tensor,reshape tgt tensor
      loss = criterion(output.view(-1, output_dimentions), tgt[:, 1:].reshape(-1))
      total_loss += loss.item()
  avg_loss = total_loss / len(dataloader)
  return avg_loss

def train(model, train_dataloader, val_dataloader, optimizer, criterion, epochs, checkpoint_path):
  """ method to train the model
  Parameters
  model- model used for training
  train_dataloader- dataloader used for training
  val_dataloader- dataloader used for validation
  optimizer- optimizer used for training
  criterion- loss function used for training
  epochs- number of epochs for training
  checkpoint_path- path to save the checkpoints]

  """
  model.train()
  for epoch in range(epochs):
    model.train()
    total_loss = 0
    for i, (src, tgt, weight) in enumerate(train_dataloader):
      src, tgt, weight = src.to(device), tgt.to(device), weight.to(device)
      #####c;eartrrrrr
      optimizer.zero_grad()
      output = model(src, tgt[:, :-1])
      loss = criterion(output.view(-1, output_dimentions), tgt[:, 1:].contiguous().view(-1))
      #class weight
      weighted_loss = (loss * weight.view(-1)).mean()
      #backpropagate loss
      weighted_loss.backward()
      optimizer.step()
      total_loss += weighted_loss.item()
      if i % 100 == 0:
        #every 100 batches
        print(f'Epoch {epoch+1}/{epochs}, Step {i+1}/{len(train_dataloader)}, Loss: {weighted_loss.item()}')

      avg_train_loss = total_loss / len(train_dataloader)
      avg_val_loss = validate(model, val_dataloader, criterion)

      print(f'Epoch {epoch+1} completed. Average Training Loss: {avg_train_loss}, Average Validation Loss: {avg_val_loss}')
      save_model_checkpoint(model, epoch + 1, checkpoint_path)


checkpoint_path = '/content/drive/My Drive/Implementation/Final Thesis/FYP/Prompt generation/New/GRU Model checkpoints'

if not os.path.exists(checkpoint_path):
  os.makedirs(checkpoint_path)

# print(f"Checkpoint directory: {checkpoint_path}")

def save_model_checkpoint(model, epoch, checkpoint_path):
  """ method to save the model checkpoint
  Parameters
  model- model used for saving
  epoch- epoch number"""
  checkpoint_file = os.path.join(checkpoint_path, f'seq2seq_gru_epoch{epoch}.pth')
  torch.save(model.state_dict(), checkpoint_file)
  print(f'Checkpoint saved for epoch {epoch} at {checkpoint_file}')

epochs=8

epoch_to_resume = 5

train(model, train_dataloader, val_dataloader, optimizer, criterion, epochs=8, checkpoint_path=checkpoint_path)

#NOT ALWAYS!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
def load_model_checkpoint(model, checkpoint_path, epoch):
  """ method to load the model checkpoint
  Parameters
  model- model used for loading"""
  checkpoint_file = os.path.join(checkpoint_path, f'seq2seq_gru_epoch{epoch}.pth')
  model.load_state_dict(torch.load(checkpoint_file))
  model.to(device)
  print(f'Model loaded from {checkpoint_file}')


# checkpoint_epoch = 5
# load_model_checkpoint(model, checkpoint_path, checkpoint_epoch)

from tqdm import tqdm

def train(model, train_dataloader, val_dataloader, optimizer, criterion, start_epoch, epochs, checkpoint_path):
    """ Method to train the model """
    for epoch in range(start_epoch, start_epoch + epochs):
        model.train()
        total_loss = 0
        with tqdm(total=len(train_dataloader), desc=f'Epoch {epoch+1}/{start_epoch + epochs}') as pbar:
            for i, (src, tgt, weight) in enumerate(train_dataloader):
                src, tgt, weight = src.to(device), tgt.to(device), weight.to(device)
                optimizer.zero_grad()
                output = model(src, tgt[:, :-1])
                loss = criterion(output.view(-1, output_dimentions), tgt[:, 1:].contiguous().view(-1))
                weighted_loss = (loss * weight.view(-1)).mean()
                weighted_loss.backward()
                optimizer.step()
                total_loss += weighted_loss.item()
                pbar.set_postfix({'Loss': weighted_loss.item()})
                pbar.update(1)

        avg_train_loss = total_loss / len(train_dataloader)
        avg_val_loss = validate(model, val_dataloader, criterion)

        print(f'Epoch {epoch+1} completed. Average Training Loss: {avg_train_loss}, Average Validation Loss: {avg_val_loss}')
        save_model_checkpoint(model, epoch + 1, checkpoint_path)

start_epoch = 5

load_model_checkpoint(model, checkpoint_path, start_epoch)

optimizer = torch.optim.Adam(model.parameters(), lr=0.001)


# Train the model for 5 more epochs
train(model, train_dataloader, val_dataloader, optimizer, criterion, start_epoch=start_epoch, epochs=7, checkpoint_path=checkpoint_path)

# load_model_checkpoint(model, checkpoint_path, epoch=5)

def save_model(model, checkpoint_path):
    """ Method to save the entire model """
    model_file = os.path.join(checkpoint_path, 'seq2seq_gru_model.pth')
    torch.save(model.state_dict(), model_file)
    print(f'Model saved at {model_file}')

def load_saved_model(model, checkpoint_path):
    """ Method to load the saved entire model """
    model_file = os.path.join(checkpoint_path, 'seq2seq_gru_model.pth')
    model.load_state_dict(torch.load(model_file))
    model.to(device)
    print(f'Model loaded from {model_file}')
    return model

save_model(model, checkpoint_path)

#LOAD
model = Seq2SeqGRU(input_dimentions, embedding_dimentions, hidden_dimentions, output_dimentions, layers_numbers)
model = load_saved_model(model, checkpoint_path)

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')


Inferences

def generate_description_for_json(model, json_input, tokenizer, max_len=50):
    model.eval()
    with torch.no_grad():
        inputs = tokenizer(json_input, return_tensors="pt", padding=True, truncation=True).to(device)
        input_ids = inputs['input_ids']

        # Converts the input IDs to embeddings
        encoder_outputs, hidden = model.encoder(model.embedding(input_ids))

        # Start-of-sequence token
        sos_token_id = tokenizer.cls_token_id
        # End-of-sequence token
        eos_token_id = tokenizer.sep_token_id
        # Target input with sos token
        tgt_input = torch.tensor([[sos_token_id]], device=device)

        generated_tokens = []
        for _ in range(max_len):
            # Pass through decoder
            decoder_output, hidden = model.decoder(model.embedding(tgt_input), hidden)
            # Output of decoder
            output_token = model.fc(decoder_output[:, -1, :])
            # Get next token, takes highest property (argmax) from logit and converts to Python integer
            next_token_id = torch.argmax(output_token, dim=1).item()

            if next_token_id == eos_token_id:
                break

            generated_tokens.append(next_token_id)
            tgt_input = torch.tensor([[next_token_id]], device=device)

        # Converts list of generated token IDs back to a string
        generated_description = tokenizer.decode(generated_tokens, skip_special_tokens=True)
        return generated_description


component_types = ['Button', 'Label', 'input-field', 'list-item', 'menu', 'icon-button']

from collections import defaultdict
import random
import torch.nn.functional as F

generated_descriptions = defaultdict(list)


# Shuffle the JSON data to ensure randomness
random.shuffle(json_data)


def generate_description_for_json(model, json_input, tokenizer, max_len=50, temperature=1.0):
    model.eval()
    with torch.no_grad():
        inputs = tokenizer(json_input, return_tensors="pt", padding=True, truncation=True).to(device)
        input_ids = inputs['input_ids']

        # Converts the input IDs to embeddings
        encoder_outputs, hidden = model.encoder(model.embedding(input_ids))

        # Start-of-sequence token
        sos_token_id = tokenizer.cls_token_id
        # End-of-sequence token
        eos_token_id = tokenizer.sep_token_id
        # Target input with sos token
        tgt_input = torch.tensor([[sos_token_id]], device=device)

        generated_tokens = []
        for _ in range(max_len):
            # Pass through decoder
            decoder_output, hidden = model.decoder(model.embedding(tgt_input), hidden)
            # Output of decoder
            output_token = model.fc(decoder_output[:, -1, :])
            # Apply temperature
            output_token = output_token / temperature
            # Convert logits to probabilities
            probabilities = F.softmax(output_token, dim=-1)
            # Sample the next token from the distribution
            next_token_id = torch.multinomial(probabilities, 1).item()

            if next_token_id == eos_token_id:
                break

            generated_tokens.append(next_token_id)
            tgt_input = torch.tensor([[next_token_id]], device=device)

        # Converts list of generated token IDs back to a string
        generated_description = tokenizer.decode(generated_tokens, skip_special_tokens=True)
        return generated_description

# Generate descriptions for each component type
for component_type in component_types:
    count = 0  # To track the number of descriptions generated for each type
    for item in json_data:
        if count >= 2:  # Limit to 2 descriptions per type
            break
        item_dict = json.loads(json.dumps(item))
        if item_dict.get('variant_properties', {}).get('component_name') == component_type:
            json_input = json.dumps(item)
            description = generate_description_for_json(model, json_input, tokenizer)
            generated_descriptions[component_type].append({'json': json_input, 'description': description})
            count += 1


# Print the generated descriptions and corresponding JSONs
for component_type, entries in generated_descriptions.items():
    print(f"Component: {component_type}")
    for entry in entries:
        print(f"JSON: {entry['json']}")
        print(f"Generated Description: {entry['description']}")
        print()

json_input = {
    "variant_properties": {
        "color": "rgba(102, 112, 132, 1.0)",
        "strokes": ["rgba(207, 212, 220, 1.0)", "rgba(102, 112, 132, 1.0)", "rgba(152, 161, 178, 1.0)"],
        "strokeWeight": 1.0,
        "text": "This is a hint text to help user.",
        "textColor": "rgba(102, 112, 132, 1.0)",
        "borderRadius": 8.0,
        "fontFamily": "Inter",
        "fontWeight": 400,
        "fontSize": 14.0,
        "effects": [{"type": "DROP_SHADOW", "color": "rgba(16, 24, 40, 0.05000000074505806)"}],
        "padding": 0,
        "width": 320.0,
        "height": 20.0,
        "x": 8092.0,
        "y": -1861.0,
        "style": "Professional",
        "component_name": "menu",
        "subtype": "Default",
        "variant_details": {"State": ["Default"]}
    }
}

json_input_str = json.dumps(json_input)


description = generate_description_for_json(model, json_input_str, tokenizer)

print(f"Generated Description: {description}")

example_json1 = "{\"style\": \"Professional\", \"component_name\": \"Button\", \"subtype\": \"Default\", \"variant_details\": {\"State\": [\"Default\"], \"Size\": [\"Small\"]}, \"name\": \"State=Default, Size=Small\", \"type\": \"COMPONENT\", \"backgroundColor\": {\"r\": 0.0, \"g\": 0.0, \"b\": 0.0, \"a\": 0.0}, \"absoluteBoundingBox\": {\"x\": -4633.0, \"y\": -2143.0, \"width\": 105.0, \"height\": 36.0}, \"constraints\": {\"vertical\": \"TOP\", \"horizontal\": \"LEFT\"}, \"children\": [{\"name\": \"_Button base\", \"type\": \"FRAME\", \"backgroundColor\": {\"r\": 0.49640899896621704, \"g\": 0.33841174840927124, \"b\": 0.849823534488678, \"a\": 1.0}, \"absoluteBoundingBox\": {\"x\": -4633.0, \"y\": -2143.0, \"width\": 105.0, \"height\": 36.0}, \"constraints\": {\"vertical\": \"TOP\", \"horizontal\": \"LEFT\"}, \"children\": [{\"name\": \"Text\", \"type\": \"TEXT\", \"backgroundColor\": {}, \"absoluteBoundingBox\": {\"x\": -4619.0, \"y\": -2135.0, \"width\": 77.0, \"height\": 20.0}, \"constraints\": {\"vertical\": \"TOP\", \"horizontal\": \"LEFT\"}, \"children\": [], \"fills\": [{\"blendMode\": \"NORMAL\", \"type\": \"SOLID\", \"color\": {\"r\": 1.0, \"g\": 1.0, \"b\": 1.0, \"a\": 1.0}}], \"strokes\": [], \"strokeWeight\": 1.0, \"effects\": [], \"characters\": \"Button CTA\", \"styles\": {\"fill\": \"1:4\", \"text\": \"1:8\"}, \"itemSpacing\": 0, \"visible\": true}], \"fills\": [{\"blendMode\": \"NORMAL\", \"type\": \"SOLID\", \"color\": {\"r\": 0.49640899896621704, \"g\": 0.33841174840927124, \"b\": 0.849823534488678, \"a\": 1.0}}], \"strokes\": [{\"blendMode\": \"NORMAL\", \"type\": \"SOLID\", \"color\": {\"r\": 0.49640899896621704, \"g\": 0.33841174840927124, \"b\": 0.849823534488678, \"a\": 1.0}}], \"strokeWeight\": 1.0, \"effects\": [{\"type\": \"DROP_SHADOW\", \"visible\": true, \"color\": {\"r\": 0.062745101749897, \"g\": 0.0941176488995552, \"b\": 0.1568627506494522, \"a\": 0.05000000074505806}, \"blendMode\": \"NORMAL\", \"offset\": {\"x\": 0.0, \"y\": 1.0}, \"radius\": 2.0, \"showShadowBehindNode\": true}], \"characters\": \"\", \"styles\": {\"strokes\": \"1:2\", \"stroke\": \"1:2\", \"effect\": \"1:3\"}, \"itemSpacing\": 8.0, \"visible\": true}], \"fills\": [{\"blendMode\": \"NORMAL\", \"visible\": false, \"type\": \"SOLID\", \"color\": {\"r\": 1.0, \"g\": 1.0, \"b\": 1.0, \"a\": 1.0}}], \"strokes\": [], \"strokeWeight\": 1.0, \"effects\": [], \"characters\": \"\", \"styles\": {}, \"itemSpacing\": 0, \"visible\": true}"
example_json2 = "{\"style\": \"Basic\", \"component_name\": \"input field\", \"subtype\": \"Default\", \"variant_details\": {\"State\": [\"Focused\"], \"Size\": [\"Medium\"]}, \"name\": \"State=Focused, Size=Medium\", \"type\": \"COMPONENT\", \"backgroundColor\": {\"r\": 1.0, \"g\": 1.0, \"b\": 1.0, \"a\": 1.0}, \"absoluteBoundingBox\": {\"x\": -200.0, \"y\": -400.0, \"width\": 200.0, \"height\": 50.0}, \"constraints\": {\"vertical\": \"TOP\", \"horizontal\": \"LEFT\"}, \"children\": [{\"name\": \"_Input base\", \"type\": \"FRAME\", \"backgroundColor\": {\"r\": 0.9, \"g\": 0.9, \"b\": 0.9, \"a\": 1.0}, \"absoluteBoundingBox\": {\"x\": -200.0, \"y\": -400.0, \"width\": 200.0, \"height\": 50.0}, \"constraints\": {\"vertical\": \"TOP\", \"horizontal\": \"LEFT\"}, \"children\": [{\"name\": \"Placeholder\", \"type\": \"TEXT\", \"backgroundColor\": {}, \"absoluteBoundingBox\": {\"x\": -190.0, \"y\": -390.0, \"width\": 180.0, \"height\": 30.0}, \"constraints\": {\"vertical\": \"TOP\", \"horizontal\": \"LEFT\"}, \"children\": [], \"fills\": [{\"blendMode\": \"NORMAL\", \"type\": \"SOLID\", \"color\": {\"r\": 0.6, \"g\": 0.6, \"b\": 0.6, \"a\": 1.0}}], \"strokes\": [], \"strokeWeight\": 1.0, \"effects\": [], \"characters\": \"Enter text here\", \"styles\": {\"fill\": \"1:4\", \"text\": \"1:8\"}, \"itemSpacing\": 0, \"visible\": true}], \"fills\": [{\"blendMode\": \"NORMAL\", \"type\": \"SOLID\", \"color\": {\"r\": 0.9, \"g\": 0.9, \"b\": 0.9, \"a\": 1.0}}], \"strokes\": [{\"blendMode\": \"NORMAL\", \"type\": \"SOLID\", \"color\": {\"r\": 0.8, \"g\": 0.8, \"b\": 0.8, \"a\": 1.0}}], \"strokeWeight\": 1.0, \"effects\": [], \"characters\": \"\", \"styles\": {\"strokes\": \"1:2\", \"stroke\": \"1:2\", \"effect\": \"1:3\"}, \"itemSpacing\": 8.0, \"visible\": true}], \"fills\": [{\"blendMode\": \"NORMAL\", \"visible\": false, \"type\": \"SOLID\", \"color\": {\"r\": 1.0, \"g\": 1.0, \"b\": 1.0, \"a\": 1.0}}], \"strokes\": [], \"strokeWeight\": 1.0, \"effects\": [], \"characters\": \"\", \"styles\": {}, \"itemSpacing\": 0, \"visible\": true}"


generated_description1 = generate_description_for_json(model, example_json1, tokenizer)
generated_description2 = generate_description_for_json(model, example_json2, tokenizer)


print(f"Generated description for example 1: {generated_description1}")
print(f"Generated description for example 2: {generated_description2}")

example_json1 = ""

def generate_prompts(model, tokenizer, json_input, device, num_return_sequences=5):
  """inference method
  Parameters
  model- modelinput
  tokenizer-tokenizer
  json_input- inpur from dataset
  device-cuda or not
  num_return_sequences- number of return sequences

  Returns
  prompts- generated prompts
  """
  model.to(device)
  model.eval()
  input_text = "generate prompt: " + json.dumps(json_input)
  input_ids = tokenizer(input_text, return_tensors="pt", padding=True, truncation=True, max_length=512).input_ids.to(device)

  generated_ids = model.generate(
    input_ids,
    max_length=100,
    num_return_sequences=num_return_sequences,
    #randomness
    temperature=1.5,
    #top k candidates
    top_k=50,
    #nucleus sampling
    top_p=0.95,
    #sampling
    do_sample=True,
    no_repeat_ngram_size=2,
    early_stopping=False,
  )

  prompts = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]
  return prompts

generated_description1 = generate_description_for_json(model, sample_json , tokenizer)


print(f"Generated description for example 1: {generated_description1}")

def generate_descriptions_for_dataset(model, data, tokenizer, max_len=50):
    model.eval()
    generated_descriptions = []
    with torch.no_grad():
        for item in data:
            json_input = item['json']
            generated_description = generate_description_for_json(model, json_input, tokenizer, max_len)
            generated_descriptions.append(generated_description)
    return generated_descriptions

generated_descriptions = generate_descriptions_for_dataset(model, data, tokenizer)

gru_descriptions_path ='/content/drive/My Drive/Implementation/Final Thesis/FYP/Data/gru_generated_data.txt'

def save_generated_descriptions(generated_descriptions, output_file_path):
  with open(output_file_path, 'w') as f:
    for description in generated_descriptions:
      f.write(description + '\n')

def load_generated_descriptions(file_path):
  with open(file_path, 'r') as f:
    generated_descriptions = [line.strip() for line in f]
  return generated_descriptions

save_generated_descriptions(generated_descriptions, gru_descriptions_path)

generated_descriptions = load_generated_descriptions(gru_descriptions_path)

# gjbh
# vgjh
# ieuhrfj


from collections import defaultdict

Bleu

reference_dict = {}
for item in data:
    json_str = item['json']
    description = item['description']
    if json_str not in reference_dict:
        reference_dict[json_str] = []
    reference_dict[json_str].append(description)

json_inputs = [item['json'] for item in data]  # List of JSON inputs in the same order as generated_descriptions


references = []
hypotheses = []

for json_input, generated_description in zip(json_inputs, generated_descriptions):
    references.append([ref.split() for ref in reference_dict[json_input]])  # Multiple references for each JSON input
    hypotheses.append(generated_description.split())  # Generated description


bleu_score = corpus_bleu(references, hypotheses, smoothing_function=SmoothingFunction().method1)
print(f'BLEU Score: {bleu_score}')

print("Example Reference: ", references[0])
print("Example Hypothesis: ", hypotheses[0])

!pip install rouge-score


from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction
from rouge_score import rouge_scorer
# from nltk.translate.meteor_score import meteor_score
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction

from pycocoevalcap.cider.cider import Cider
from pycocoevalcap.tokenizer.ptbtokenizer import PTBTokenizer

#initialise rouge scorer
scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)


#calculate
rouge_scores = [scorer.score(ref, hyp) for ref, hyp in zip([item['description'] for item in data], generated_descriptions)]


rouge1_scores = []
rouge2_scores = []
rougeL_scores = []


for i, (refs, hyp) in enumerate(zip(references, hypotheses)):
    if i % 100 == 0:
        print(f'Processing {i}/{len(references)}')

    refs_text = [' '.join(ref) for ref in refs]
    hyp_text = ' '.join(hyp)
    scores = [scorer.score(ref, hyp_text) for ref in refs_text]

    # Take the maximum score among the multiple references
    rouge1_scores.append(max(score['rouge1'].fmeasure for score in scores))
    rouge2_scores.append(max(score['rouge2'].fmeasure for score in scores))
    rougeL_scores.append(max(score['rougeL'].fmeasure for score in scores))


#average score
rouge1 = sum(rouge1_scores) / len(rouge1_scores)
rouge2 = sum(rouge2_scores) / len(rouge2_scores)
rougeL = sum(rougeL_scores) / len(rougeL_scores)

print(f'ROUGE-1: {rouge1}')
print(f'ROUGE-2: {rouge2}')
print(f'ROUGE-L: {rougeL}')

def average_rouge_scores(rouge_scores):
    rouge1 = sum(score['rouge1'].fmeasure for score in rouge_scores) / len(rouge_scores)
    rouge2 = sum(score['rouge2'].fmeasure for score in rouge_scores) / len(rouge_scores)
    rougeL = sum(score['rougeL'].fmeasure for score in rouge_scores) / len(rouge_scores)
    return rouge1, rouge2, rougeL

avg_rouge1, avg_rouge2, avg_rougeL = average_rouge_scores(rouge_scores)


print(f'Average ROUGE-1: {avg_rouge1}')
print(f'Average ROUGE-2: {avg_rouge2}')
print(f'Average ROUGE-L: {avg_rougeL}')

from sklearn.metrics import precision_score, recall_score, f1_score
from collections import Counter

def token_level_metrics(references, hypotheses):
    ref_tokens = [token for ref in references for token in ref.split()]
    hyp_tokens = [token for hyp in hypotheses for token in hyp.split()]

    ref_counter = Counter(ref_tokens)
    hyp_counter = Counter(hyp_tokens)

    common_tokens = set(ref_tokens) & set(hyp_tokens)

    precision = sum(hyp_counter[token] for token in common_tokens) / sum(hyp_counter.values())
    recall = sum(ref_counter[token] for token in common_tokens) / sum(ref_counter.values())
    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0

    return precision, recall, f1


precision, recall, f1 = token_level_metrics(references, generated_descriptions)


print(f'Precision: {precision}')
print(f'Recall: {recall}')
print(f'F1 Score: {f1}')

def token_level_accuracy(references, hypotheses):
    total_tokens = 0
    matching_tokens = 0

    for refs, hyp in zip(references, hypotheses):
        ref_tokens = set(token for ref in refs for token in ref)
        hyp_tokens = set(hyp)
        total_tokens += len(hyp_tokens)
        matching_tokens += len(hyp_tokens & ref_tokens)

    return matching_tokens / total_tokens

accuracy = token_level_accuracy(references, hypotheses)


print(f'Token-level Accuracy: {accuracy}')

## Stats

def calculate_bleu(reference, hypothesis):
  """ method to calculate the bleu score
  Parameters
  reference- reference description
  hypothesis- generated description
  Returns
  bleu score
  """
  smooth = SmoothingFunction().method4
  return sentence_bleu([reference.split()], hypothesis.split(), smoothing_function=smooth)


def evaluate_model(model, val_dataloader, tokenizer):
  """ method to evaluate the model
  Parameters
  model- model used for evaluation
  val_dataloader- dataloader used for evaluation
  tokenizer- tokenizer used for evaluation
  Returns
  avg_bleu_score
  bleu_score_std
  """
  model.eval()
  total_bleu_score = 0
  num_samples = 0
  bleu_scores = []

  with torch.no_grad():
    for src, tgt in val_dataloader:
      src = src.to(device)
      tgt = tgt.to(device)
      for i in range(src.size(0)):
        json_input = tokenizer.decode(src[i], skip_special_tokens=True)
        generated_desc = generate_description_for_json(model, json_input, tokenizer)
        reference_desc = tokenizer.decode(tgt[i], skip_special_tokens=True)
        bleu_score = calculate_bleu(reference_desc, generated_desc)
        total_bleu_score += bleu_score
        bleu_scores.append(bleu_score)
        num_samples += 1

  avg_bleu_score = total_bleu_score / num_samples
  bleu_score_std = np.std(bleu_scores)
  print(f'Average BLEU Score: {avg_bleu_score}')
  print(f'BLEU Score Standard Deviation: {bleu_score_std}')
  return avg_bleu_score, bleu_score_std

import numpy as np

avg_bleu_score, bleu_score_std = evaluate_model(model, val_dataloader, tokenizer)


print(f"Average BLEU Score: {avg_bleu_score}")
print(f"BLEU Score Standard Deviation: {bleu_score_std}")

import random
import matplotlib.pyplot as plt
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, roc_curve, roc_auc_score
import seaborn as sns
import re
from nltk.translate.bleu_score import corpus_bleu

# Function to extract component name from description
def get_component_name_from_description(description):
    components = ['icon-button', 'button', 'label', 'input-field', 'menu', 'list-item']
    description_lower = description.lower()

    # Normalize description by removing spaces and hyphens
    normalized_description = description_lower.replace(" ", "").replace("-", "")

    for component in components:
        normalized_component = component.replace("-", "")
        if normalized_component in normalized_description:
            return component

    return 'Unknown'

# Function to calculate component name metrics
def calculate_component_metrics(true_components, generated_components):
    accuracy = accuracy_score(true_components, generated_components) * 100
    precision = precision_score(true_components, generated_components, average='weighted') * 100
    recall = recall_score(true_components, generated_components, average='weighted') * 100
    f1 = f1_score(true_components, generated_components, average='weighted') * 100
    return accuracy, precision, recall, f1


def evaluate_model(model, dataset, tokenizer, sample_size=10, max_len=50):
    sample_indices = random.sample(range(len(dataset)), sample_size)

    true_component_names = []
    generated_component_names = []

    for idx in sample_indices:
        json_input = dataset[idx]['json']
        true_descriptions_sample = dataset[idx]['description']  # assuming this returns a list of descriptions
        true_component_name = json.loads(json_input).get('variant_properties', {}).get('component_name', 'Unknown').lower()

        generated_description = generate_description_for_json(model, json_input, tokenizer, max_len)
        generated_component_name = get_component_name_from_description(generated_description)

        true_component_names.append(true_component_name)
        generated_component_names.append(generated_component_name)

        print(f"JSON Input: {json_input}")
        print(f"True Descriptions: {true_descriptions_sample}")
        print(f"Generated Description: {generated_description}")
        print(f"True Component Name: {true_component_name}")
        print(f"Generated Component Name: {generated_component_name}")
        print("----")

    component_accuracy, component_precision, component_recall, component_f1 = calculate_component_metrics(true_component_names, generated_component_names)

    metrics = {
        'Component Accuracy (%)': component_accuracy,
        'Component Precision (%)': component_precision,
        'Component Recall (%)': component_recall,
        'Component F1 (%)': component_f1,
    }

    return metrics, true_component_names, generated_component_names

def progressive_evaluation(model, dataset, tokenizer, max_len=50):
    sample_sizes = [100, 200, 300, 400, 500]  # Different sample sizes for progressive evaluation
    accuracy_list = []
    precision_list = []
    recall_list = []
    f1_list = []

    all_true_components = []
    all_generated_components = []

    for sample_size in sample_sizes:
        metrics, true_components, generated_components = evaluate_model(model, dataset, tokenizer, sample_size=sample_size, max_len=max_len)
        accuracy_list.append(metrics['Component Accuracy (%)'])
        precision_list.append(metrics['Component Precision (%)'])
        recall_list.append(metrics['Component Recall (%)'])
        f1_list.append(metrics['Component F1 (%)'])
        all_true_components.extend(true_components)
        all_generated_components.extend(generated_components)
        print(f"Sample Size: {sample_size}, Metrics: {metrics}")

    # Plotting
    plt.figure(figsize=(10, 6))
    plt.plot(sample_sizes, accuracy_list, label='Accuracy (%)', marker='o')
    plt.plot(sample_sizes, precision_list, label='Precision (%)', marker='o')
    plt.plot(sample_sizes, recall_list, label='Recall (%)', marker='o')
    plt.plot(sample_sizes, f1_list, label='F1 Score (%)', marker='o')
    plt.xlabel('Sample Size')
    plt.ylabel('Percentage')
    plt.title('Progressive Evaluation Metrics')
    plt.legend()
    plt.grid(True)
    plt.show()

    # Confusion Matrix
    labels = list(set(all_true_components))
    cm = confusion_matrix(all_true_components, all_generated_components, labels=labels)

    plt.figure(figsize=(10, 7))
    sns.heatmap(cm, annot=True, fmt='d', xticklabels=labels, yticklabels=labels, cmap='Blues')
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title('Confusion Matrix')
    plt.show()

    # ROC-AUC for each class
    plt.figure(figsize=(10, 7))
    for label in labels:
        true_binary = [1 if x == label else 0 for x in all_true_components]
        pred_binary = [1 if x == label else 0 for x in all_generated_components]
        fpr, tpr, _ = roc_curve(true_binary, pred_binary)
        auc = roc_auc_score(true_binary, pred_binary)
        plt.plot(fpr, tpr, label=f'{label} (AUC = {auc:.2f})')

    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC-AUC for Each Class')
    plt.legend()
    plt.grid(True)
    plt.show()

progressive_evaluation(model, data, tokenizer, max_len=50)

