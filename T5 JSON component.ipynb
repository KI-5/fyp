#gdrive
from google.colab import drive
drive.mount('/content/drive')

## Libaries

!pip install transformers[torch] accelerate -U

!pip install torch torchvision transformers

!pip install datasets
!pip install rouge-score

#imports
import os
import json
import torch
from transformers import T5Tokenizer, T5ForConditionalGeneration, BertTokenizer, BertModel
from torch.utils.data import Dataset, DataLoader, random_split
from transformers import TrainingArguments, Trainer, DataCollatorForSeq2Seq
from sklearn.model_selection import train_test_split
import tensorflow as tf
import torch

import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
from collections import defaultdict
import random

torch.cuda.empty_cache()

torch.cuda.is_available()

tf.test.gpu_device_name()

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

print(f"Using device: {device}")

!nvidia-smi

## Simple

#lists
json_data=[]
prompts=[]

# load json-only
json_only= '/content/drive/My Drive/Implementation/Final Thesis/FYP/Data/full_json_only.jsonl'

with open(json_only, 'r', encoding='utf-8') as f:
  for line in f:
    json_data.append(json.loads(line))

# load prompt only
prompt_only ='/content/drive/My Drive/Implementation/Final Thesis//FYP/Data/t5_descriptions.txt'

with open(prompt_only, 'r', encoding='utf-8') as f:
  prompts=[line.strip() for line in f]

print(f"Number of prompts: {len(prompts)}")
print(f"Number of JSON entries: {len(json_data)}")

#length of prompts
prompt_lengths= [len(prompt.split()) for prompt in prompts]
print(f"Average prompt length: {sum(prompt_lengths)/len(prompt_lengths):.2f} words")


#plotting the distribution of prompt lengths
plt.figure(figsize=(10, 5))
plt.hist(prompt_lengths, bins=30, color='skyblue', edgecolor='black')
plt.title('Distribution of Prompt Lengths')
plt.xlabel('Number of Words')
plt.ylabel('Frequency')
plt.show()


#lenth of json entries
json_lengths=[len(entry.keys()) for entry in json_data]
print(f"Average JSON length: {sum(json_lengths)/len(json_lengths):.2f} keys")


#plotting json length distribution
plt.figure(figsize=(10, 5))
plt.hist(json_lengths, bins=30, color='lightgreen', edgecolor='black')
plt.title('Distribution of JSON lengths')
plt.xlabel('Number of kys')
plt.ylabel('Frequency')
plt.show()

#most common words in prompts
all_words= ' '.join(prompts).split()
word_freq= Counter(all_words)
common_words =word_freq.most_common(20)
words, freqs= zip(*common_words)


#plot common words
plt.figure(figsize=(12, 6))
sns.barplot(x=list(words), y=list(freqs), palette='viridis')
plt.title('Most Common Words in Prompts')
plt.xlabel('Words')
plt.ylabel('Frequency')
plt.xticks(rotation=45)
plt.show()

#testing
print("Example JSON entry:")
print(json.dumps(json_data[0], indent=2))

class UIDataset(Dataset):
  """class for loading the dataset
  """
  def __init__(self, prompts, json_data):
    """ method to initialise the dataset
    Parameters
    prompts: list of prompts
    json_data: list of json entries
    """
    #initialise prompts data
    self.prompts= prompts
    #initialise json data
    self.json_data =json_data
    #loading the t5 tokenizer
    self.tokenizer= T5Tokenizer.from_pretrained('t5-small')

  def __len__(self):
    """ method to return the length of the dataset
    """
    #length of dataset
    return len(self.prompts)

  def __getitem__(self, idx):
    """ method ot return the item at the given index
    Parameters
    idx: index of the item
    Returns
    input_ids: input ids of the prompt
    attention_mask: attention mask of the prompt
    labels: labels of the json entry
    """
    #get by their indices
    prompt= self.prompts[idx]
    json_data =json.dumps(self.json_data[idx])
    #tokenize prompt
    inputs= self.tokenizer(prompt, return_tensors='pt', padding='longest', truncation=True)
    #tokenize json data
    targets= self.tokenizer(json_data, return_tensors='pt', padding='longest', truncation=True)
    return {
      'input_ids': inputs['input_ids'].squeeze(),
      'attention_mask': inputs['attention_mask'].squeeze(),
      'labels': targets['input_ids'].squeeze()
    }

dataset= UIDataset(prompts, json_data)
#split to train validation and test sets
train_size =int(0.8 * len(dataset))
val_size= int(0.1 * len(dataset))
test_size= len(dataset) - train_size - val_size

train_dataset, val_dataset, test_dataset= random_split(dataset, [train_size, val_size, test_size])

#dataloader instances
train_loader= DataLoader(train_dataset, batch_size=4, shuffle=True)
val_loader =DataLoader(val_dataset, batch_size=4)
test_loader= DataLoader(test_dataset, batch_size=4)


#loading tokenizer and model
tokenizer= T5Tokenizer.from_pretrained('t5-small')
model =T5ForConditionalGeneration.from_pretrained('t5-small')

epochs=12

#training args for Trainer
training_args= TrainingArguments(
  output_dir='/content/drive/My Drive/Implementation/Final Thesis/FYP/results',
  num_train_epochs=epochs,
  #trainning batch size
  per_device_train_batch_size=4,
  #eval batch size
  per_device_eval_batch_size=4,
  save_steps=500,
  save_total_limit=2,
  logging_steps=10,
  eval_steps=500,
  evaluation_strategy="steps",
  prediction_loss_only=True,
  load_best_model_at_end=True,
  #only if gpu avalable
  fp16=torch.cuda.is_available(),
  #no to push to HF
  push_to_hub=False
)

#data collator for seq2seq
data_collator= DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)

#Trainer instance
trainer= Trainer(
  model=model,
  args=training_args,
  train_dataset=train_dataset,
  eval_dataset=val_dataset,
  data_collator=data_collator
)

#train here first time!
trainer.train()

#save to drive
model_save_path= '/content/drive/My Drive/Implementation/Final Thesis/FYP/t5_model'
model.save_pretrained(model_save_path)
tokenizer.save_pretrained(model_save_path)

model_save_path= '/content/drive/My Drive/Implementation/Final Thesis/FYP/t5_model'

def load_model_and_tokenizer(model_path):
  """ method to load the model and tokenizer
  Parameters
  model_path: path to the model and tokenizer
  Return
  model: loaded model
  tokenizer: loaded tokenizer
  """
    model= T5ForConditionalGeneration.from_pretrained(model_path)
    tokenizer =T5Tokenizer.from_pretrained(model_path)
    return model,tokenizer

#define th emodel and trokenizer from loaded
model, tokenizer = load_model_and_tokenizer(model_save_path)

def generate_json(prompt, model, tokenizer, max_length=512):
  """ method to generate the json from the prompt
  Parameters
  prompt: prompt to generate the json
  model: model to generate the json
  tokenizer: tokenizer to generate the json
  max_length: maximum length of the json"""
  #tokenizing prompt
  inputs =tokenizer(prompt, return_tensors='pt', padding='max_length', truncation=True, max_length=max_length).to(device)
  #generate from model
  outputs= model.generate(inputs['input_ids'], max_length=max_length, num_beams=5, early_stopping=True)
  #decode output to json string
  generated_json_str = tokenizer.decode(outputs[0], skip_special_tokens=True)
  return generated_json_str

#example
prompt= "Generate a Professional Button with State of Default, Size of Small."

generated_json= generate_json(prompt, model, tokenizer)

print("Generated JSON:", generated_json)

results= trainer.evaluate(eval_dataset=val_dataset)

print("Evaluation results:", results)

## Training again!

import json
import torch
from torch.utils.data import Dataset, DataLoader, random_split
from transformers import BertTokenizer, BertModel, T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments, DataCollatorForSeq2Seq
from transformers.modeling_outputs import Seq2SeqLMOutput


#load paired dataset
pairs_file_1= '/content/drive/My Drive/Implementation/Final Thesis/FYP/Data/t5_json_description_pairs.jsonl'

#define
data_pairs_1 = []

with open(pairs_file_1, 'r', encoding='utf-8') as f:
    for line in f:
        data_pairs_1.append(json.loads(line))


#json is input in the dataset
json_data_1 = [pair['input'] for pair in data_pairs_1]

#description is output in the dataset
prompts_1 = [pair['output'] for pair in data_pairs_1]

#lengths
print(f"Number of pairs: {len(data_pairs_1)}")

print(f"Number of jsno: {len(json_data_1)}")

print(f"Number of description: {len(prompts_1)}")


for i in range(2):
    print(f"JSON: {json_data_1[i]}")
    print(f"Description: {prompts_1[i]}")


# #load dataset
# pairs_file_2= '/content/drive/My Drive/Implementation/Final Thesis/FYP/Data/pairs_data.jsonl'

# data_pairs_2 = []


# with open(pairs_file_2, 'r', encoding='utf-8') as f:
#     data_pairs_2 = json.load(f)

# json_to_descriptions = defaultdict(list)

# for pair in data_pairs_2:
#     json_to_descriptions[pair['json']].append(pair['description'])


# filtered_data_pairs_2 = defaultdict(list)

# for pair in data_pairs_2:
#     json_key = pair['json']
#     if len(filtered_data_pairs_2[json_key]) < 4:
#         filtered_data_pairs_2[json_key].append(pair)


# filtered_data_pairs_2_list = [pair for pairs in filtered_data_pairs_2.values() for pair in pairs]


# json_data_2 = [pair['json'] for pair in filtered_data_pairs_2_list]


# prompts_2 = [pair['description'] for pair in filtered_data_pairs_2_list]

# print(f"Number of json: {len(json_data_2)}")

# print(f"Number of pairs after filtering: {len(filtered_data_pairs_2_list)}")

# print(f"Number of unique JSONs: {len(filtered_data_pairs_2)}")

# for i in range(2):
#     print(f"JSON: {json_data_2[i]}")
#     print(f"Description: {prompts_2[i]}")


# combined_json_data = json_data_1 + json_data_2

# combined_prompts = prompts_1 + prompts_2



# print(f"Number of pairs: {len(combined_prompts)}")

# for i in range(4):
#     print(f"JSON: {combined_json_data[i]}")
#     print(f"Description: {combined_prompts[i]}")

class UIDataset(Dataset):
  """class for loading the dataset
  """
  def __init__(self, prompts, json_data):
    """ method to initialise the dataset
    Parameters
    prompts: list of prompts
    json_data: list of json entries
    """
    #initialise prompts data
    self.prompts= prompts
    #initialise json data
    self.json_data =json_data
    #loading the t5 tokenizer
    self.tokenizer= T5Tokenizer.from_pretrained('t5-small')

  def __len__(self):
    """ method to return the length of the dataset
    """
    #length of dataset
    return len(self.prompts)

  def __getitem__(self, idx):
    """ method ot return the item at the given index
    Parameters
    idx: index of the item
    Returns
    input_ids: input ids of the prompt
    attention_mask: attention mask of the prompt
    labels: labels of the json entry
    """
    #get by their indices
    prompt= self.prompts[idx]
    json_entry =json.dumps(self.json_data[idx])
     #tokenize prompt
    inputs= self.tokenizer(prompt, return_tensors='pt', padding='longest', truncation=True)
    #tokenize json data
    targets= self.tokenizer(json_entry, return_tensors='pt', padding='longest', truncation=True)
    return {
      'input_ids': inputs['input_ids'].squeeze(),
      'attention_mask': inputs['attention_mask'].squeeze(),
      'labels': targets['input_ids'].squeeze()
    }

dataset = UIDataset(prompts_1,json_data_1)
#split to train validation and test sets
train_size = int(0.8 * len(dataset))
val_size = int(0.1 * len(dataset))
test_size = len(dataset) - train_size - val_size

train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])


#dataloader instances
train_loader=DataLoader(train_dataset, batch_size=8, shuffle=True)
val_loader=DataLoader(val_dataset, batch_size=8)
test_loader= DataLoader(test_dataset, batch_size=8)


#loading tokenizer and model
tokenizer=T5Tokenizer.from_pretrained('t5-small')
model=T5ForConditionalGeneration.from_pretrained('t5-small')

#training args for Trainer
training_args= TrainingArguments(
  output_dir='/content/drive/My Drive/Implementation/Final Thesis/FYP/JSON generation/T5 NORMAL SIMPLE',
  num_train_epochs=epochs,
  #trainning batch size
  per_device_train_batch_size=32,
  #eval batch size
  per_device_eval_batch_size=32,
  save_steps=500,
  save_total_limit=2,
  logging_steps=10,
  eval_steps=500,
  evaluation_strategy="steps",
  prediction_loss_only=True,
  load_best_model_at_end=True,
  #run! fast
  fp16=True,
  #no to push to HF
  push_to_hub=False
)

#data collator for seq2seq
data_collator= DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)

#Trainer instance
trainer= Trainer(
  model=model,
  args=training_args,
  train_dataset=train_dataset,
  eval_dataset=val_dataset,
  data_collator=data_collator
)

#train here first time!
trainer.train()

#path to save MODEL
model_save_path= '/content/drive/My Drive/Implementation/Final Thesis/FYP/JSON generation/T5 NORMAL SIMPLE/'

model.save_pretrained(model_save_path)

tokenizer.save_pretrained(model_save_path)

def load_model_and_tokenizer(model_path):
   """ method to load the model and tokenizer
  Parameters
  model_path: path to the model and tokenizer
  Return
  model: loaded model
  tokenizer: loaded tokenizer
  """
  model= T5ForConditionalGeneration.from_pretrained(model_path)
  tokenizer= T5Tokenizer.from_pretrained(model_path)
  return model,tokenizer

#define th emodel and trokenizer from loaded
model, tokenizer=load_model_and_tokenizer(model_save_path)

def generate_json(prompt, model, tokenizer, max_length=1024):
 """ method to generate the json from the prompt
  Parameters
  prompt: prompt to generate the json
  model: model to generate the json
  tokenizer: tokenizer to generate the json
  max_length: maximum length of the json"""
  device='cuda' if torch.cuda.is_available() else 'cpu'
  model.to(device)
  #tokenizing prompt
  inputs= tokenizer(prompt, return_tensors='pt', padding='max_length', truncation=True, max_length=max_length).to(device)
  #generate from model
  outputs =model.generate(inputs['input_ids'],max_length=max_length,num_beams=5,early_stopping=True,temperature=0)
  #decode output to json string
  generated_json_str = tokenizer.decode(outputs[0], skip_special_tokens=True)
  return generated_json_str

device = 'cuda' if torch.cuda.is_available() else 'cpu'
model.to(device)

results= trainer.evaluate(eval_dataset=val_dataset)

print("Evaluation results:", results)

## Nested

#lists
json_data=[]
prompts=[]

# load json-only
json_only_input= '/content/drive/My Drive/Implementation/Final Thesis/FYP/Data/nested_dataset.jsonl'

# json_only= '/content/drive/My Drive/Implementation/Final Thesis/FYP/Data/nested_dataset_5.jsonl'

jsons = []

# with open(json_only_input, 'r', encoding='utf-8') as f:
#   for line in f:
#     json_data = json.loads(line)
#    #get 5 times
#     for _ in range(5):
#       jsons.append(json_data)

with open(json_only, 'w', encoding='utf-8') as f:
  for data in jsons:
    f.write(json.dumps(data) + '\n')

#load prompt only
prompt_only ='/content/drive/My Drive/Implementation/Final Thesis//FYP/Data/t5_descriptions.txt'

with open(prompt_only, 'r', encoding='utf-8') as f:
  prompts=[line.strip() for line in f]

print(f"Number of prompts: {len(prompts)}")
print(f"Number of JSON entries: {len(jsons)}")

class UIDataset(Dataset):
  """class for loading the dataset
  """
  def __init__(self, prompts, json_data):
    """ method to initialise the dataset
    Parameters
    prompts: list of prompts
    json_data: list of json entries
    """
    self.prompts= prompts
    self.json_data =json_data
    self.tokenizer= T5Tokenizer.from_pretrained('t5-small')

  def __len__(self):
    """ method to return the length of the dataset """
    return len(self.prompts)

  def __getitem__(self, idx):
    """ method ot return the item at the given index
    Parameters
    idx: index of the item
    Returns
    input_ids: input ids of the prompt
    attention_mask: attention mask of the prompt
    labels: labels of the json entry
    """
    #get by their indices
    prompt= self.prompts[idx]
    json_entry =json.dumps(self.json_data[idx])
    #tokenize prompt
    inputs= self.tokenizer(prompt, return_tensors='pt', padding='longest', truncation=True)
    #tokenize json data
    targets= self.tokenizer(json_entry, return_tensors='pt', padding='longest', truncation=True)
    return {
      'input_ids': inputs['input_ids'].squeeze(),
      'attention_mask': inputs['attention_mask'].squeeze(),
      'labels': targets['input_ids'].squeeze()
    }

dataset = UIDataset(prompts,jsons)
#split to train validation and test sets
train_size = int(0.8 * len(dataset))
val_size = int(0.1 * len(dataset))
test_size = len(dataset) - train_size - val_size

train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])

#dataloader instances
train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=8)
test_loader = DataLoader(test_dataset, batch_size=8)


#loading tokenizer and model
tokenizer = T5Tokenizer.from_pretrained('t5-small')
model = T5ForConditionalGeneration.from_pretrained('t5-small')

#training args for Trainer
training_args= TrainingArguments(
    output_dir='/content/drive/My Drive/Implementation/Final Thesis/FYP/JSON generation/T5 NESTED',
    num_train_epochs=epochs,
     #trainning batch size
    per_device_train_batch_size=32,
    #eval batch size
    per_device_eval_batch_size=32,
    save_steps=500,
    save_total_limit=2,
    logging_steps=10,
    eval_steps=500,
    evaluation_strategy="steps",
    prediction_loss_only=True,
    load_best_model_at_end=True,
    #to train fast
    fp16=True,
    #no to push to HF
    push_to_hub=False
)

#data collator for seq2seq
data_collator= DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)

#Trainer instance
trainer= Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    data_collator=data_collator
)

#train here nested first time!
trainer.train()

#save to drive
model_save_path= '/content/drive/My Drive/Implementation/Final Thesis/FYP/JSON generation/T5 NESTED'

model.save_pretrained(model_save_path)

tokenizer.save_pretrained(model_save_path)

def load_model_and_tokenizer(model_path):
  """ method to load the model and tokenizer
  Parameters
  model_path: path to the model and tokenizer
  Return
  model: loaded model
  tokenizer: loaded tokenizer
  """
  model= T5ForConditionalGeneration.from_pretrained(model_path)
  tokenizer= T5Tokenizer.from_pretrained(model_path)
  return model,tokenizer

model, tokenizer = load_model_and_tokenizer(model_save_path)

### Retrain

#Trainer to retrain
trainer= Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    data_collator=data_collator
)

#train again!
trainer.train()

def generate_json(prompt, model, tokenizer, max_length=1024):
  """ method to generate the json from the prompt
  Parameters
  prompt: prompt to generate the json
  model: model to generate the json
  tokenizer: tokenizer to generate the json
  max_length: maximum length of the json
  Return
  generated_json_str: generated json string
  """

  device= 'cuda' if torch.cuda.is_available() else 'cpu'
  model.to(device)
  #tokenizing prompt
  inputs= tokenizer(prompt, return_tensors='pt', padding='max_length', truncation=True, max_length=max_length).to(device)
  outputs =model.generate(inputs['input_ids'],max_length=max_length,num_beams=5,early_stopping=True, temperature=0)
  generated_json_str= tokenizer.decode(outputs[0], skip_special_tokens=True)
  return generated_json_str

#TEST
description="Create a trendy icon-button with a state of hover"

generated_json= generate_json(description, model, tokenizer)


print(generated_json)

